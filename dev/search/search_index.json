{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"fptools","text":""},{"location":"#tools-for-fiber-photometry-analysis","title":"Tools for Fiber Photometry Analysis","text":"<p>Codes/Notebooks to analyze Fiber Photometry and behavioral data</p> <p>See also related package for analysing event data from TDT and MedAssociate systems: med-associates-utils</p>"},{"location":"#about","title":"About","text":"<p>This package allows you to load data produced by TDT Synapse software, and eases working with the resulting data. Also includes a growing library of analysis routines.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>read TDT tank files</li> <li>metadata management and propagation</li> <li></li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>We recommend you use anaconda virtual environments.</p> <p>There are two main ways to install this package. The first, create the virtual environment, and install this package directly (more useful on \"production\" systems). The second, clone the repository, and then pass to anaconda the <code>environment.yaml</code> file during environment creation (more useful for development). <pre><code>conda create -n fptools python=3.12\npip install git+https://github.com/tischfieldlab/fptools.git\n</code></pre> OR <pre><code>git clone https://github.com/tischfieldlab/fptools.git\ncd fptools\nconda env create -f environment.yml\n</code></pre></p>"},{"location":"#support","title":"Support","text":"<p>For technical inquiries specific to this package, please open an Issue with a description of your problem or request.</p> <p>For general usage, see the main website.</p> <p>Other questions? Reach out to <code>thackray@rutgers.edu</code>.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"loading/","title":"Loading Data","text":"<p><code>fptools</code> offers robust data loading functionality for TDT tank/block files, including metadata injection, parallelism, cacheing, preprocessing and selection/renaming of streams to load.</p>"},{"location":"loading/#signal-map","title":"Signal Map","text":"<p>Signal maps tell the system what data to load, the relationship between different streams, and allows renaming of data streams.</p>"},{"location":"loading/#manifest","title":"Manifest","text":"<p>Provide a tabular data file (ex: xlsx, csv, tsv) keyed by a column named <code>blockname</code>, and the fields in that row will be added to a given sessions metadata</p>"},{"location":"loading/#parallelism-and-cacheing","title":"Parallelism and Cacheing","text":"<p>Data loading can occur in parallel. Just specify the number of workers to use via the <code>max_workers</code> parameter. each worker runs in a separate process, suitable for running preprocessing routines. The optimal number of workers would depend on the resources of the computer running the analysis.</p> <p>preprocessed data can be cached for quick retrieval later, without needing to re-perform expensive operations. To enable cacheing, set the <code>cache</code> parameter to <code>True</code>, setting to <code>False</code> will disable the cache. Cached data needs to be stored someplace on disk, and can be controlled by providing a filesystem path to the <code>cache_dir</code> parameter to a directory to contain the cache.</p>"},{"location":"loading/#preprocessors","title":"Preprocessors","text":"<p>We offer several preprocessing routines you may choose from, or you may provide your own implementation; simply pass a function to the <code>preprocess</code>. If no preprocessor implementation is passed, the signals specified in the <code>signal_map</code> are simply added to the <code>Session</code> with no further changes.</p>"},{"location":"loading/#example","title":"Example","text":"<p>See the notebook <code>01_Data_Loading.ipnb</code> for an example of data loading.</p>"},{"location":"mkdocs/","title":"Mkdocs","text":""},{"location":"mkdocs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"mkdocs/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"session/","title":"Sessions and SessionCollections","text":"<p>A <code>Session</code> serves as the basic container for data. It typically would correspond to data from a single animal and a single bout of recording. A group of <code>Session</code>s can be added to a <code>SessionCollection</code>. A <code>SessionCollection</code> offers several convience methods for working with many sessions. Many functions in this package accept a <code>Session</code> or <code>SessionCollection</code>.</p> <p><code>Session</code>s typically will contain the following types of data on the attributes:</p> <ul> <li><code>metadata</code>: arbitrary metadata for a given session</li> <li><code>signals</code>: continuous-valued time-series data sampled at fixed intervals</li> <li><code>epocs</code>: discrete event timestamps</li> </ul>"},{"location":"session/#metadata","title":"Metadata","text":"<p>Sessions can keep track of metadata about a recording as a simple dictionary, accessable via the <code>metadata</code> property. Metadata is typically populated when a session is loaded, but metadata can be added at anytime.</p> <p>When working with a <code>SessionCollection</code>, metadata for all sessions can be returned as a <code>pandas.DataFrame</code> from the <code>.metadata</code> property. There are several conveince methods that are useful for decorating sessions with addition metadata.</p>"},{"location":"session/#signals","title":"Signals","text":"<p>A <code>Signal</code> encapsulates continuous-valued time-series data sampled at fixed intervals, along with some metadata, such as <code>name</code>, <code>unit</code>, <code>marks</code>. A <code>Signal</code> can describe itself.</p> <pre><code>&gt; signal.describe()\nDopamine:\n    units = \u0394F/F\n    n_observations = 1\n    n_samples = 366051\n    duration = 0:59:58.417848\n    sample_rate = 101.72526245132634\n</code></pre>"},{"location":"session/#shape-of-signalsignal","title":"Shape of <code>Signal.signal</code>","text":"<p>Signal data can first be described by the number of samples it contains (i.e. the length of the array), and is retrievable by <code>Signal.nsamples</code>. The duration, as a wall clock amount of time, can be retrieved as a <code>datetime</code> by <code>Signal.duration</code>, and is equivelent to <code>Signal.nsamples * Signal.fs</code>. </p> <p>A <code>Signal</code> can contain data from a single observation (in this case <code>Signal.nobs == 1</code>), or from muliple observation (typical in the case of the return from <code>collect_signals()</code>, in this case <code>Signal.nobs &gt; 1</code>). In the case of a single observation, the shape of the signal will be 1D, while in the case of multiple observations, the signal will be 2D <code>(nobs, nsamples)</code>.</p>"},{"location":"session/#signal-aggregation","title":"Signal aggregation","text":"<p>In the case there are multiple observations, one would commonly want to aggregate to produce a mean/median signal representative of all observations. To do so, use the <code>aggregate()</code> method of the signal object. This method return a new <code>Signal</code> with propegated marks, units, sampling frequency and time. The new signal will be named according to this signal, with <code>#{func}</code> appended. The parameter <code>func</code> determines how the signal is aggregated, and can accept several types of argument.</p> <ul> <li>simple strings, like <code>mean</code>, <code>median</code>, <code>min</code>, <code>max</code> will invoke the <code>numpy</code> implementation (really any numpy function that is a <code>ufunc</code> and exists in the global <code>numpy</code> namespace)</li> <li>reference to a <code>numpy</code> <code>ufunc</code>, such as <code>numpy.mean</code></li> <li>any other arbitrary function that accepts and array and returns an array</li> </ul>"},{"location":"session/#epocs","title":"Epocs","text":"<p>Epocs contain behavioral data. Currently, these are stored as a simple dictionary on <code>Session</code> objects, with string keys as the event name, and the values as numpy arrays of relative event times, in seconds.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>fptools<ul> <li>io<ul> <li>session</li> <li>signal</li> <li>tdt</li> </ul> </li> <li>measure<ul> <li>peaks</li> <li>signal_collector</li> <li>snr</li> </ul> </li> <li>preprocess<ul> <li>lib</li> <li>pipelines<ul> <li>dxp_motion_dff</li> <li>lowpass_dff</li> <li>tdt_default</li> </ul> </li> </ul> </li> <li>viz<ul> <li>viz</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/fptools/","title":"fptools","text":""},{"location":"reference/fptools/#fptools","title":"<code>fptools</code>","text":"<p>Modules:</p> <ul> <li> <code>io</code>           \u2013            </li> <li> <code>measure</code>           \u2013            </li> <li> <code>preprocess</code>           \u2013            </li> <li> <code>viz</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/io/","title":"io","text":""},{"location":"reference/fptools/io/#fptools.io","title":"<code>fptools.io</code>","text":"<p>Modules:</p> <ul> <li> <code>session</code>           \u2013            </li> <li> <code>signal</code>           \u2013            </li> <li> <code>tdt</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/io/session/","title":"session","text":""},{"location":"reference/fptools/io/session/#fptools.io.session","title":"<code>fptools.io.session</code>","text":"<p>Classes:</p> <ul> <li> <code>Session</code>           \u2013            <p>Holds data and metadata for a single session.</p> </li> <li> <code>SessionCollection</code>           \u2013            <p>Collection of session data.</p> </li> </ul>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session","title":"<code>Session</code>","text":"<p>               Bases: <code>object</code></p> <p>Holds data and metadata for a single session.</p> <p>Methods:</p> <ul> <li> <code>__init__</code>             \u2013              <p>Initialize this Session object.</p> </li> <li> <code>add_signal</code>             \u2013              <p>Add a signal to this Session.</p> </li> <li> <code>describe</code>             \u2013              <p>Describe this session.</p> </li> <li> <code>rename_epoc</code>             \u2013              <p>Rename a epoc, from <code>old_name</code> to <code>new_name</code>.</p> </li> <li> <code>rename_signal</code>             \u2013              <p>Rename a signal, from <code>old_name</code> to <code>new_name</code>.</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>class Session(object):\n    \"\"\"Holds data and metadata for a single session.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize this Session object.\"\"\"\n        self.metadata: dict[str, Any] = {}\n        self.signals: dict[str, Signal] = {}\n        self.epocs: dict[str, np.ndarray] = defaultdict(partial(np.ndarray, 0))\n\n    def describe(self, as_str: bool = False) -&gt; Union[str, None]:\n        \"\"\"Describe this session.\n\n        describes the metadata, scalars, and arrays contained in this session.\n\n        Args:\n            as_str: if True, return description as a string, otherwise print the description and return None\n\n        Returns:\n            `None` if `as_str` is `False`; if `as_str` is `True`, returns the description as a `str`\n        \"\"\"\n        buffer = \"\"\n\n        buffer += \"Metadata:\\n\"\n        if len(self.metadata) &gt; 0:\n            for k, v in self.metadata.items():\n                buffer += f\"    {k}: {v}\\n\"\n        else:\n            buffer += \"    &lt; No Metadata Available &gt;\\n\"\n        buffer += \"\\n\"\n\n        buffer += \"Epocs:\\n\"\n        if len(self.epocs) &gt; 0:\n            for k, v in self.epocs.items():\n                # buffer += f'    \"{k}\" with shape {v.shape}:\\n    {np.array2string(v, prefix=\"    \")}\\n\\n'\n                buffer += f\"    {k}:\\n\"\n                buffer += f\"        num_events = {v.shape}\\n\"\n                buffer += f\"        avg_rate = {datetime.timedelta(seconds=np.diff(v)[0])}\\n\"\n                buffer += f\"        earliest = {datetime.timedelta(seconds=v[0])}\\n\"\n                buffer += f\"        latest = {datetime.timedelta(seconds=v[-1])}\\n\"\n        else:\n            buffer += \"    &lt; No Epocs Available &gt;\\n\"\n        buffer += \"\\n\"\n\n        buffer += \"Signals:\\n\"\n        if len(self.signals) &gt; 0:\n            for k, v in self.signals.items():\n                buffer += v.describe(as_str=True, prefix=\"    \")\n        else:\n            buffer += \"    &lt; No Signals Available &gt;\\n\"\n        buffer += \"\\n\"\n\n        if as_str:\n            return buffer\n        else:\n            print(buffer)\n            return None\n\n    def add_signal(self, signal: Signal, overwrite: bool = False) -&gt; None:\n        \"\"\"Add a signal to this Session.\n\n        Raises an error if the new signal name already exists and `overwrite` is not True.\n\n        Args:\n            signal: the signal to add to this Session\n            overwrite: if True, allow overwriting a pre-existing signal with the same name, if False, will raise instead.\n        \"\"\"\n        if signal.name in self.signals and not overwrite:\n            raise KeyError(f\"Key `{signal.name}` already exists in data!\")\n\n        self.signals[signal.name] = signal\n\n    def rename_signal(self, old_name: str, new_name: str) -&gt; None:\n        \"\"\"Rename a signal, from `old_name` to `new_name`.\n\n        Raises an error if the new signal name already exists.\n\n        Args:\n            old_name: the current name for the signal\n            new_name: the new name for the signal\n        \"\"\"\n        if new_name in self.signals:\n            raise KeyError(f\"Key `{new_name}` already exists in data!\")\n\n        self.signals[new_name] = self.signals[old_name]\n        self.signals.pop(old_name)\n\n    def rename_epoc(self, old_name: str, new_name: str) -&gt; None:\n        \"\"\"Rename a epoc, from `old_name` to `new_name`.\n\n        Raises an error if the new epoc name already exists.\n\n        Args:\n            old_name: the current name for the epoc\n            new_name: the new name for the epoc\n        \"\"\"\n        if new_name in self.epocs:\n            raise KeyError(f\"Key `{new_name}` already exists in data!\")\n\n        self.epocs[new_name] = self.epocs[old_name]\n        self.epocs.pop(old_name)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.__init__","title":"<code>__init__()</code>","text":"<p>Initialize this Session object.</p> Source code in <code>fptools/io/session.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize this Session object.\"\"\"\n    self.metadata: dict[str, Any] = {}\n    self.signals: dict[str, Signal] = {}\n    self.epocs: dict[str, np.ndarray] = defaultdict(partial(np.ndarray, 0))\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.add_signal","title":"<code>add_signal(signal, overwrite=False)</code>","text":"<p>Add a signal to this Session.</p> <p>Raises an error if the new signal name already exists and <code>overwrite</code> is not True.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>Signal</code>)           \u2013            <p>the signal to add to this Session</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if True, allow overwriting a pre-existing signal with the same name, if False, will raise instead.</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def add_signal(self, signal: Signal, overwrite: bool = False) -&gt; None:\n    \"\"\"Add a signal to this Session.\n\n    Raises an error if the new signal name already exists and `overwrite` is not True.\n\n    Args:\n        signal: the signal to add to this Session\n        overwrite: if True, allow overwriting a pre-existing signal with the same name, if False, will raise instead.\n    \"\"\"\n    if signal.name in self.signals and not overwrite:\n        raise KeyError(f\"Key `{signal.name}` already exists in data!\")\n\n    self.signals[signal.name] = signal\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.describe","title":"<code>describe(as_str=False)</code>","text":"<p>Describe this session.</p> <p>describes the metadata, scalars, and arrays contained in this session.</p> <p>Parameters:</p> <ul> <li> <code>as_str</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if True, return description as a string, otherwise print the description and return None</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, None]</code>           \u2013            <p><code>None</code> if <code>as_str</code> is <code>False</code>; if <code>as_str</code> is <code>True</code>, returns the description as a <code>str</code></p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def describe(self, as_str: bool = False) -&gt; Union[str, None]:\n    \"\"\"Describe this session.\n\n    describes the metadata, scalars, and arrays contained in this session.\n\n    Args:\n        as_str: if True, return description as a string, otherwise print the description and return None\n\n    Returns:\n        `None` if `as_str` is `False`; if `as_str` is `True`, returns the description as a `str`\n    \"\"\"\n    buffer = \"\"\n\n    buffer += \"Metadata:\\n\"\n    if len(self.metadata) &gt; 0:\n        for k, v in self.metadata.items():\n            buffer += f\"    {k}: {v}\\n\"\n    else:\n        buffer += \"    &lt; No Metadata Available &gt;\\n\"\n    buffer += \"\\n\"\n\n    buffer += \"Epocs:\\n\"\n    if len(self.epocs) &gt; 0:\n        for k, v in self.epocs.items():\n            # buffer += f'    \"{k}\" with shape {v.shape}:\\n    {np.array2string(v, prefix=\"    \")}\\n\\n'\n            buffer += f\"    {k}:\\n\"\n            buffer += f\"        num_events = {v.shape}\\n\"\n            buffer += f\"        avg_rate = {datetime.timedelta(seconds=np.diff(v)[0])}\\n\"\n            buffer += f\"        earliest = {datetime.timedelta(seconds=v[0])}\\n\"\n            buffer += f\"        latest = {datetime.timedelta(seconds=v[-1])}\\n\"\n    else:\n        buffer += \"    &lt; No Epocs Available &gt;\\n\"\n    buffer += \"\\n\"\n\n    buffer += \"Signals:\\n\"\n    if len(self.signals) &gt; 0:\n        for k, v in self.signals.items():\n            buffer += v.describe(as_str=True, prefix=\"    \")\n    else:\n        buffer += \"    &lt; No Signals Available &gt;\\n\"\n    buffer += \"\\n\"\n\n    if as_str:\n        return buffer\n    else:\n        print(buffer)\n        return None\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.rename_epoc","title":"<code>rename_epoc(old_name, new_name)</code>","text":"<p>Rename a epoc, from <code>old_name</code> to <code>new_name</code>.</p> <p>Raises an error if the new epoc name already exists.</p> <p>Parameters:</p> <ul> <li> <code>old_name</code>               (<code>str</code>)           \u2013            <p>the current name for the epoc</p> </li> <li> <code>new_name</code>               (<code>str</code>)           \u2013            <p>the new name for the epoc</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def rename_epoc(self, old_name: str, new_name: str) -&gt; None:\n    \"\"\"Rename a epoc, from `old_name` to `new_name`.\n\n    Raises an error if the new epoc name already exists.\n\n    Args:\n        old_name: the current name for the epoc\n        new_name: the new name for the epoc\n    \"\"\"\n    if new_name in self.epocs:\n        raise KeyError(f\"Key `{new_name}` already exists in data!\")\n\n    self.epocs[new_name] = self.epocs[old_name]\n    self.epocs.pop(old_name)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.rename_signal","title":"<code>rename_signal(old_name, new_name)</code>","text":"<p>Rename a signal, from <code>old_name</code> to <code>new_name</code>.</p> <p>Raises an error if the new signal name already exists.</p> <p>Parameters:</p> <ul> <li> <code>old_name</code>               (<code>str</code>)           \u2013            <p>the current name for the signal</p> </li> <li> <code>new_name</code>               (<code>str</code>)           \u2013            <p>the new name for the signal</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def rename_signal(self, old_name: str, new_name: str) -&gt; None:\n    \"\"\"Rename a signal, from `old_name` to `new_name`.\n\n    Raises an error if the new signal name already exists.\n\n    Args:\n        old_name: the current name for the signal\n        new_name: the new name for the signal\n    \"\"\"\n    if new_name in self.signals:\n        raise KeyError(f\"Key `{new_name}` already exists in data!\")\n\n    self.signals[new_name] = self.signals[old_name]\n    self.signals.pop(old_name)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection","title":"<code>SessionCollection</code>","text":"<p>               Bases: <code>list[Session]</code></p> <p>Collection of session data.</p> <p>Methods:</p> <ul> <li> <code>__init__</code>             \u2013              <p>Initialize this <code>SessionCollection</code>.</p> </li> <li> <code>add_metadata</code>             \u2013              <p>Set a metadata field on each session in this collection.</p> </li> <li> <code>aggregate_signals</code>             \u2013              <p>Aggregate signals across sessions in this collection for the signal name <code>name</code>.</p> </li> <li> <code>apply</code>             \u2013              <p>Apply a function to each session in this collection.</p> </li> <li> <code>describe</code>             \u2013              <p>Describe this collection of sessions.</p> </li> <li> <code>filter</code>             \u2013              <p>Filter the items in this collection, returning a new <code>SessionCollection</code> containing sessions which pass <code>predicate</code>.</p> </li> <li> <code>get_signal</code>             \u2013              <p>Get data across sessions in this collection for the signal named <code>name</code>.</p> </li> <li> <code>get_signal_dataframe</code>             \u2013              <p>Get data for a given signal across sessions, also injecting metadata.</p> </li> <li> <code>map</code>             \u2013              <p>Apply a function to each session in this collection, returning a new collection with the results.</p> </li> <li> <code>merge</code>             \u2013              <p>Merge session collections while preserving data.</p> </li> <li> <code>rename_epoc</code>             \u2013              <p>Rename an epoc on each session in this collection.</p> </li> <li> <code>rename_signal</code>             \u2013              <p>Rename a signal on each session in this collection.</p> </li> <li> <code>select</code>             \u2013              <p>Select sessions in this collection, returning a new <code>SessionCollection</code> containing sessions which all bool masks are true.</p> </li> <li> <code>set_metadata_props</code>             \u2013              <p>Set properties of a metadata column.</p> </li> <li> <code>update_metadata</code>             \u2013              <p>Set multiple metadata fields on each session in this collection.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>metadata</code>               (<code>DataFrame</code>)           \u2013            <p>Get a dataframe containing metadata across all sessions in this collection.</p> </li> <li> <code>metadata_keys</code>               (<code>list[str]</code>)           \u2013            <p>Get a list of the keys present in metadata across all sessions in this collection.</p> </li> <li> <code>signal_keys</code>               (<code>list[str]</code>)           \u2013            <p>Get a list of Signal keys in this SessionCollection.</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>class SessionCollection(list[Session]):\n    \"\"\"Collection of session data.\"\"\"\n\n    def __init__(self, *args) -&gt; None:\n        \"\"\"Initialize this `SessionCollection`.\"\"\"\n        super().__init__(*args)\n        self.__meta_meta: dict[str, dict[Literal[\"order\"], Any]] = {}\n\n    @property\n    def metadata(self) -&gt; pd.DataFrame:\n        \"\"\"Get a dataframe containing metadata across all sessions in this collection.\"\"\"\n        df = pd.DataFrame([item.metadata for item in self])\n\n        for k, v in self.__meta_meta.items():\n            if \"order\" in v:\n                df[k] = pd.Categorical(df[k], categories=v[\"order\"], ordered=True)\n\n        return df\n\n    @property\n    def metadata_keys(self) -&gt; list[str]:\n        \"\"\"Get a list of the keys present in metadata across all sessions in this collection.\"\"\"\n        return list(set([key for item in self for key in item.metadata.keys()]))\n\n    def add_metadata(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set a metadata field on each session in this collection.\n\n        Args:\n            key: name of the metadata field\n            value: value for the metadata field\n        \"\"\"\n        for item in self:\n            item.metadata[key] = value\n\n    def update_metadata(self, meta: dict[str, Any]) -&gt; None:\n        \"\"\"Set multiple metadata fields on each session in this collection.\n\n        Args:\n            meta: metadata information to set on each session\n        \"\"\"\n        for item in self:\n            item.metadata.update(meta)\n\n    def set_metadata_props(self, key: str, order: Optional[list[Any]] = None):\n        \"\"\"Set properties of a metadata column.\n\n        Args:\n            key: name of the metadata item, always required\n            order: optional, if specified will set the metadata column to be ordered categorical, according to `order`\n        \"\"\"\n        assert key in self.metadata_keys\n\n        if key not in self.__meta_meta:\n            self.__meta_meta[key] = {}\n\n        if order is not None:\n            self.__meta_meta[key][\"order\"] = order\n\n    def rename_signal(self, old_name: str, new_name: str) -&gt; None:\n        \"\"\"Rename a signal on each session in this collection.\n\n        Args:\n            old_name: current name of the signal\n            new_name: the new name for the signal\n        \"\"\"\n        for item in self:\n            item.rename_signal(old_name, new_name)\n\n    def rename_epoc(self, old_name: str, new_name: str) -&gt; None:\n        \"\"\"Rename an epoc on each session in this collection.\n\n        Args:\n            old_name: current name of the epoc\n            new_name: the new name for the epoc\n        \"\"\"\n        for item in self:\n            item.rename_epoc(old_name, new_name)\n\n    def filter(self, predicate: Callable[[Session], bool]) -&gt; \"SessionCollection\":\n        \"\"\"Filter the items in this collection, returning a new `SessionCollection` containing sessions which pass `predicate`.\n\n        Args:\n            predicate: a callable accepting a single session and returning bool.\n\n        Returns:\n            a new `SessionCollection` containing only items which pass `predicate`.\n        \"\"\"\n        sc = type(self)(item for item in self if predicate(item))\n        sc.__meta_meta.update(**copy.deepcopy(self.__meta_meta))\n        return sc\n\n    def select(self, *bool_masks: np.ndarray) -&gt; \"SessionCollection\":\n        \"\"\"Select sessions in this collection, returning a new `SessionCollection` containing sessions which all bool masks are true.\n\n        Args:\n            bool_masks: one or more boolean arrays, the reduced logical_and indicating which sessions to select\n\n        Returns:\n            a new `SessionCollection` containing only items which pass bool_masks.\n        \"\"\"\n        sc = type(self)(item for item, include in zip(self, np.logical_and.reduce(bool_masks)) if include)\n        sc.__meta_meta.update(**copy.deepcopy(self.__meta_meta))\n        return sc\n\n    def map(self, action: Callable[[Session], Session]) -&gt; \"SessionCollection\":\n        \"\"\"Apply a function to each session in this collection, returning a new collection with the results.\n\n        Args:\n            action: callable accepting a single session and returning a new session\n\n        Returns:\n            a new `SessionCollection` containing the results of `action`\n        \"\"\"\n        sc = type(self)(action(item) for item in self)\n        sc.__meta_meta.update(**copy.deepcopy(self.__meta_meta))\n        return sc\n\n    def apply(self, func: Callable[[Session], None]) -&gt; None:\n        \"\"\"Apply a function to each session in this collection.\n\n        Args:\n            func: callable accepting a single session and returning None\n        \"\"\"\n        for item in self:\n            func(item)\n\n    WHAT_LIST = Literal[\"all\", \"signal\", \"epocs\", \"metadata\"]\n\n    @staticmethod\n    def merge(\n        *session_collections: \"SessionCollection\", primary_key: str, what: Union[WHAT_LIST, list[WHAT_LIST]], prefixes: list[str]\n    ) -&gt; \"SessionCollection\":\n        \"\"\"Merge session collections while preserving data.\n\n        Args:\n            session_collections: SessionCollections to merge\n            primary_key: metadata key used to join sessions\n            what: the data within each session to merge\n            prefixes: list of prefixes, of the same length as the number of passed SessionCollections. each prefix will be preprended to signals to avoid overwriting\n        \"\"\"\n        available_whats = [\"signal\", \"epocs\", \"metadata\"]\n        use_what: list[str] = []\n        if isinstance(what, str):\n            if what == \"all\":\n                use_what.extend(available_whats)\n            else:\n                use_what.append(what)\n        else:\n            use_what.extend(what)\n\n        sorter: dict[str, list[Session]] = defaultdict(list[Session])\n        for collection in session_collections:\n            for session in collection:\n                sorter[session.metadata[primary_key]].append(session)\n\n        final = SessionCollection()\n        for k, v in sorter.items():\n            new_session = Session()\n            for i, old_session in enumerate(v):\n                if \"signal\" in use_what:\n                    for _, sig in old_session.signals.items():\n                        new_session.add_signal(sig.copy(f\"{prefixes[i]}{sig.name}\"))\n\n                if \"epocs\" in use_what:\n                    for name, epocs in old_session.epocs.items():\n                        new_session.epocs[name] = epocs\n\n                if \"metadata\" in use_what:\n                    new_session.metadata.update(old_session.metadata)\n            final.append(new_session)\n        return final\n\n    @property\n    def signal_keys(self) -&gt; list[str]:\n        \"\"\"Get a list of Signal keys in this SessionCollection.\"\"\"\n        return list(set([key for item in self for key in item.signals.keys()]))\n\n    def get_signal(self, name: str) -&gt; list[Signal]:\n        \"\"\"Get data across sessions in this collection for the signal named `name`.\n\n        Args:\n            name: Name of the signals to collect\n\n        Returns:\n            List of Signals, each corresponding to a single session\n        \"\"\"\n        return [item.signals[name] for item in self]\n\n    def get_signal_dataframe(self, signal: str, include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n        \"\"\"Get data for a given signal across sessions, also injecting metadata.\n\n        See also: `Signal.to_dataframe()`\n\n        Args:\n            signal: Name of the signal to collect\n            include_meta: include_meta: metadata fields to include in the final output. Special string \"all\" will include all metadata fields\n\n        Returns:\n            signal across sessions as a `pandas.DataFrame`\n        \"\"\"\n        dfs = []\n        for session in self:\n            if include_meta == \"all\":\n                meta = session.metadata\n            else:\n                meta = {k: v for k, v in session.metadata.items() if k in include_meta}\n\n            sig = session.signals[signal]\n            df = sig.to_dataframe()\n\n            df[\"obs\"] = list(range(sig.nobs))\n\n            for k, v in meta.items():\n                df[k] = v\n\n            # reorder columns\n            df = df[\n                [c for c in df.columns.values if not str(c).startswith(\"Y.\")] + [c for c in df.columns.values if str(c).startswith(\"Y.\")]\n            ]\n\n            dfs.append(df)\n\n        return pd.concat(dfs, ignore_index=True)\n\n    def aggregate_signals(self, name: str, method: Union[str, np.ufunc, Callable[[np.ndarray], np.ndarray]] = \"median\") -&gt; Signal:\n        \"\"\"Aggregate signals across sessions in this collection for the signal name `name`.\n\n        Args:\n            name: name of the signal to aggregate\n            method: the method used for aggregation\n\n        Returns:\n            Aggregated `Signal`\n        \"\"\"\n        signals = self.get_signal(name)\n        if len(signals) &lt;= 0:\n            raise ValueError(\"No signals were passed!\")\n\n        # check all signals have the same number of samples\n        assert np.all(np.equal([s.nsamples for s in signals], signals[0].nsamples))\n\n        if method is not None:\n            signals = [s.aggregate(method) for s in signals]\n\n        s = Signal(signals[0].name, np.vstack([s.signal for s in signals]), time=signals[0].time, units=signals[0].units)\n        s.marks.update(signals[0].marks)\n        return s\n\n    def describe(self, as_str: bool = False) -&gt; Union[str, None]:\n        \"\"\"Describe this collection of sessions.\n\n        Args:\n            as_str: if True, return description as a string, otherwise print the description and return None\n\n        Returns:\n            `None` if `as_str` is `False`; if `as_str` is `True`, returns the description as a `str`\n        \"\"\"\n        buffer = \"\"\n\n        buffer += f\"Number of sessions: {len(self)}\\n\\n\"\n\n        signals = Counter([item for session in self for item in session.signals.keys()])\n        buffer += \"Signals present in data with counts:\\n\"\n        for k, v in signals.items():\n            buffer += f'({v}) \"{k}\"\\n'\n        buffer += \"\\n\"\n\n        epocs = Counter([item for session in self for item in session.epocs.keys()])\n        buffer += \"Epocs present in data with counts:\\n\"\n        for k, v in epocs.items():\n            buffer += f'({v}) \"{k}\"\\n'\n        buffer += \"\\n\"\n\n        if as_str:\n            return buffer\n        else:\n            print(buffer)\n            return None\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Get a dataframe containing metadata across all sessions in this collection.</p>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.metadata_keys","title":"<code>metadata_keys</code>  <code>property</code>","text":"<p>Get a list of the keys present in metadata across all sessions in this collection.</p>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.signal_keys","title":"<code>signal_keys</code>  <code>property</code>","text":"<p>Get a list of Signal keys in this SessionCollection.</p>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.__init__","title":"<code>__init__(*args)</code>","text":"<p>Initialize this <code>SessionCollection</code>.</p> Source code in <code>fptools/io/session.py</code> <pre><code>def __init__(self, *args) -&gt; None:\n    \"\"\"Initialize this `SessionCollection`.\"\"\"\n    super().__init__(*args)\n    self.__meta_meta: dict[str, dict[Literal[\"order\"], Any]] = {}\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.add_metadata","title":"<code>add_metadata(key, value)</code>","text":"<p>Set a metadata field on each session in this collection.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>name of the metadata field</p> </li> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>value for the metadata field</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def add_metadata(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set a metadata field on each session in this collection.\n\n    Args:\n        key: name of the metadata field\n        value: value for the metadata field\n    \"\"\"\n    for item in self:\n        item.metadata[key] = value\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.aggregate_signals","title":"<code>aggregate_signals(name, method='median')</code>","text":"<p>Aggregate signals across sessions in this collection for the signal name <code>name</code>.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>name of the signal to aggregate</p> </li> <li> <code>method</code>               (<code>Union[str, ufunc, Callable[[ndarray], ndarray]]</code>, default:                   <code>'median'</code> )           \u2013            <p>the method used for aggregation</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Signal</code>           \u2013            <p>Aggregated <code>Signal</code></p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def aggregate_signals(self, name: str, method: Union[str, np.ufunc, Callable[[np.ndarray], np.ndarray]] = \"median\") -&gt; Signal:\n    \"\"\"Aggregate signals across sessions in this collection for the signal name `name`.\n\n    Args:\n        name: name of the signal to aggregate\n        method: the method used for aggregation\n\n    Returns:\n        Aggregated `Signal`\n    \"\"\"\n    signals = self.get_signal(name)\n    if len(signals) &lt;= 0:\n        raise ValueError(\"No signals were passed!\")\n\n    # check all signals have the same number of samples\n    assert np.all(np.equal([s.nsamples for s in signals], signals[0].nsamples))\n\n    if method is not None:\n        signals = [s.aggregate(method) for s in signals]\n\n    s = Signal(signals[0].name, np.vstack([s.signal for s in signals]), time=signals[0].time, units=signals[0].units)\n    s.marks.update(signals[0].marks)\n    return s\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.apply","title":"<code>apply(func)</code>","text":"<p>Apply a function to each session in this collection.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Callable[[Session], None]</code>)           \u2013            <p>callable accepting a single session and returning None</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def apply(self, func: Callable[[Session], None]) -&gt; None:\n    \"\"\"Apply a function to each session in this collection.\n\n    Args:\n        func: callable accepting a single session and returning None\n    \"\"\"\n    for item in self:\n        func(item)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.describe","title":"<code>describe(as_str=False)</code>","text":"<p>Describe this collection of sessions.</p> <p>Parameters:</p> <ul> <li> <code>as_str</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if True, return description as a string, otherwise print the description and return None</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, None]</code>           \u2013            <p><code>None</code> if <code>as_str</code> is <code>False</code>; if <code>as_str</code> is <code>True</code>, returns the description as a <code>str</code></p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def describe(self, as_str: bool = False) -&gt; Union[str, None]:\n    \"\"\"Describe this collection of sessions.\n\n    Args:\n        as_str: if True, return description as a string, otherwise print the description and return None\n\n    Returns:\n        `None` if `as_str` is `False`; if `as_str` is `True`, returns the description as a `str`\n    \"\"\"\n    buffer = \"\"\n\n    buffer += f\"Number of sessions: {len(self)}\\n\\n\"\n\n    signals = Counter([item for session in self for item in session.signals.keys()])\n    buffer += \"Signals present in data with counts:\\n\"\n    for k, v in signals.items():\n        buffer += f'({v}) \"{k}\"\\n'\n    buffer += \"\\n\"\n\n    epocs = Counter([item for session in self for item in session.epocs.keys()])\n    buffer += \"Epocs present in data with counts:\\n\"\n    for k, v in epocs.items():\n        buffer += f'({v}) \"{k}\"\\n'\n    buffer += \"\\n\"\n\n    if as_str:\n        return buffer\n    else:\n        print(buffer)\n        return None\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.filter","title":"<code>filter(predicate)</code>","text":"<p>Filter the items in this collection, returning a new <code>SessionCollection</code> containing sessions which pass <code>predicate</code>.</p> <p>Parameters:</p> <ul> <li> <code>predicate</code>               (<code>Callable[[Session], bool]</code>)           \u2013            <p>a callable accepting a single session and returning bool.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SessionCollection</code>           \u2013            <p>a new <code>SessionCollection</code> containing only items which pass <code>predicate</code>.</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def filter(self, predicate: Callable[[Session], bool]) -&gt; \"SessionCollection\":\n    \"\"\"Filter the items in this collection, returning a new `SessionCollection` containing sessions which pass `predicate`.\n\n    Args:\n        predicate: a callable accepting a single session and returning bool.\n\n    Returns:\n        a new `SessionCollection` containing only items which pass `predicate`.\n    \"\"\"\n    sc = type(self)(item for item in self if predicate(item))\n    sc.__meta_meta.update(**copy.deepcopy(self.__meta_meta))\n    return sc\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.get_signal","title":"<code>get_signal(name)</code>","text":"<p>Get data across sessions in this collection for the signal named <code>name</code>.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of the signals to collect</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Signal]</code>           \u2013            <p>List of Signals, each corresponding to a single session</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def get_signal(self, name: str) -&gt; list[Signal]:\n    \"\"\"Get data across sessions in this collection for the signal named `name`.\n\n    Args:\n        name: Name of the signals to collect\n\n    Returns:\n        List of Signals, each corresponding to a single session\n    \"\"\"\n    return [item.signals[name] for item in self]\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.get_signal_dataframe","title":"<code>get_signal_dataframe(signal, include_meta='all')</code>","text":"<p>Get data for a given signal across sessions, also injecting metadata.</p> <p>See also: <code>Signal.to_dataframe()</code></p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>str</code>)           \u2013            <p>Name of the signal to collect</p> </li> <li> <code>include_meta</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>include_meta: metadata fields to include in the final output. Special string \"all\" will include all metadata fields</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>signal across sessions as a <code>pandas.DataFrame</code></p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def get_signal_dataframe(self, signal: str, include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n    \"\"\"Get data for a given signal across sessions, also injecting metadata.\n\n    See also: `Signal.to_dataframe()`\n\n    Args:\n        signal: Name of the signal to collect\n        include_meta: include_meta: metadata fields to include in the final output. Special string \"all\" will include all metadata fields\n\n    Returns:\n        signal across sessions as a `pandas.DataFrame`\n    \"\"\"\n    dfs = []\n    for session in self:\n        if include_meta == \"all\":\n            meta = session.metadata\n        else:\n            meta = {k: v for k, v in session.metadata.items() if k in include_meta}\n\n        sig = session.signals[signal]\n        df = sig.to_dataframe()\n\n        df[\"obs\"] = list(range(sig.nobs))\n\n        for k, v in meta.items():\n            df[k] = v\n\n        # reorder columns\n        df = df[\n            [c for c in df.columns.values if not str(c).startswith(\"Y.\")] + [c for c in df.columns.values if str(c).startswith(\"Y.\")]\n        ]\n\n        dfs.append(df)\n\n    return pd.concat(dfs, ignore_index=True)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.map","title":"<code>map(action)</code>","text":"<p>Apply a function to each session in this collection, returning a new collection with the results.</p> <p>Parameters:</p> <ul> <li> <code>action</code>               (<code>Callable[[Session], Session]</code>)           \u2013            <p>callable accepting a single session and returning a new session</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SessionCollection</code>           \u2013            <p>a new <code>SessionCollection</code> containing the results of <code>action</code></p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def map(self, action: Callable[[Session], Session]) -&gt; \"SessionCollection\":\n    \"\"\"Apply a function to each session in this collection, returning a new collection with the results.\n\n    Args:\n        action: callable accepting a single session and returning a new session\n\n    Returns:\n        a new `SessionCollection` containing the results of `action`\n    \"\"\"\n    sc = type(self)(action(item) for item in self)\n    sc.__meta_meta.update(**copy.deepcopy(self.__meta_meta))\n    return sc\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.merge","title":"<code>merge(*session_collections, primary_key, what, prefixes)</code>  <code>staticmethod</code>","text":"<p>Merge session collections while preserving data.</p> <p>Parameters:</p> <ul> <li> <code>session_collections</code>               (<code>SessionCollection</code>, default:                   <code>()</code> )           \u2013            <p>SessionCollections to merge</p> </li> <li> <code>primary_key</code>               (<code>str</code>)           \u2013            <p>metadata key used to join sessions</p> </li> <li> <code>what</code>               (<code>Union[WHAT_LIST, list[WHAT_LIST]]</code>)           \u2013            <p>the data within each session to merge</p> </li> <li> <code>prefixes</code>               (<code>list[str]</code>)           \u2013            <p>list of prefixes, of the same length as the number of passed SessionCollections. each prefix will be preprended to signals to avoid overwriting</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>@staticmethod\ndef merge(\n    *session_collections: \"SessionCollection\", primary_key: str, what: Union[WHAT_LIST, list[WHAT_LIST]], prefixes: list[str]\n) -&gt; \"SessionCollection\":\n    \"\"\"Merge session collections while preserving data.\n\n    Args:\n        session_collections: SessionCollections to merge\n        primary_key: metadata key used to join sessions\n        what: the data within each session to merge\n        prefixes: list of prefixes, of the same length as the number of passed SessionCollections. each prefix will be preprended to signals to avoid overwriting\n    \"\"\"\n    available_whats = [\"signal\", \"epocs\", \"metadata\"]\n    use_what: list[str] = []\n    if isinstance(what, str):\n        if what == \"all\":\n            use_what.extend(available_whats)\n        else:\n            use_what.append(what)\n    else:\n        use_what.extend(what)\n\n    sorter: dict[str, list[Session]] = defaultdict(list[Session])\n    for collection in session_collections:\n        for session in collection:\n            sorter[session.metadata[primary_key]].append(session)\n\n    final = SessionCollection()\n    for k, v in sorter.items():\n        new_session = Session()\n        for i, old_session in enumerate(v):\n            if \"signal\" in use_what:\n                for _, sig in old_session.signals.items():\n                    new_session.add_signal(sig.copy(f\"{prefixes[i]}{sig.name}\"))\n\n            if \"epocs\" in use_what:\n                for name, epocs in old_session.epocs.items():\n                    new_session.epocs[name] = epocs\n\n            if \"metadata\" in use_what:\n                new_session.metadata.update(old_session.metadata)\n        final.append(new_session)\n    return final\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.rename_epoc","title":"<code>rename_epoc(old_name, new_name)</code>","text":"<p>Rename an epoc on each session in this collection.</p> <p>Parameters:</p> <ul> <li> <code>old_name</code>               (<code>str</code>)           \u2013            <p>current name of the epoc</p> </li> <li> <code>new_name</code>               (<code>str</code>)           \u2013            <p>the new name for the epoc</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def rename_epoc(self, old_name: str, new_name: str) -&gt; None:\n    \"\"\"Rename an epoc on each session in this collection.\n\n    Args:\n        old_name: current name of the epoc\n        new_name: the new name for the epoc\n    \"\"\"\n    for item in self:\n        item.rename_epoc(old_name, new_name)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.rename_signal","title":"<code>rename_signal(old_name, new_name)</code>","text":"<p>Rename a signal on each session in this collection.</p> <p>Parameters:</p> <ul> <li> <code>old_name</code>               (<code>str</code>)           \u2013            <p>current name of the signal</p> </li> <li> <code>new_name</code>               (<code>str</code>)           \u2013            <p>the new name for the signal</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def rename_signal(self, old_name: str, new_name: str) -&gt; None:\n    \"\"\"Rename a signal on each session in this collection.\n\n    Args:\n        old_name: current name of the signal\n        new_name: the new name for the signal\n    \"\"\"\n    for item in self:\n        item.rename_signal(old_name, new_name)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.select","title":"<code>select(*bool_masks)</code>","text":"<p>Select sessions in this collection, returning a new <code>SessionCollection</code> containing sessions which all bool masks are true.</p> <p>Parameters:</p> <ul> <li> <code>bool_masks</code>               (<code>ndarray</code>, default:                   <code>()</code> )           \u2013            <p>one or more boolean arrays, the reduced logical_and indicating which sessions to select</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SessionCollection</code>           \u2013            <p>a new <code>SessionCollection</code> containing only items which pass bool_masks.</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def select(self, *bool_masks: np.ndarray) -&gt; \"SessionCollection\":\n    \"\"\"Select sessions in this collection, returning a new `SessionCollection` containing sessions which all bool masks are true.\n\n    Args:\n        bool_masks: one or more boolean arrays, the reduced logical_and indicating which sessions to select\n\n    Returns:\n        a new `SessionCollection` containing only items which pass bool_masks.\n    \"\"\"\n    sc = type(self)(item for item, include in zip(self, np.logical_and.reduce(bool_masks)) if include)\n    sc.__meta_meta.update(**copy.deepcopy(self.__meta_meta))\n    return sc\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.set_metadata_props","title":"<code>set_metadata_props(key, order=None)</code>","text":"<p>Set properties of a metadata column.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>name of the metadata item, always required</p> </li> <li> <code>order</code>               (<code>Optional[list[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>optional, if specified will set the metadata column to be ordered categorical, according to <code>order</code></p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def set_metadata_props(self, key: str, order: Optional[list[Any]] = None):\n    \"\"\"Set properties of a metadata column.\n\n    Args:\n        key: name of the metadata item, always required\n        order: optional, if specified will set the metadata column to be ordered categorical, according to `order`\n    \"\"\"\n    assert key in self.metadata_keys\n\n    if key not in self.__meta_meta:\n        self.__meta_meta[key] = {}\n\n    if order is not None:\n        self.__meta_meta[key][\"order\"] = order\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.update_metadata","title":"<code>update_metadata(meta)</code>","text":"<p>Set multiple metadata fields on each session in this collection.</p> <p>Parameters:</p> <ul> <li> <code>meta</code>               (<code>dict[str, Any]</code>)           \u2013            <p>metadata information to set on each session</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def update_metadata(self, meta: dict[str, Any]) -&gt; None:\n    \"\"\"Set multiple metadata fields on each session in this collection.\n\n    Args:\n        meta: metadata information to set on each session\n    \"\"\"\n    for item in self:\n        item.metadata.update(meta)\n</code></pre>"},{"location":"reference/fptools/io/signal/","title":"signal","text":""},{"location":"reference/fptools/io/signal/#fptools.io.signal","title":"<code>fptools.io.signal</code>","text":"<p>Classes:</p> <ul> <li> <code>Signal</code>           \u2013            <p>Represents a real valued signal with fixed interval sampling.</p> </li> </ul>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal","title":"<code>Signal</code>","text":"<p>               Bases: <code>object</code></p> <p>Represents a real valued signal with fixed interval sampling.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name for this Signal</p> </li> <li> <code>signal</code>               (<code>ndarray</code>)           \u2013            <p>Array of signal values</p> </li> <li> <code>units</code>               (<code>str</code>)           \u2013            <p>units of measurement for this signal</p> </li> <li> <code>marks</code>               (<code>dict[str, float]</code>)           \u2013            <p>dictionary of timestamp labels</p> </li> <li> <code>time</code>               (<code>ndarray</code>)           \u2013            <p>Array of relative time values</p> </li> <li> <code>fs</code>               (<code>float</code>)           \u2013            <p>Sampling frequency, in Hz</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__add__</code>             \u2013              <p>Add another signal to this signal, returning a new signal.</p> </li> <li> <code>__div__</code>             \u2013              <p>Divide this signal by another signal, returning a new signal.</p> </li> <li> <code>__init__</code>             \u2013              <p>Initialize a this Signal Object.</p> </li> <li> <code>__mul__</code>             \u2013              <p>Multiply another signal against this signal, returning a new signal.</p> </li> <li> <code>__sub__</code>             \u2013              <p>Subtract another signal from this signal, returning a new signal.</p> </li> <li> <code>aggregate</code>             \u2013              <p>Aggregate this signal.</p> </li> <li> <code>copy</code>             \u2013              <p>Return a deep copy of this signal.</p> </li> <li> <code>describe</code>             \u2013              <p>Describe this Signal.</p> </li> <li> <code>tindex</code>             \u2013              <p>Get the sample index closest to time <code>t</code>.</p> </li> <li> <code>to_dataframe</code>             \u2013              <p>Get the signal data as a <code>pandas.DataFrame</code>.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>duration</code>               (<code>timedelta</code>)           \u2013            <p>Get the duration of this Signal.</p> </li> <li> <code>nobs</code>               (<code>int</code>)           \u2013            <p>Get the number of observations for this Signal (i.e. number of trials).</p> </li> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Get the number of samples for this Signal (i.e. length of signal).</p> </li> </ul> Source code in <code>fptools/io/signal.py</code> <pre><code>class Signal(object):\n    \"\"\"Represents a real valued signal with fixed interval sampling.\n\n    Attributes:\n        name: Name for this Signal\n        signal: Array of signal values\n        units: units of measurement for this signal\n        marks: dictionary of timestamp labels\n        time: Array of relative time values\n        fs: Sampling frequency, in Hz\n    \"\"\"\n\n    def __init__(\n        self, name: str, signal: np.ndarray, time: Optional[np.ndarray] = None, fs: Optional[float] = None, units: str = \"AU\"\n    ) -&gt; None:\n        \"\"\"Initialize a this Signal Object.\n\n        At least one of `time` or `fs` must be provided. If `time` is provided, the sampling frequency (`fs`) will be estimated\n        from `time`. If `fs` is provided, the sampled timepoints (`time`) will be estimated. If both are provided, the values\n        will be checked against one another, and if they do not match, a ValueError will be raised.\n\n        Args:\n            name: Name of this signal\n            signal: Array of signal values\n            time: Array of sampled timepoints\n            fs: Sampling frequency, in Hz\n            units: units of this signal\n        \"\"\"\n        self.name: str = name\n        self.signal: np.ndarray = signal\n        self.units: str = units\n        self.marks: dict[str, float] = {}\n        self.time: np.ndarray\n        self.fs: float\n\n        if time is not None and fs is not None:\n            # both time and sampling frequency provided\n            self.time = time\n            self.fs = fs\n            # just do a sanity check that the two pieces of information make sense\n            if not np.isclose(self.fs, 1 / np.median(np.diff(time))):\n                raise ValueError(\n                    f\"Both `time` and `fs` were provided, but they do not match!\\n  fs={fs}\\ntime={1 / np.median(np.diff(time))}\"\n                )\n\n        elif time is None and fs is not None:\n            # sampleing frequency is provided, infer time from fs\n            self.fs = fs\n            self.time = np.linspace(1, signal.shape[0], signal.shape[0]) / self.fs\n\n        elif fs is None and time is not None:\n            # time is provided, so lets estimate the sampling frequency\n            self.time = time\n            self.fs = 1 / np.median(np.diff(time))\n\n        else:\n            # neither time or sampling frequency provided, we need at least one!\n            raise ValueError(\"Both `time` and `fs` cannot be `None`, one must be supplied!\")\n\n        if not self.signal.shape[-1] == self.time.shape[0]:\n            raise ValueError(f\"Signal and time must have the same length! signal.shape={self.signal.shape}; time.shape={self.time.shape}\")\n\n    @property\n    def nobs(self) -&gt; int:\n        \"\"\"Get the number of observations for this Signal (i.e. number of trials).\"\"\"\n        return self.signal.shape[0] if len(self.signal.shape) &gt; 1 else 1\n\n    @property\n    def nsamples(self) -&gt; int:\n        \"\"\"Get the number of samples for this Signal (i.e. length of signal).\"\"\"\n        return self.signal.shape[-1]\n\n    @property\n    def duration(self) -&gt; datetime.timedelta:\n        \"\"\"Get the duration of this Signal.\"\"\"\n        return datetime.timedelta(seconds=self.time[-1] - self.time[0])\n\n    def tindex(self, t: float) -&gt; int:\n        \"\"\"Get the sample index closest to time `t`.\"\"\"\n        return int((np.abs(self.time - t)).argmin())\n\n    def copy(self, new_name: Optional[str] = None) -&gt; \"Signal\":\n        \"\"\"Return a deep copy of this signal.\n\n        Args:\n            new_name: if not None, assign the copy this new name\n\n        Returns:\n            A copy of this Signal\n        \"\"\"\n        if new_name is None:\n            new_name = self.name\n        s = type(self)(new_name, self.signal.copy(), time=self.time.copy(), fs=self.fs, units=self.units)\n        s.marks.update(**self.marks)\n        return s\n\n    def __check_other_compatible(self, other: \"Signal\") -&gt; bool:\n        return self.nsamples == other.nsamples and self.nobs == self.nobs and self.fs == other.fs\n\n    def __add__(self, other: \"Signal\") -&gt; \"Signal\":\n        \"\"\"Add another signal to this signal, returning a new signal.\n\n        Args:\n            other: the other signal to be added to this signal\n\n        Return:\n            A new Signal with the addition result.\n        \"\"\"\n        assert self.__check_other_compatible(other)\n        s = self.copy()\n        s.signal += other.signal\n        return s\n\n    def __sub__(self, other: \"Signal\") -&gt; \"Signal\":\n        \"\"\"Subtract another signal from this signal, returning a new signal.\n\n        Args:\n            other: the other signal to be subtracted from this signal\n\n        Return:\n            A new Signal with the subtraction result.\n        \"\"\"\n        assert self.__check_other_compatible(other)\n        s = self.copy()\n        s.signal -= other.signal\n        return s\n\n    def __mul__(self, other: \"Signal\") -&gt; \"Signal\":\n        \"\"\"Multiply another signal against this signal, returning a new signal.\n\n        Args:\n            other: the other signal to be multiplied by this signal\n\n        Return:\n            A new Signal with the multiplication result.\n        \"\"\"\n        assert self.__check_other_compatible(other)\n        s = self.copy()\n        s.signal *= other.signal\n        return s\n\n    def __div__(self, other: \"Signal\") -&gt; \"Signal\":\n        \"\"\"Divide this signal by another signal, returning a new signal.\n\n        Args:\n            other: the other signal to divide this signal by\n\n        Return:\n            A new Signal with the division result.\n        \"\"\"\n        assert self.__check_other_compatible(other)\n        s = self.copy()\n        s.signal /= other.signal\n        return s\n\n    def aggregate(self, func: Union[str, np.ufunc, Callable[[np.ndarray], np.ndarray]]) -&gt; \"Signal\":\n        \"\"\"Aggregate this signal.\n\n        If there is only a single observation, that observation is returned unchanged, otherwise  observations will\n        be aggregated by `func` along axis=0.\n\n        Marks, units, and time will be propegated. The new signal will be named according to this signal, with `#{func_name}` appended.\n\n        Args:\n            func: string or callable that take a (nobs x nsample) array and returns a (nsample,) shaped array. If a string\n                will be interpreted as the name of a numpy function (e.x. mean, median, etc)\n\n        Returns:\n            aggregated `Signal`\n        \"\"\"\n        if self.nobs == 1:\n            return self  # maybe we should raise??\n\n        else:\n            f: Callable[[np.ndarray], np.ndarray]\n            if isinstance(func, str):\n                f = partial(cast(np.ufunc, getattr(np, func)), axis=0)\n                f_name = func\n\n            elif isinstance(func, np.ufunc):\n                f = partial(func, axis=0)\n                f_name = func.__name__\n\n            else:\n                f = func\n                f_name = func.__name__\n\n            s = Signal(f\"{self.name}#{f_name}\", f(self.signal), time=self.time, units=self.units)\n            s.marks.update(self.marks)\n            return s\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Get the signal data as a `pandas.DataFrame`.\n\n        Observations are across rows, and samples are across columns. Each sample colum is named with\n        the pattern `Y.{i+1}` where i is the sample index. This also implies 1-based indexing on the output.\n        \"\"\"\n        return pd.DataFrame(self.signal, columns=[f\"Y.{i+1}\" for i in range(self.nsamples)])\n\n    def describe(self, as_str: bool = False, prefix: str = \"\") -&gt; Union[str, None]:\n        \"\"\"Describe this Signal.\n\n        Args:\n            as_str: if True, return description as a string, otherwise print the description and return None\n            prefix: a string to prepend to each line of output\n\n        Returns:\n            `None` if `as_str` is `False`; if `as_str` is `True`, returns the description as a `str`\n        \"\"\"\n        buffer = f\"{prefix}{self.name}:\\n\"\n        buffer += f\"{prefix}    units = {self.units}\\n\"\n        buffer += f\"{prefix}    n_observations = {self.nobs}\\n\"\n        buffer += f\"{prefix}    n_samples = {self.nsamples}\\n\"\n        buffer += f\"{prefix}    duration = {self.duration}\\n\"\n        buffer += f\"{prefix}    sample_rate = {self.fs}\\n\"\n        buffer += f\"{prefix}    min|mean|max = {self.signal.min():.2f}|{self.signal.mean():.2f}|{self.signal.max():.2f}\\n\"\n        if len(self.marks) &gt; 0:\n            buffer += f\"{prefix}    marks ({len(self.marks)}):\\n\"\n            for k, v in self.marks.items():\n                buffer += f\"{prefix}        {datetime.timedelta(seconds=v)} {k}\\n\"\n\n        if as_str:\n            return buffer\n        else:\n            print(buffer)\n            return None\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.duration","title":"<code>duration</code>  <code>property</code>","text":"<p>Get the duration of this Signal.</p>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.nobs","title":"<code>nobs</code>  <code>property</code>","text":"<p>Get the number of observations for this Signal (i.e. number of trials).</p>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.nsamples","title":"<code>nsamples</code>  <code>property</code>","text":"<p>Get the number of samples for this Signal (i.e. length of signal).</p>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.__add__","title":"<code>__add__(other)</code>","text":"<p>Add another signal to this signal, returning a new signal.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Signal</code>)           \u2013            <p>the other signal to be added to this signal</p> </li> </ul> Return <p>A new Signal with the addition result.</p> Source code in <code>fptools/io/signal.py</code> <pre><code>def __add__(self, other: \"Signal\") -&gt; \"Signal\":\n    \"\"\"Add another signal to this signal, returning a new signal.\n\n    Args:\n        other: the other signal to be added to this signal\n\n    Return:\n        A new Signal with the addition result.\n    \"\"\"\n    assert self.__check_other_compatible(other)\n    s = self.copy()\n    s.signal += other.signal\n    return s\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.__div__","title":"<code>__div__(other)</code>","text":"<p>Divide this signal by another signal, returning a new signal.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Signal</code>)           \u2013            <p>the other signal to divide this signal by</p> </li> </ul> Return <p>A new Signal with the division result.</p> Source code in <code>fptools/io/signal.py</code> <pre><code>def __div__(self, other: \"Signal\") -&gt; \"Signal\":\n    \"\"\"Divide this signal by another signal, returning a new signal.\n\n    Args:\n        other: the other signal to divide this signal by\n\n    Return:\n        A new Signal with the division result.\n    \"\"\"\n    assert self.__check_other_compatible(other)\n    s = self.copy()\n    s.signal /= other.signal\n    return s\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.__init__","title":"<code>__init__(name, signal, time=None, fs=None, units='AU')</code>","text":"<p>Initialize a this Signal Object.</p> <p>At least one of <code>time</code> or <code>fs</code> must be provided. If <code>time</code> is provided, the sampling frequency (<code>fs</code>) will be estimated from <code>time</code>. If <code>fs</code> is provided, the sampled timepoints (<code>time</code>) will be estimated. If both are provided, the values will be checked against one another, and if they do not match, a ValueError will be raised.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of this signal</p> </li> <li> <code>signal</code>               (<code>ndarray</code>)           \u2013            <p>Array of signal values</p> </li> <li> <code>time</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>Array of sampled timepoints</p> </li> <li> <code>fs</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Sampling frequency, in Hz</p> </li> <li> <code>units</code>               (<code>str</code>, default:                   <code>'AU'</code> )           \u2013            <p>units of this signal</p> </li> </ul> Source code in <code>fptools/io/signal.py</code> <pre><code>def __init__(\n    self, name: str, signal: np.ndarray, time: Optional[np.ndarray] = None, fs: Optional[float] = None, units: str = \"AU\"\n) -&gt; None:\n    \"\"\"Initialize a this Signal Object.\n\n    At least one of `time` or `fs` must be provided. If `time` is provided, the sampling frequency (`fs`) will be estimated\n    from `time`. If `fs` is provided, the sampled timepoints (`time`) will be estimated. If both are provided, the values\n    will be checked against one another, and if they do not match, a ValueError will be raised.\n\n    Args:\n        name: Name of this signal\n        signal: Array of signal values\n        time: Array of sampled timepoints\n        fs: Sampling frequency, in Hz\n        units: units of this signal\n    \"\"\"\n    self.name: str = name\n    self.signal: np.ndarray = signal\n    self.units: str = units\n    self.marks: dict[str, float] = {}\n    self.time: np.ndarray\n    self.fs: float\n\n    if time is not None and fs is not None:\n        # both time and sampling frequency provided\n        self.time = time\n        self.fs = fs\n        # just do a sanity check that the two pieces of information make sense\n        if not np.isclose(self.fs, 1 / np.median(np.diff(time))):\n            raise ValueError(\n                f\"Both `time` and `fs` were provided, but they do not match!\\n  fs={fs}\\ntime={1 / np.median(np.diff(time))}\"\n            )\n\n    elif time is None and fs is not None:\n        # sampleing frequency is provided, infer time from fs\n        self.fs = fs\n        self.time = np.linspace(1, signal.shape[0], signal.shape[0]) / self.fs\n\n    elif fs is None and time is not None:\n        # time is provided, so lets estimate the sampling frequency\n        self.time = time\n        self.fs = 1 / np.median(np.diff(time))\n\n    else:\n        # neither time or sampling frequency provided, we need at least one!\n        raise ValueError(\"Both `time` and `fs` cannot be `None`, one must be supplied!\")\n\n    if not self.signal.shape[-1] == self.time.shape[0]:\n        raise ValueError(f\"Signal and time must have the same length! signal.shape={self.signal.shape}; time.shape={self.time.shape}\")\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.__mul__","title":"<code>__mul__(other)</code>","text":"<p>Multiply another signal against this signal, returning a new signal.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Signal</code>)           \u2013            <p>the other signal to be multiplied by this signal</p> </li> </ul> Return <p>A new Signal with the multiplication result.</p> Source code in <code>fptools/io/signal.py</code> <pre><code>def __mul__(self, other: \"Signal\") -&gt; \"Signal\":\n    \"\"\"Multiply another signal against this signal, returning a new signal.\n\n    Args:\n        other: the other signal to be multiplied by this signal\n\n    Return:\n        A new Signal with the multiplication result.\n    \"\"\"\n    assert self.__check_other_compatible(other)\n    s = self.copy()\n    s.signal *= other.signal\n    return s\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.__sub__","title":"<code>__sub__(other)</code>","text":"<p>Subtract another signal from this signal, returning a new signal.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Signal</code>)           \u2013            <p>the other signal to be subtracted from this signal</p> </li> </ul> Return <p>A new Signal with the subtraction result.</p> Source code in <code>fptools/io/signal.py</code> <pre><code>def __sub__(self, other: \"Signal\") -&gt; \"Signal\":\n    \"\"\"Subtract another signal from this signal, returning a new signal.\n\n    Args:\n        other: the other signal to be subtracted from this signal\n\n    Return:\n        A new Signal with the subtraction result.\n    \"\"\"\n    assert self.__check_other_compatible(other)\n    s = self.copy()\n    s.signal -= other.signal\n    return s\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.aggregate","title":"<code>aggregate(func)</code>","text":"<p>Aggregate this signal.</p> <p>If there is only a single observation, that observation is returned unchanged, otherwise  observations will be aggregated by <code>func</code> along axis=0.</p> <p>Marks, units, and time will be propegated. The new signal will be named according to this signal, with <code>#{func_name}</code> appended.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Union[str, ufunc, Callable[[ndarray], ndarray]]</code>)           \u2013            <p>string or callable that take a (nobs x nsample) array and returns a (nsample,) shaped array. If a string will be interpreted as the name of a numpy function (e.x. mean, median, etc)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Signal</code>           \u2013            <p>aggregated <code>Signal</code></p> </li> </ul> Source code in <code>fptools/io/signal.py</code> <pre><code>def aggregate(self, func: Union[str, np.ufunc, Callable[[np.ndarray], np.ndarray]]) -&gt; \"Signal\":\n    \"\"\"Aggregate this signal.\n\n    If there is only a single observation, that observation is returned unchanged, otherwise  observations will\n    be aggregated by `func` along axis=0.\n\n    Marks, units, and time will be propegated. The new signal will be named according to this signal, with `#{func_name}` appended.\n\n    Args:\n        func: string or callable that take a (nobs x nsample) array and returns a (nsample,) shaped array. If a string\n            will be interpreted as the name of a numpy function (e.x. mean, median, etc)\n\n    Returns:\n        aggregated `Signal`\n    \"\"\"\n    if self.nobs == 1:\n        return self  # maybe we should raise??\n\n    else:\n        f: Callable[[np.ndarray], np.ndarray]\n        if isinstance(func, str):\n            f = partial(cast(np.ufunc, getattr(np, func)), axis=0)\n            f_name = func\n\n        elif isinstance(func, np.ufunc):\n            f = partial(func, axis=0)\n            f_name = func.__name__\n\n        else:\n            f = func\n            f_name = func.__name__\n\n        s = Signal(f\"{self.name}#{f_name}\", f(self.signal), time=self.time, units=self.units)\n        s.marks.update(self.marks)\n        return s\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.copy","title":"<code>copy(new_name=None)</code>","text":"<p>Return a deep copy of this signal.</p> <p>Parameters:</p> <ul> <li> <code>new_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>if not None, assign the copy this new name</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Signal</code>           \u2013            <p>A copy of this Signal</p> </li> </ul> Source code in <code>fptools/io/signal.py</code> <pre><code>def copy(self, new_name: Optional[str] = None) -&gt; \"Signal\":\n    \"\"\"Return a deep copy of this signal.\n\n    Args:\n        new_name: if not None, assign the copy this new name\n\n    Returns:\n        A copy of this Signal\n    \"\"\"\n    if new_name is None:\n        new_name = self.name\n    s = type(self)(new_name, self.signal.copy(), time=self.time.copy(), fs=self.fs, units=self.units)\n    s.marks.update(**self.marks)\n    return s\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.describe","title":"<code>describe(as_str=False, prefix='')</code>","text":"<p>Describe this Signal.</p> <p>Parameters:</p> <ul> <li> <code>as_str</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if True, return description as a string, otherwise print the description and return None</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>a string to prepend to each line of output</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, None]</code>           \u2013            <p><code>None</code> if <code>as_str</code> is <code>False</code>; if <code>as_str</code> is <code>True</code>, returns the description as a <code>str</code></p> </li> </ul> Source code in <code>fptools/io/signal.py</code> <pre><code>def describe(self, as_str: bool = False, prefix: str = \"\") -&gt; Union[str, None]:\n    \"\"\"Describe this Signal.\n\n    Args:\n        as_str: if True, return description as a string, otherwise print the description and return None\n        prefix: a string to prepend to each line of output\n\n    Returns:\n        `None` if `as_str` is `False`; if `as_str` is `True`, returns the description as a `str`\n    \"\"\"\n    buffer = f\"{prefix}{self.name}:\\n\"\n    buffer += f\"{prefix}    units = {self.units}\\n\"\n    buffer += f\"{prefix}    n_observations = {self.nobs}\\n\"\n    buffer += f\"{prefix}    n_samples = {self.nsamples}\\n\"\n    buffer += f\"{prefix}    duration = {self.duration}\\n\"\n    buffer += f\"{prefix}    sample_rate = {self.fs}\\n\"\n    buffer += f\"{prefix}    min|mean|max = {self.signal.min():.2f}|{self.signal.mean():.2f}|{self.signal.max():.2f}\\n\"\n    if len(self.marks) &gt; 0:\n        buffer += f\"{prefix}    marks ({len(self.marks)}):\\n\"\n        for k, v in self.marks.items():\n            buffer += f\"{prefix}        {datetime.timedelta(seconds=v)} {k}\\n\"\n\n    if as_str:\n        return buffer\n    else:\n        print(buffer)\n        return None\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.tindex","title":"<code>tindex(t)</code>","text":"<p>Get the sample index closest to time <code>t</code>.</p> Source code in <code>fptools/io/signal.py</code> <pre><code>def tindex(self, t: float) -&gt; int:\n    \"\"\"Get the sample index closest to time `t`.\"\"\"\n    return int((np.abs(self.time - t)).argmin())\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Get the signal data as a <code>pandas.DataFrame</code>.</p> <p>Observations are across rows, and samples are across columns. Each sample colum is named with the pattern <code>Y.{i+1}</code> where i is the sample index. This also implies 1-based indexing on the output.</p> Source code in <code>fptools/io/signal.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Get the signal data as a `pandas.DataFrame`.\n\n    Observations are across rows, and samples are across columns. Each sample colum is named with\n    the pattern `Y.{i+1}` where i is the sample index. This also implies 1-based indexing on the output.\n    \"\"\"\n    return pd.DataFrame(self.signal, columns=[f\"Y.{i+1}\" for i in range(self.nsamples)])\n</code></pre>"},{"location":"reference/fptools/io/tdt/","title":"tdt","text":""},{"location":"reference/fptools/io/tdt/#fptools.io.tdt","title":"<code>fptools.io.tdt</code>","text":"<p>Classes:</p> <ul> <li> <code>Preprocessor</code>           \u2013            </li> </ul> <p>Functions:</p> <ul> <li> <code>load_data</code>             \u2013              <p>Load blocks from <code>tank_path</code> and return a <code>SessionCollection</code>.</p> </li> <li> <code>load_manifest</code>             \u2013              <p>Load a manifest file, accepting most common tabular formats.</p> </li> </ul>"},{"location":"reference/fptools/io/tdt/#fptools.io.tdt.Preprocessor","title":"<code>Preprocessor</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Preprocessor protocol.</p> </li> </ul> Source code in <code>fptools/io/tdt.py</code> <pre><code>class Preprocessor(Protocol):\n    def __call__(self, session: Session, block: Any, signal_map: list[SignalMapping], **kwargs) -&gt; Session:\n        \"\"\"Preprocessor protocol.\n\n        Args:\n            session: the session to operate upon\n            block: data, result of `tdt.load_block()`\n            signal_map: mapping of signal information\n            **kwargs: additional kwargs a preprocessor might need\n\n        Returns:\n            Session object with preprocessed data added.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/fptools/io/tdt/#fptools.io.tdt.Preprocessor.__call__","title":"<code>__call__(session, block, signal_map, **kwargs)</code>","text":"<p>Preprocessor protocol.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to operate upon</p> </li> <li> <code>block</code>               (<code>Any</code>)           \u2013            <p>data, result of <code>tdt.load_block()</code></p> </li> <li> <code>signal_map</code>               (<code>list[SignalMapping]</code>)           \u2013            <p>mapping of signal information</p> </li> <li> <code>**kwargs</code>           \u2013            <p>additional kwargs a preprocessor might need</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session object with preprocessed data added.</p> </li> </ul> Source code in <code>fptools/io/tdt.py</code> <pre><code>def __call__(self, session: Session, block: Any, signal_map: list[SignalMapping], **kwargs) -&gt; Session:\n    \"\"\"Preprocessor protocol.\n\n    Args:\n        session: the session to operate upon\n        block: data, result of `tdt.load_block()`\n        signal_map: mapping of signal information\n        **kwargs: additional kwargs a preprocessor might need\n\n    Returns:\n        Session object with preprocessed data added.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fptools/io/tdt/#fptools.io.tdt.__default_preprocess","title":"<code>__default_preprocess(session, block, signal_map)</code>","text":"<p>A default preprocessing pipeline that doesnt do much of anything.</p> <p>Only adds each stream to the session as  a <code>Signal</code></p> Source code in <code>fptools/io/tdt.py</code> <pre><code>def __default_preprocess(session: Session, block: Any, signal_map: list[SignalMapping]) -&gt; Session:\n    \"\"\"A default preprocessing pipeline that doesnt do much of anything.\n\n    Only adds each stream to the session as  a `Signal`\n    \"\"\"\n    for sm in signal_map:\n        stream = block.streams[sm[\"tdt_name\"]]\n        session.add_signal(Signal(sm[\"dest_name\"], stream.data, fs=stream.fs, units=\"mV\"))\n\n    return session\n</code></pre>"},{"location":"reference/fptools/io/tdt/#fptools.io.tdt.load_data","title":"<code>load_data(tank_path, signal_map, manifest_path=None, max_workers=None, preprocess=None, cache=True, cache_dir='cache', **kwargs)</code>","text":"<p>Load blocks from <code>tank_path</code> and return a <code>SessionCollection</code>.</p> <p>Loading will happen in parallel, split across <code>max_workers</code> worker processes.</p> <p>For quicker future loading, results may be cached. Cacheing is controlled by the <code>cache</code> parameter, and the location of cached files is controlled by the <code>cache_dir</code> parameter.</p> <p>You can specify a manifest (in TSV, CSV or XLSX formats) containing additional metadata to be injected into the loaded data. This manifest should have at minimum one column with header <code>blockname</code> containing each block's name. You may include any other arbitrary data columns you may wish. One special column name is <code>exclude</code> which should contain boolean <code>True</code> or <code>False</code>. If a block is marked with <code>True</code> in this column, then the block will not be loaded or returned in the resulting <code>SessionCollection</code>.</p> <p>You can also specify a preprocess routine to be applied to each block prior to being returned via the <code>preprocess</code> parameter. This should be a callable taking a <code>Session</code> as the first parameter, a TDT struct object (the return value from calling <code>tdt.read_block()</code>) as the second parameter, and any additional kwargs necessary. Any **kwargs passed to <code>load_data()</code> will be passed to the preprocess callable. Your callable preprocess routine should attach any data to the passed <code>Session</code> object and return this <code>Session</code> object as it's sole return value. For example preprocessing routines, please see the implementations in the <code>tdt.preprocess.pipelines</code> module.</p> <p>Parameters:</p> <ul> <li> <code>tank_path</code>               (<code>str</code>)           \u2013            <p>path that will be recursively searched for blocks</p> </li> <li> <code>manifest_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>if provided, path to metadata in a tabular format, indexed with <code>blockname</code>. See above for more details</p> </li> <li> <code>max_workers</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>number of workers in the process pool for loading blocks. If None, defaults to the number of CPUs on the machine.</p> </li> <li> <code>preprocess</code>               (<code>Optional[Preprocessor]</code>, default:                   <code>None</code> )           \u2013            <p>preprocess routine to run on the data. See above for more details.</p> </li> <li> <code>cache</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, results will be cached for future use, or results will be loaded from the cache.</p> </li> <li> <code>cache_dir</code>               (<code>str</code>, default:                   <code>'cache'</code> )           \u2013            <p>path to the cache</p> </li> <li> <code>**kwargs</code>           \u2013            <p>additional keyword arguments to pass to the <code>preprocess</code> callable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SessionCollection</code>           \u2013            <p><code>SessionCollection</code> containing loaded data</p> </li> </ul> Source code in <code>fptools/io/tdt.py</code> <pre><code>def load_data(\n    tank_path: str,\n    signal_map: list[SignalMapping],\n    manifest_path: Optional[str] = None,\n    max_workers: Optional[int] = None,\n    preprocess: Optional[Preprocessor] = None,\n    cache: bool = True,\n    cache_dir: str = \"cache\",\n    **kwargs,\n) -&gt; SessionCollection:\n    \"\"\"Load blocks from `tank_path` and return a `SessionCollection`.\n\n    Loading will happen in parallel, split across `max_workers` worker processes.\n\n    For quicker future loading, results may be cached. Cacheing is controlled by the `cache` parameter, and the location of cached\n    files is controlled by the `cache_dir` parameter.\n\n    You can specify a manifest (in TSV, CSV or XLSX formats) containing additional metadata to be injected into the loaded data.\n    This manifest should have at minimum one column with header `blockname` containing each block's name. You may include any other arbitrary\n    data columns you may wish. One special column name is `exclude` which should contain boolean `True` or `False`. If a block\n    is marked with `True` in this column, then the block will not be loaded or returned in the resulting `SessionCollection`.\n\n    You can also specify a preprocess routine to be applied to each block prior to being returned via the `preprocess` parameter. This\n    should be a callable taking a `Session` as the first parameter, a TDT struct object (the return value from calling `tdt.read_block()`)\n    as the second parameter, and any additional kwargs necessary. Any **kwargs passed to `load_data()` will be passed to the preprocess callable.\n    Your callable preprocess routine should attach any data to the passed `Session` object and return this `Session` object as it's sole return value.\n    For example preprocessing routines, please see the implementations in the `tdt.preprocess.pipelines` module.\n\n    Args:\n        tank_path: path that will be recursively searched for blocks\n        manifest_path: if provided, path to metadata in a tabular format, indexed with `blockname`. See above for more details\n        max_workers: number of workers in the process pool for loading blocks. If None, defaults to the number of CPUs on the machine.\n        preprocess: preprocess routine to run on the data. See above for more details.\n        cache: If `True`, results will be cached for future use, or results will be loaded from the cache.\n        cache_dir: path to the cache\n        **kwargs: additional keyword arguments to pass to the `preprocess` callable.\n\n    Returns:\n        `SessionCollection` containing loaded data\n    \"\"\"\n    has_manifest = False\n    if manifest_path is not None:\n        manifest = load_manifest(manifest_path)\n        has_manifest = True\n\n    if cache:\n        os.makedirs(cache_dir, exist_ok=True)\n\n    futures: dict[concurrent.futures.Future[Session], str] = {}\n    sessions = SessionCollection()\n    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n        # collect common worker args in one place\n        worker_args = {\"preprocess\": preprocess, \"cache\": cache, \"cache_dir\": cache_dir, **kwargs}\n        for p in glob.glob(os.path.join(tank_path, \"**/*.Tbk\"), recursive=True):\n            block_dir = os.path.dirname(p)\n            block_name = os.path.basename(block_dir)\n\n            if has_manifest:\n                try:\n                    block_meta = manifest.loc[block_name]\n                except KeyError:\n                    # this block is not listed in the manifest! Err on the side of caution and exclude the block\n                    tqdm.write(f\"WARNING: Excluding block {block_name} because it is not listed in the manifest!!\")\n                    continue\n\n                # possibly exclude the block, if flagged in the manifest\n                if \"exclude\" in block_meta and block_meta[\"exclude\"]:\n                    tqdm.write(f\"Excluding block {block_name} due to manifest exclude flag\")\n                    continue\n\n            # submit the task to the pool\n            f = executor.submit(__load_block, block_dir, signal_map, **worker_args)\n            futures[f] = block_name\n\n        # collect completed tasks\n        for f in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n            try:\n                s = f.result()\n                if has_manifest:\n                    s.metadata.update(manifest.loc[futures[f]].to_dict())\n                sessions.append(s)\n            except Exception as e:\n                tqdm.write(\n                    f'Encountered problem loading data at \"{futures[f]}\":\\n{traceback.format_exc()}\\nData will be omitted from the final collection!\\n'\n                )\n                pass\n\n    return sessions\n</code></pre>"},{"location":"reference/fptools/io/tdt/#fptools.io.tdt.load_manifest","title":"<code>load_manifest(path)</code>","text":"<p>Load a manifest file, accepting most common tabular formats.</p> <p>Expects to find a column named <code>blockname</code>.</p> Source code in <code>fptools/io/tdt.py</code> <pre><code>def load_manifest(path: str) -&gt; pd.DataFrame:\n    \"\"\"Load a manifest file, accepting most common tabular formats.\n\n    Expects to find a column named `blockname`.\n    \"\"\"\n    ext = os.path.splitext(path)[1]\n\n    df: pd.DataFrame\n    if ext == \".tsv\":\n        df = pd.read_csv(path, sep=\"\\t\")\n    elif ext == \".csv\":\n        df = pd.read_csv(path)\n    elif ext == \".xlsx\":\n        df = pd.read_excel(path)\n    else:\n        raise ValueError(\"did not understand format\")\n\n    if \"blockname\" in df.columns:\n        df.set_index(\"blockname\", inplace=True)\n\n    return df\n</code></pre>"},{"location":"reference/fptools/measure/","title":"measure","text":""},{"location":"reference/fptools/measure/#fptools.measure","title":"<code>fptools.measure</code>","text":"<p>Modules:</p> <ul> <li> <code>peaks</code>           \u2013            </li> <li> <code>signal_collector</code>           \u2013            </li> <li> <code>snr</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/measure/peaks/","title":"peaks","text":""},{"location":"reference/fptools/measure/peaks/#fptools.measure.peaks","title":"<code>fptools.measure.peaks</code>","text":"<p>Functions:</p> <ul> <li> <code>measure_peaks</code>             \u2013              <p>Measure peaks within a signal.</p> </li> </ul>"},{"location":"reference/fptools/measure/peaks/#fptools.measure.peaks.measure_peaks","title":"<code>measure_peaks(sessions, signal, include_meta='all', **kwargs)</code>","text":"<p>Measure peaks within a signal.</p> <p>Parameters:</p> <ul> <li> <code>sessions</code>               (<code>SessionCollection</code>)           \u2013            <p>collection of sessions to work on</p> </li> <li> <code>signal</code>               (<code>str</code>)           \u2013            <p>name of the signal to measure</p> </li> <li> <code>include_meta</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>metadata fields to include in the final output. Special string \"all\" will include all metadata fields</p> </li> <li> <code>**kwargs</code>           \u2013            <p>additional kwargs to pass to <code>scipy.signal.find_peaks()</code></p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pandas <code>DataFrame</code> with peak measurements.</p> </li> </ul> Source code in <code>fptools/measure/peaks.py</code> <pre><code>def measure_peaks(sessions: SessionCollection, signal: str, include_meta: FieldList = \"all\", **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Measure peaks within a signal.\n\n    Args:\n        sessions: collection of sessions to work on\n        signal: name of the signal to measure\n        include_meta: metadata fields to include in the final output. Special string \"all\" will include all metadata fields\n        **kwargs: additional kwargs to pass to `scipy.signal.find_peaks()`\n\n    Returns:\n        pandas `DataFrame` with peak measurements.\n    \"\"\"\n    detection_params = {\"prominence\": (None, None), \"distance\": 10000, \"height\": (None, None)}\n    detection_params.update(**kwargs)\n    peak_data = []\n    for session in sessions:\n        # determine metadata fileds to include\n        if include_meta == \"all\":\n            meta = session.metadata\n        else:\n            meta = {k: v for k, v in session.metadata.items() if k in include_meta}\n\n        t = session.signals[signal].time\n        sig = np.atleast_2d(session.signals[signal].signal)\n        for i in range(sig.shape[0]):\n            peaks, props = scipy.signal.find_peaks(sig[i, :], **detection_params)\n            if len(peaks) &gt; 1:\n                print(\"found more than one peak!!\")\n\n            peak_data.append(\n                {\n                    **meta,\n                    \"trial\": i,\n                    \"peak_i\": peaks[0],\n                    \"peak_t\": t[peaks[0]],\n                    \"height\": props[\"peak_heights\"][0],\n                    \"prominence\": props[\"prominences\"][0],\n                    \"auc\": np.abs(metrics.auc(t, sig[i])),\n                }\n            )\n\n    peak_data = pd.DataFrame(peak_data)\n    return peak_data\n</code></pre>"},{"location":"reference/fptools/measure/signal_collector/","title":"signal_collector","text":""},{"location":"reference/fptools/measure/signal_collector/#fptools.measure.signal_collector","title":"<code>fptools.measure.signal_collector</code>","text":"<p>Functions:</p> <ul> <li> <code>collect_signals</code>             \u2013              <p>Collect a signal from a session around an event.</p> </li> <li> <code>collect_signals_2event</code>             \u2013              <p>Collect a signal from a session around two events.</p> </li> </ul>"},{"location":"reference/fptools/measure/signal_collector/#fptools.measure.signal_collector.collect_signals","title":"<code>collect_signals(session, event, signal, start=-1.0, stop=3.0, out_name=None)</code>","text":"<p>Collect a signal from a session around an event.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the Session to operate on</p> </li> <li> <code>event</code>               (<code>str</code>)           \u2013            <p>the name of the event to use</p> </li> <li> <code>signal</code>               (<code>str</code>)           \u2013            <p>the name of the signal to collect</p> </li> <li> <code>start</code>               (<code>float</code>, default:                   <code>-1.0</code> )           \u2013            <p>start of the collection interval, in seconds, relative to each event. Negative values imply prior to event, positive values imply after the event</p> </li> <li> <code>stop</code>               (<code>float</code>, default:                   <code>3.0</code> )           \u2013            <p>end of the collection interval, in seconds, relative to each event. Negative values imply prior to event, positive values imply after the event</p> </li> <li> <code>out_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>if not None, the name of the returned <code>Signal</code> object. Otherwise, the new <code>Signal</code> object's name will be generated as <code>{signal}@{event}</code></p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Signal</code>           \u2013            <p>the collected Signal</p> </li> </ul> Source code in <code>fptools/measure/signal_collector.py</code> <pre><code>def collect_signals(\n    session: Session, event: str, signal: str, start: float = -1.0, stop: float = 3.0, out_name: Optional[str] = None\n) -&gt; Signal:\n    \"\"\"Collect a signal from a session around an event.\n\n    Args:\n        session: the Session to operate on\n        event: the name of the event to use\n        signal: the name of the signal to collect\n        start: start of the collection interval, in seconds, relative to each event. Negative values imply prior to event, positive values imply after the event\n        stop: end of the collection interval, in seconds, relative to each event. Negative values imply prior to event, positive values imply after the event\n        out_name: if not None, the name of the returned `Signal` object. Otherwise, the new `Signal` object's name will be generated as `{signal}@{event}`\n\n    Returns:\n        the collected Signal\n    \"\"\"\n    assert start &lt; stop\n\n    sig = session.signals[signal]\n    events = session.epocs[event]\n\n    n_samples = int(np.rint((stop - start) * sig.fs))\n    offest = int(np.rint(start * sig.fs))\n    new_time = fs2t(sig.fs, n_samples) + start\n\n    accum = np.zeros_like(sig.signal, shape=(events.shape[0], n_samples))\n    padding = abs(offest) + n_samples\n    padded_signal = np.pad(sig.signal, (padding, padding), mode=\"constant\", constant_values=0)\n    for ei, evt in enumerate(events):\n        event_idx = sig.tindex(evt) + padding  # add padding\n        accum[ei, :] = padded_signal[(event_idx + offest) : (event_idx + offest + n_samples)]\n\n    if out_name is None:\n        out_name = f\"{signal}@{event}\"\n\n    s = Signal(out_name, accum, time=new_time, units=sig.units)\n    s.marks[event] = 0\n    return s\n</code></pre>"},{"location":"reference/fptools/measure/signal_collector/#fptools.measure.signal_collector.collect_signals_2event","title":"<code>collect_signals_2event(session, event1, event2, signal, pre=2.0, inter=2.0, post=2.0, out_name=None)</code>","text":"<p>Collect a signal from a session around two events.</p> <p>Collects a fixed amount of time before event1 and after event2. The \"real\" time between event1 and event2 is scaled to a \"meta\" time specified by <code>inter</code>. This is done by resampling the inter-event time using linear interpolation.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the Session to operate on</p> </li> <li> <code>event1</code>               (<code>str</code>)           \u2013            <p>the name of the first event to use</p> </li> <li> <code>event2</code>               (<code>str</code>)           \u2013            <p>the name of the second event to use</p> </li> <li> <code>signal</code>               (<code>str</code>)           \u2013            <p>the name of the signal to collect</p> </li> <li> <code>pre</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>amount of time, in seconds, to collect prior to each event</p> </li> <li> <code>inter</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>amouont of \"meta\" time, in seconds, to collect between events</p> </li> <li> <code>post</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>amount of time, in seconds, to collect after each event</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Signal</code>           \u2013            <p>the collected Signal</p> </li> </ul> Source code in <code>fptools/measure/signal_collector.py</code> <pre><code>def collect_signals_2event(\n    session: Session,\n    event1: str,\n    event2: str,\n    signal: str,\n    pre: float = 2.0,\n    inter: float = 2.0,\n    post: float = 2.0,\n    out_name: Optional[str] = None,\n) -&gt; Signal:\n    \"\"\"Collect a signal from a session around two events.\n\n    Collects a fixed amount of time before event1 and after event2. The \"real\" time between event1 and event2\n    is scaled to a \"meta\" time specified by `inter`. This is done by resampling the inter-event time using\n    linear interpolation.\n\n    Args:\n        session: the Session to operate on\n        event1: the name of the first event to use\n        event2: the name of the second event to use\n        signal: the name of the signal to collect\n        pre: amount of time, in seconds, to collect prior to each event\n        inter: amouont of \"meta\" time, in seconds, to collect between events\n        post: amount of time, in seconds, to collect after each event\n\n    Returns:\n        the collected Signal\n    \"\"\"\n    # unpack arguments\n    sig = session.signals[signal]\n    events_1 = session.epocs[event1]\n    events_2 = session.epocs[event2]\n    # in a rare case, a stray event2 before event1, this will prune those\n    # not sure if it's the best idea to do this............\n    events_2 = events_2[events_2 &gt; events_1.min()]\n    # assert len(events_1) == len(events_2)\n\n    # calculate index offsets etc\n    pre_idxs = int(np.rint(pre * sig.fs))\n    inter_idxs = int(np.rint(inter * sig.fs))\n    post_idxs = int(np.rint(post * sig.fs))\n    n_samples = pre_idxs + inter_idxs + post_idxs\n    new_time = fs2t(sig.fs, n_samples) - pre\n\n    # destination slices in the final signal\n    slice1 = slice(0, pre_idxs)\n    slice2 = slice(pre_idxs, pre_idxs + inter_idxs)\n    slice3 = slice(pre_idxs + inter_idxs, pre_idxs + inter_idxs + post_idxs)\n\n    accum = np.zeros_like(sig.signal, shape=(events_1.shape[0], n_samples))\n    padded_signal = np.pad(sig.signal, (pre_idxs, post_idxs), mode=\"constant\", constant_values=0)\n    for ei, (evt1, evt2) in enumerate(zip(events_1, events_2)):\n        event1_idx = sig.tindex(evt1)\n        event2_idx = sig.tindex(evt2)\n\n        # collect the first third (pre to event1)\n        accum[ei, slice1] = padded_signal[event1_idx : (event1_idx + pre_idxs)]\n\n        # collect the second third (event1 to event2)\n        # idea here is train a scipy.interpolate.interp1d() object with our real data\n        # and then resample `inter_idxs` number of points from `inter_time`\n        inter_time = sig.time[event1_idx:event2_idx]\n        inter_sig = padded_signal[(event1_idx + pre_idxs) : (event2_idx + pre_idxs)]\n        inter_source_intp = scipy.interpolate.interp1d(inter_time, inter_sig)\n        inter_time_query = np.linspace(inter_time[0], inter_time[-1], inter_idxs, endpoint=True)\n        accum[ei, slice2] = inter_source_intp(inter_time_query)\n\n        # collect the final third (event 2 to post)\n        accum[ei, slice3] = padded_signal[(event2_idx + pre_idxs) : (event2_idx + pre_idxs + post_idxs)]\n\n    # construct the new signal object, and copy over proper metadata and add marks\n    if out_name is None:\n        out_name = f\"{signal}@{event1}&gt;{event2}\"\n\n    s = Signal(out_name, accum, time=new_time, units=sig.units)\n    s.marks[event1] = 0\n    s.marks[event2] = inter\n\n    # return the collected signal\n    return s\n</code></pre>"},{"location":"reference/fptools/measure/snr/","title":"snr","text":""},{"location":"reference/fptools/measure/snr/#fptools.measure.snr","title":"<code>fptools.measure.snr</code>","text":"<p>Functions:</p> <ul> <li> <code>measure_snr_event</code>             \u2013              <p>Measure Signal to Noise ratio (SNR) in signals surrounding events.</p> </li> <li> <code>measure_snr_overall</code>             \u2013              <p>Measure Signal to Noise ratio (SNR) of the overall stream.</p> </li> </ul>"},{"location":"reference/fptools/measure/snr/#fptools.measure.snr.measure_snr_event","title":"<code>measure_snr_event(sessions, signals, events, noise_range, signal_range, include_meta='all')</code>","text":"<p>Measure Signal to Noise ratio (SNR) in signals surrounding events.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>Union[str, list[str]]</code>)           \u2013            <p>one or more signals to measure</p> </li> <li> <code>events</code>               (<code>Union[str, list[str]]</code>)           \u2013            <p>one or more events at which to measure signals</p> </li> <li> <code>noise_range</code>               (<code>Union[tuple[float, float], list[tuple[float, float]]]</code>)           \u2013            <p>tuple(s) of start/stop times relative to the event to consider as noise</p> </li> <li> <code>signal_range</code>               (<code>Union[tuple[float, float], list[tuple[float, float]]]</code>)           \u2013            <p>tuple(s) of start/stop times relative to the event to consider as signal</p> </li> <li> <code>include_meta</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>metadata fields to include in output. if \"all\" then all fields will be included</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pandas.DataFrame with collected SNR data</p> </li> </ul> Source code in <code>fptools/measure/snr.py</code> <pre><code>def measure_snr_event(\n    sessions: SessionCollection,\n    signals: Union[str, list[str]],\n    events: Union[str, list[str]],\n    noise_range: Union[tuple[float, float], list[tuple[float, float]]],\n    signal_range: Union[tuple[float, float], list[tuple[float, float]]],\n    include_meta: FieldList = \"all\",\n) -&gt; pd.DataFrame:\n    \"\"\"Measure Signal to Noise ratio (SNR) in signals surrounding events.\n\n    Args:\n        signals: one or more signals to measure\n        events: one or more events at which to measure signals\n        noise_range: tuple(s) of start/stop times relative to the event to consider as noise\n        signal_range: tuple(s) of start/stop times relative to the event to consider as signal\n        include_meta: metadata fields to include in output. if \"all\" then all fields will be included\n\n    Returns:\n        pandas.DataFrame with collected SNR data\n    \"\"\"\n    sigs_to_measure = []\n    if isinstance(signals, str):\n        sigs_to_measure.append(signals)\n    else:\n        sigs_to_measure.extend(signals)\n\n    events_to_measure = []\n    if isinstance(events, str):\n        events_to_measure.append(events)\n    else:\n        events_to_measure.extend(events)\n\n    nrs: list[tuple[float, float]] = []\n    if len(np.array(noise_range).shape) == 1:\n        nrs = [cast(tuple, noise_range)] * len(events_to_measure)\n    else:\n        nrs.extend(cast(list, noise_range))\n\n    srs: list[tuple[float, float]] = []\n    if len(np.array(signal_range).shape) == 1:\n        srs = [cast(tuple, signal_range)] * len(events_to_measure)\n    else:\n        srs.extend(cast(list, signal_range))\n\n    data = []\n    for session in sessions:\n        # determine metadata fileds to include\n        if include_meta == \"all\":\n            meta = session.metadata\n        else:\n            meta = {k: v for k, v in session.metadata.items() if k in include_meta}\n\n        for sig_name in sigs_to_measure:\n            for ei, event_name in enumerate(events_to_measure):\n                n = collect_signals(session, event_name, sig_name, start=nrs[ei][0], stop=nrs[ei][1])\n                s = collect_signals(session, event_name, sig_name, start=srs[ei][0], stop=srs[ei][1])\n                data.append(\n                    {\n                        **meta,\n                        \"signal\": sig_name,\n                        \"snr\": np.median((s.signal.max(axis=1) - s.signal.min(axis=1)) ** 2 / n.signal.std(axis=1) ** 2),\n                    }\n                )\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"reference/fptools/measure/snr/#fptools.measure.snr.measure_snr_overall","title":"<code>measure_snr_overall(sessions, signals, include_meta='all')</code>","text":"<p>Measure Signal to Noise ratio (SNR) of the overall stream.</p> <p>SNR defined as log10(mean(signal)^2 / std(signal)^2), and expressed in decibels (dB)</p> <p>Parameters:</p> <ul> <li> <code>sessions</code>               (<code>SessionCollection</code>)           \u2013            <p>sessions to pull signals from</p> </li> <li> <code>signals</code>               (<code>Union[str, list[str]]</code>)           \u2013            <p>one or more signal names to operate on\"</p> </li> <li> <code>include_meta</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>metadata fields to include in the resulting dataframe. If \"all\", include all metadata fields</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pandas.DataFrame with calculated SNR</p> </li> </ul> Source code in <code>fptools/measure/snr.py</code> <pre><code>def measure_snr_overall(sessions: SessionCollection, signals: Union[str, list[str]], include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n    \"\"\"Measure Signal to Noise ratio (SNR) of the overall stream.\n\n    SNR defined as log10(mean(signal)^2 / std(signal)^2), and expressed in decibels (dB)\n\n    Args:\n        sessions: sessions to pull signals from\n        signals: one or more signal names to operate on\"\n        include_meta: metadata fields to include in the resulting dataframe. If \"all\", include all metadata fields\n\n    Returns:\n        pandas.DataFrame with calculated SNR\n    \"\"\"\n    sigs_to_measure = []\n    if isinstance(signals, str):\n        sigs_to_measure.append(signals)\n    else:\n        sigs_to_measure.extend(signals)\n\n    data = []\n    for session in sessions:\n        # determine metadata fileds to include\n        if include_meta == \"all\":\n            meta = session.metadata\n        else:\n            meta = {k: v for k, v in session.metadata.items() if k in include_meta}\n\n        for sig_name in sigs_to_measure:\n            sig = session.signals[sig_name]\n            data.append(\n                {\n                    **meta,\n                    \"signal\": sig_name,\n                    \"snr\": np.log10(np.power(np.mean(sig.signal), 2) / np.power(np.std(sig.signal), 2)),\n                }\n            )\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"reference/fptools/preprocess/","title":"preprocess","text":""},{"location":"reference/fptools/preprocess/#fptools.preprocess","title":"<code>fptools.preprocess</code>","text":"<p>Modules:</p> <ul> <li> <code>lib</code>           \u2013            </li> <li> <code>pipelines</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/preprocess/lib/","title":"lib","text":""},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib","title":"<code>fptools.preprocess.lib</code>","text":"<p>Functions:</p> <ul> <li> <code>are_arrays_same_length</code>             \u2013              <p>Check if all arrays are the same shape in the first axis.</p> </li> <li> <code>detrend_double_exponential</code>             \u2013              <p>Detrend a signal by fitting and subtracting a double exponential curve to the data.</p> </li> <li> <code>double_exponential</code>             \u2013              <p>Compute a double exponential function with constant offset.</p> </li> <li> <code>downsample</code>             \u2013              <p>Downsample one or more signals by factor across windows of size <code>window</code>.</p> </li> <li> <code>estimate_motion</code>             \u2013              <p>Estimate the contribution of motion artifacts in <code>signal</code>.</p> </li> <li> <code>fit_double_exponential</code>             \u2013              <p>Run the fitting procedure for a double exponential curve.</p> </li> <li> <code>fs2t</code>             \u2013              <p>Generate a time array given a sample frequency and number of samples.</p> </li> <li> <code>lowpass_filter</code>             \u2013              <p>zero-phase lowpass filter a signal.</p> </li> <li> <code>t2fs</code>             \u2013              <p>Estimate the sample frequency given a time array.</p> </li> <li> <code>trim</code>             \u2013              <p>Trim samples from the beginning or end of a signal.</p> </li> <li> <code>zscore_signals</code>             \u2013              <p>Z-score one or more signals.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.are_arrays_same_length","title":"<code>are_arrays_same_length(*arrays)</code>","text":"<p>Check if all arrays are the same shape in the first axis.</p> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def are_arrays_same_length(*arrays: np.ndarray) -&gt; bool:\n    \"\"\"Check if all arrays are the same shape in the first axis.\"\"\"\n    lengths = [arr.shape[0] for arr in arrays]\n    return bool(np.all(np.array(lengths) == lengths[0]))\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.detrend_double_exponential","title":"<code>detrend_double_exponential(time, signal)</code>","text":"<p>Detrend a signal by fitting and subtracting a double exponential curve to the data.</p> <p>See <code>double_exponential</code> for the underlying curve design, and <code>fit_double_exponential</code> for the curve fitting procedure.</p> <p>Parameters:</p> <ul> <li> <code>time</code>               (<code>ndarray</code>)           \u2013            <p>array of sample/observation times</p> </li> <li> <code>signal</code>               (<code>ndarray</code>)           \u2013            <p>array of samples</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ndarray]</code>           \u2013            <p>Tuple of (detrended_signal, signal_fit)</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def detrend_double_exponential(time: np.ndarray, signal: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Detrend a signal by fitting and subtracting a double exponential curve to the data.\n\n    See `double_exponential` for the underlying curve design, and `fit_double_exponential` for the curve fitting procedure.\n\n    Args:\n        time: array of sample/observation times\n        signal: array of samples\n\n    Returns:\n        Tuple of (detrended_signal, signal_fit)\n    \"\"\"\n    signal_fit = fit_double_exponential(time, signal)\n    return signal - signal_fit, signal_fit\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.double_exponential","title":"<code>double_exponential(t, const, amp_fast, amp_slow, tau_slow, tau_multiplier)</code>","text":"<p>Compute a double exponential function with constant offset.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>ndarray</code>)           \u2013            <p>Time vector in seconds.</p> </li> <li> <code>const</code>               (<code>float</code>)           \u2013            <p>Amplitude of the constant offset.</p> </li> <li> <code>amp_fast</code>               (<code>float</code>)           \u2013            <p>Amplitude of the fast component.</p> </li> <li> <code>amp_slow</code>               (<code>float</code>)           \u2013            <p>Amplitude of the slow component.</p> </li> <li> <code>tau_slow</code>               (<code>float</code>)           \u2013            <p>Time constant of slow component in seconds.</p> </li> <li> <code>tau_multiplier</code>               (<code>float</code>)           \u2013            <p>Time constant of fast component relative to slow.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>dependent values evaluated over <code>t</code> given the remaining parameters.</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def double_exponential(t: np.ndarray, const: float, amp_fast: float, amp_slow: float, tau_slow: float, tau_multiplier: float) -&gt; np.ndarray:\n    \"\"\"Compute a double exponential function with constant offset.\n\n    Args:\n        t: Time vector in seconds.\n        const: Amplitude of the constant offset.\n        amp_fast: Amplitude of the fast component.\n        amp_slow: Amplitude of the slow component.\n        tau_slow: Time constant of slow component in seconds.\n        tau_multiplier: Time constant of fast component relative to slow.\n\n    Returns:\n        dependent values evaluated over `t` given the remaining parameters.\n    \"\"\"\n    tau_fast = tau_slow * tau_multiplier\n    return const + amp_slow * np.exp(-t / tau_slow) + amp_fast * np.exp(-t / tau_fast)\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.downsample","title":"<code>downsample(*signals, window=10, factor=10)</code>","text":"<p>Downsample one or more signals by factor across windows of size <code>window</code>.</p> <p>performs a moving window average using windows of size <code>window</code>, then takes every n-th observation as given by <code>factor</code>.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>ndarray</code>, default:                   <code>()</code> )           \u2013            <p>one or more signals to downsample</p> </li> <li> <code>window</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>size of the window used for averaging</p> </li> <li> <code>factor</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>step size for taking the final downsampled signal</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ...]</code>           \u2013            <p>downsampled signal(s)</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def downsample(*signals: np.ndarray, window: int = 10, factor: int = 10) -&gt; tuple[np.ndarray, ...]:\n    \"\"\"Downsample one or more signals by factor across windows of size `window`.\n\n    performs a moving window average using windows of size `window`, then takes every\n    n-th observation as given by `factor`.\n\n    Args:\n        signals: one or more signals to downsample\n        window: size of the window used for averaging\n        factor: step size for taking the final downsampled signal\n\n    Returns:\n        downsampled signal(s)\n    \"\"\"\n    # assert are_arrays_same_length(*signals)\n    return tuple(np.convolve(sig, np.ones(window) / window, mode=\"valid\")[::factor] for sig in signals)\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.estimate_motion","title":"<code>estimate_motion(signal, control)</code>","text":"<p>Estimate the contribution of motion artifacts in <code>signal</code>.</p> <p>Performs linear regression of control (x, independent) vs signal (y, dependent)</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>ndarray</code>)           \u2013            <p>the signal to correct for motion artifacts</p> </li> <li> <code>control</code>               (<code>ndarray</code>)           \u2013            <p>signal to use for background signal</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ndarray]</code>           \u2013            <p>tuple of (corrected_signal, estimated_motion)</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def estimate_motion(signal: np.ndarray, control: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Estimate the contribution of motion artifacts in `signal`.\n\n    Performs linear regression of control (x, independent) vs signal (y, dependent)\n\n    Args:\n        signal: the signal to correct for motion artifacts\n        control: signal to use for background signal\n\n    Returns:\n        tuple of (corrected_signal, estimated_motion)\n    \"\"\"\n    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x=control, y=signal)\n    est_motion = intercept + slope * (control)\n    return signal - est_motion, est_motion\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.fit_double_exponential","title":"<code>fit_double_exponential(time, signal)</code>","text":"<p>Run the fitting procedure for a double exponential curve.</p> <p>Parameters:</p> <ul> <li> <code>time</code>               (<code>ndarray</code>)           \u2013            <p>array of sample times</p> </li> <li> <code>signal</code>               (<code>ndarray</code>)           \u2013            <p>array of sample values</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>array of values from the fitted double exponential curve, samples at the times in <code>time</code>.</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def fit_double_exponential(time: np.ndarray, signal: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Run the fitting procedure for a double exponential curve.\n\n    Args:\n        time: array of sample times\n        signal: array of sample values\n\n    Returns:\n        array of values from the fitted double exponential curve, samples at the times in `time`.\n    \"\"\"\n    max_sig = np.max(signal)\n    inital_params = [max_sig / 2, max_sig / 4, max_sig / 4, 3600, 0.1]\n    bounds = ([0, 0, 0, 600, 0], [max_sig, max_sig, max_sig, 36000, 1])\n    parm_opt, parm_cov = scipy.optimize.curve_fit(double_exponential, time, signal, p0=inital_params, bounds=bounds, maxfev=1000)\n    return double_exponential(time, *parm_opt)\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.fs2t","title":"<code>fs2t(fs, length)</code>","text":"<p>Generate a time array given a sample frequency and number of samples.</p> <p>Parameters:</p> <ul> <li> <code>fs</code>               (<code>float</code>)           \u2013            <p>sampling rate, in Hz</p> </li> <li> <code>length</code>               (<code>int</code>)           \u2013            <p>number of samples to generate</p> </li> </ul> <p>Returns     array of time values, in seconds</p> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def fs2t(fs: float, length: int) -&gt; np.ndarray:\n    \"\"\"Generate a time array given a sample frequency and number of samples.\n\n    Args:\n        fs: sampling rate, in Hz\n        length: number of samples to generate\n\n    Returns\n        array of time values, in seconds\n    \"\"\"\n    return np.linspace(1, length, length) / fs\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.lowpass_filter","title":"<code>lowpass_filter(signal, fs, Wn=10)</code>","text":"<p>zero-phase lowpass filter a signal.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>ndarray</code>)           \u2013            <p>array to be filtered</p> </li> <li> <code>fs</code>               (<code>float</code>)           \u2013            <p>sampling frequency of the signal, in Hz</p> </li> <li> <code>Wn</code>               (<code>float</code>, default:                   <code>10</code> )           \u2013            <p>critical frequency, see <code>scipy.signal.butter()</code></p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>lowpass filtered signal</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def lowpass_filter(signal: np.ndarray, fs: float, Wn: float = 10) -&gt; np.ndarray:\n    \"\"\"zero-phase lowpass filter a signal.\n\n    Args:\n        signal: array to be filtered\n        fs: sampling frequency of the signal, in Hz\n        Wn: critical frequency, see `scipy.signal.butter()`\n\n    Returns:\n        lowpass filtered signal\n    \"\"\"\n    b, a = scipy.signal.butter(2, Wn, btype=\"lowpass\", fs=fs)\n    return scipy.signal.filtfilt(b, a, signal)\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.t2fs","title":"<code>t2fs(time)</code>","text":"<p>Estimate the sample frequency given a time array.</p> <p>Parameters:</p> <ul> <li> <code>time</code>               (<code>ndarray</code>)           \u2013            <p>array of time values, in seconds, from which to estimate the sample frequency</p> </li> </ul> <p>Returns     The estimated sample frequency, in Hz</p> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def t2fs(time: np.ndarray) -&gt; float:\n    \"\"\"Estimate the sample frequency given a time array.\n\n    Args:\n        time: array of time values, in seconds, from which to estimate the sample frequency\n\n    Returns\n        The estimated sample frequency, in Hz\n    \"\"\"\n    return 1 / np.median(np.diff(time))\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.trim","title":"<code>trim(*signals, begin=None, end=None)</code>","text":"<p>Trim samples from the beginning or end of a signal.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>ndarray</code>, default:                   <code>()</code> )           \u2013            <p>one or more signals to be trimmed</p> </li> <li> <code>begin</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>number of samples to trim from the beginning</p> </li> <li> <code>end</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>number of samples to trim from the end</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ...]</code>           \u2013            <p>tuple of trimmed signals</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def trim(*signals: np.ndarray, begin: Optional[int] = None, end: Optional[int] = None) -&gt; tuple[np.ndarray, ...]:\n    \"\"\"Trim samples from the beginning or end of a signal.\n\n    Args:\n        signals: one or more signals to be trimmed\n        begin: number of samples to trim from the beginning\n        end: number of samples to trim from the end\n\n    Returns:\n        tuple of trimmed signals\n    \"\"\"\n    assert are_arrays_same_length(*signals)\n    if begin is None:\n        begin = 0\n    if end is None:\n        end = signals[0].shape[0]\n    return tuple(sig[begin:end] for sig in signals)\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.zscore_signals","title":"<code>zscore_signals(*signals)</code>","text":"<p>Z-score one or more signals.</p> <p>see: scipy.stats.zscore()</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>ndarray</code>, default:                   <code>()</code> )           \u2013            <p>one or more signals to be z-scores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ...]</code>           \u2013            <p>tuple of z-scored signals</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def zscore_signals(*signals: np.ndarray) -&gt; tuple[np.ndarray, ...]:\n    \"\"\"Z-score one or more signals.\n\n    see: scipy.stats.zscore()\n\n    Args:\n        signals: one or more signals to be z-scores\n\n    Returns:\n        tuple of z-scored signals\n    \"\"\"\n    return tuple(scipy.stats.zscore(sig) for sig in signals)\n</code></pre>"},{"location":"reference/fptools/preprocess/pipelines/","title":"pipelines","text":""},{"location":"reference/fptools/preprocess/pipelines/#fptools.preprocess.pipelines","title":"<code>fptools.preprocess.pipelines</code>","text":"<p>Modules:</p> <ul> <li> <code>dxp_motion_dff</code>           \u2013            </li> <li> <code>lowpass_dff</code>           \u2013            </li> <li> <code>tdt_default</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/preprocess/pipelines/dxp_motion_dff/","title":"dxp_motion_dff","text":""},{"location":"reference/fptools/preprocess/pipelines/dxp_motion_dff/#fptools.preprocess.pipelines.dxp_motion_dff","title":"<code>fptools.preprocess.pipelines.dxp_motion_dff</code>","text":"<p>Functions:</p> <ul> <li> <code>dxp_motion_dff</code>             \u2013              <p>Preprocess using a double exponential fit for detrending, producing df/f values.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/pipelines/dxp_motion_dff/#fptools.preprocess.pipelines.dxp_motion_dff.dxp_motion_dff","title":"<code>dxp_motion_dff(session, block, signal_map, show_steps=True, plot_dir='', trim_extent='auto')</code>","text":"<p>Preprocess using a double exponential fit for detrending, producing df/f values.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to populate.</p> </li> <li> <code>block</code>               (<code>Any</code>)           \u2013            <p>block data struct from <code>tdt.read_block()</code>.</p> </li> <li> <code>signal_map</code>               (<code>list[SignalMapping]</code>)           \u2013            <p>mapping of signals to perform</p> </li> <li> <code>show_steps</code>           \u2013            <p>if <code>True</code>, produce diagnostic plots of the preprocessing steps.</p> </li> <li> <code>plot_dir</code>           \u2013            <p>path where diagnostic plots of the preprocessing steps should be saved.</p> </li> <li> <code>trim_extent</code>               (<code>Union[None, Literal['auto'], int, tuple[int, int]]</code>, default:                   <code>'auto'</code> )           \u2013            <p>specification for trimming. None disables trimming, auto uses the offset stored in <code>block.scalars.Fi1i.ts</code>, a single int trims that many samples from the beginning, a tuple of two ints specifies the number of samples from the beginning and end to trim, respectively.</p> </li> <li> <code>#</code>               (<code>downsample</code>)           \u2013            <p>if not <code>None</code>, downsample signal by <code>downsample</code> factor.</p> </li> </ul> Source code in <code>fptools/preprocess/pipelines/dxp_motion_dff.py</code> <pre><code>def dxp_motion_dff(\n    session: Session,\n    block: Any,\n    signal_map: list[SignalMapping],\n    show_steps=True,\n    plot_dir=\"\",\n    trim_extent: Union[None, Literal[\"auto\"], int, tuple[int, int]] = \"auto\",\n):\n    \"\"\"Preprocess using a double exponential fit for detrending, producing df/f values.\n\n    Args:\n        session: the session to populate.\n        block: block data struct from `tdt.read_block()`.\n        signal_map: mapping of signals to perform\n        show_steps: if `True`, produce diagnostic plots of the preprocessing steps.\n        plot_dir: path where diagnostic plots of the preprocessing steps should be saved.\n        trim_extent: specification for trimming. None disables trimming, auto uses the offset stored in `block.scalars.Fi1i.ts`, a single int trims that many samples from the beginning, a tuple of two ints specifies the number of samples from the beginning and end to trim, respectively.\n        # downsample: if not `None`, downsample signal by `downsample` factor.\n    \"\"\"\n    try:\n        if show_steps:\n            fig, axs = plt.subplots(6, 1, figsize=(24, 6 * 6))\n            palette = sns.color_palette(\"colorblind\", n_colors=len(signal_map))\n\n        signals: list[Signal] = []\n        for sm in signal_map:\n            stream = block.streams[sm[\"tdt_name\"]]\n            signals.append(Signal(sm[\"dest_name\"], stream.data, fs=stream.fs))\n\n        if show_steps:\n            for i, sig in enumerate(signals):\n                axs[0].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n            axs[0].set_title(\"Raw signal\")\n            axs[0].legend()\n\n        # trim raw signal start to when the optical system came online\n        if trim_extent is not None:\n            if trim_extent == \"auto\":\n                trim_args = {\"begin\": int(block.scalars.Fi1i.ts[0] * sig.fs)}\n            elif isinstance(trim_extent, int):\n                trim_args = {\"begin\": trim_extent}\n            elif len(trim_extent) == 2:\n                trim_args = {\"begin\": trim_extent[0], \"end\": trim_extent[1]}\n\n            for sig in signals:\n                sig.signal, sig.time = trim(sig.signal, sig.time, **trim_args)\n\n            if show_steps:\n                for i, sig in enumerate(signals):\n                    axs[1].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n                axs[1].set_title(\"Trimmed Raw signal\")\n                axs[1].legend()\n        else:\n            axs[1].set_title(\"Trimming Disabled\")\n\n        # detrend using a double exponential fit\n        fits: list[np.ndarray] = []\n        for sig in signals:\n            detrended_sig, fit = detrend_double_exponential(sig.time, sig.signal)\n            fits.append(fit)\n\n            if show_steps:\n                axs[2].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n                axs[2].plot(sig.time, fit, label=f\"{sig.name} dExp Fit\", c=sns.desaturate(palette[i], 0.3))\n\n            sig.signal = detrended_sig\n\n        if show_steps:\n            axs[2].set_title(\"Double Exponential Fit\")\n            axs[2].legend()\n\n            for i, sig in enumerate(signals):\n                axs[3].plot(sig.time, detrended_sig, label=sig.name, c=palette[i])\n            axs[3].set_title(\"De-trended signals\")\n            axs[3].legend()\n\n        # correct for motion artifacts\n        ctrl_idx = next((i for i, sm in enumerate(signal_map) if sm[\"role\"] == \"control\"), None)\n        if ctrl_idx is None:\n            raise ValueError('To use motion correction, at least one signal must be marked with `role`=\"Control\" in the `signal_map`!')\n        for sm, sig in zip(signal_map, signals):\n            if sm[\"role\"] == \"experimental\":\n                motion_corrected, est_motion = estimate_motion(sig.signal, signals[ctrl_idx].signal)\n\n                if show_steps:\n                    axs[4].plot(sig.time, motion_corrected, label=sig.name, c=palette[i])\n                    axs[4].plot(sig.time, est_motion, label=f\"{sig.name} Est. Motion\", c=sns.desaturate(palette[i], 0.3))\n\n        if show_steps:\n            axs[4].set_title(\"Motion Correction\")\n            axs[4].legend()\n\n        # calculate dF/F\n        for sig, fit in zip(signals, fits):\n            sig.signal = (sig.signal / fit) * 100\n            sig.units = \"\u0394F/F\"\n\n        if show_steps:\n            for i, sig in enumerate(signals):\n                axs[5].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n            axs[5].set_title(\"Normalized\")\n            axs[5].legend()\n\n        # construct Signals and add to the Session\n        for sig in signals:\n            session.add_signal(sig)\n\n        return session\n    except:\n        raise\n    finally:\n        if show_steps:\n            fig.savefig(os.path.join(plot_dir, f\"{block.info.blockname}.png\"), dpi=600)\n            fig.savefig(os.path.join(plot_dir, f\"{block.info.blockname}.pdf\"))\n            plt.close(fig)\n</code></pre>"},{"location":"reference/fptools/preprocess/pipelines/lowpass_dff/","title":"lowpass_dff","text":""},{"location":"reference/fptools/preprocess/pipelines/lowpass_dff/#fptools.preprocess.pipelines.lowpass_dff","title":"<code>fptools.preprocess.pipelines.lowpass_dff</code>","text":"<p>Functions:</p> <ul> <li> <code>lowpass_dff</code>             \u2013              <p>A \"simple\" preprocess pipeline based on ultra-lowpass filtering.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/pipelines/lowpass_dff/#fptools.preprocess.pipelines.lowpass_dff.lowpass_dff","title":"<code>lowpass_dff(session, block, signal_map, show_steps=True, plot_dir='', trim_extent='auto', downsample=None)</code>","text":"<p>A \"simple\" preprocess pipeline based on ultra-lowpass filtering.</p> <p>Implemented as described in: Cai, Kaeser, et al. Dopamine dynamics are dispensable for movement but promote reward responses. Nature, 2024. https://doi.org/10.1038/s41586-024-08038-z</p> <p>Pipeline steps: 1) Signals are trimmed to the optical system start. 2) Signals are lowpass filterd at 0.01 Hz (~100 second timescale) 3) Signals are converted to dF/F using lowpass filtered signals as F0 4) Signals are optionally downsampled factor <code>downsample</code></p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to populate.</p> </li> <li> <code>block</code>               (<code>Any</code>)           \u2013            <p>block data struct from <code>tdt.read_block()</code>.</p> </li> <li> <code>signal_map</code>               (<code>list[SignalMapping]</code>)           \u2013            <p>mapping of signals to perform</p> </li> <li> <code>show_steps</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if <code>True</code>, produce diagnostic plots of the preprocessing steps.</p> </li> <li> <code>plot_dir</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>path where diagnostic plots of the preprocessing steps should be saved.</p> </li> <li> <code>trim_extent</code>               (<code>Union[None, Literal['auto'], int, tuple[int, int]]</code>, default:                   <code>'auto'</code> )           \u2013            <p>specification for trimming. None disables trimming, auto uses the offset stored in <code>block.scalars.Fi1i.ts</code>, a single int trims that many samples from the beginning, a tuple of two ints specifies the number of samples from the beginning and end to trim, respectively.</p> </li> <li> <code>downsample</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>if not <code>None</code>, downsample signal by <code>downsample</code> factor.</p> </li> </ul> Source code in <code>fptools/preprocess/pipelines/lowpass_dff.py</code> <pre><code>def lowpass_dff(\n    session: Session,\n    block: Any,\n    signal_map: list[SignalMapping],\n    show_steps: bool = True,\n    plot_dir: str = \"\",\n    trim_extent: Union[None, Literal[\"auto\"], int, tuple[int, int]] = \"auto\",\n    downsample: Optional[int] = None,\n) -&gt; Session:\n    \"\"\"A \"simple\" preprocess pipeline based on ultra-lowpass filtering.\n\n    Implemented as described in:\n    Cai, Kaeser, et al. Dopamine dynamics are dispensable for movement but promote reward responses.\n    Nature, 2024. https://doi.org/10.1038/s41586-024-08038-z\n\n    Pipeline steps:\n    1) Signals are trimmed to the optical system start.\n    2) Signals are lowpass filterd at 0.01 Hz (~100 second timescale)\n    3) Signals are converted to dF/F using lowpass filtered signals as F0\n    4) Signals are optionally downsampled factor `downsample`\n\n    Args:\n        session: the session to populate.\n        block: block data struct from `tdt.read_block()`.\n        signal_map: mapping of signals to perform\n        show_steps: if `True`, produce diagnostic plots of the preprocessing steps.\n        plot_dir: path where diagnostic plots of the preprocessing steps should be saved.\n        trim_extent: specification for trimming. None disables trimming, auto uses the offset stored in `block.scalars.Fi1i.ts`, a single int trims that many samples from the beginning, a tuple of two ints specifies the number of samples from the beginning and end to trim, respectively.\n        downsample: if not `None`, downsample signal by `downsample` factor.\n    \"\"\"\n    try:\n        if show_steps:\n            fig, axs = plt.subplots(4, 1, figsize=(24, 6 * 4))\n            palette = sns.color_palette(\"colorblind\", n_colors=len(signal_map))\n\n        signals: list[Signal] = []\n        for sm in signal_map:\n            stream = block.streams[sm[\"tdt_name\"]]\n            signals.append(Signal(sm[\"dest_name\"], stream.data, fs=stream.fs))\n\n        if show_steps:\n            for i, sig in enumerate(signals):\n                axs[0].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n            axs[0].set_title(\"Raw signal\")\n            axs[0].legend()\n\n        # trim raw signal start to when the optical system came online\n        if trim_extent is not None:\n            if trim_extent == \"auto\":\n                trim_args = {\"begin\": int(block.scalars.Fi1i.ts[0] * sig.fs)}\n            elif isinstance(trim_extent, int):\n                trim_args = {\"begin\": trim_extent}\n            elif len(trim_extent) == 2:\n                trim_args = {\"begin\": trim_extent[0], \"end\": trim_extent[1]}\n\n            for sig in signals:\n                sig.signal, sig.time = trim(sig.signal, sig.time, **trim_args)\n\n            if show_steps:\n                for i, sig in enumerate(signals):\n                    axs[1].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n                axs[1].set_title(\"Trimmed Raw signal\")\n                axs[1].legend()\n        else:\n            axs[1].set_title(\"Trimming Disabled\")\n\n        # lowpass filter at 0.01 Hz\n        lowpass_signals = []\n        for sig in signals:\n            s = sig.copy()\n            s.signal = lowpass_filter(sig.signal, sig.fs, 0.01)\n            lowpass_signals.append(s)\n\n        if show_steps:\n            for i, sig in enumerate(lowpass_signals):\n                axs[2].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n            axs[2].set_title(\"Lowpasss filtered (0.01 Hz)\")\n            axs[2].legend()\n\n        # calculate dF/F\n        for sig, lowpass_sig in zip(signals, lowpass_signals):\n            sig.signal = ((sig.signal - lowpass_sig.signal) / lowpass_sig.signal) * 100\n            sig.units = \"\u0394F/F\"\n\n        if show_steps:\n            for i, sig in enumerate(signals):\n                axs[3].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n            axs[3].set_title(\"Normalized (dff)\")\n            axs[3].legend()\n\n        # possibly downsample\n        if downsample is not None:\n            for sig in signals:\n                sig.signal, sig.time = downsample_fn(sig.signal, sig.time, window=downsample, factor=downsample)\n                sig.fs = t2fs(sig.time)\n\n        # construct Signals and add to the Session\n        for sig in signals:\n            session.add_signal(sig)\n\n        return session\n    except:\n        raise\n    finally:\n        if show_steps:\n            fig.savefig(os.path.join(plot_dir, f\"{block.info.blockname}.png\"), dpi=600)\n            fig.savefig(os.path.join(plot_dir, f\"{block.info.blockname}.pdf\"))\n            plt.close(fig)\n</code></pre>"},{"location":"reference/fptools/preprocess/pipelines/tdt_default/","title":"tdt_default","text":""},{"location":"reference/fptools/preprocess/pipelines/tdt_default/#fptools.preprocess.pipelines.tdt_default","title":"<code>fptools.preprocess.pipelines.tdt_default</code>","text":"<p>Functions:</p> <ul> <li> <code>tdt_default</code>             \u2013              <p>A preprocess pipeline based on TDT tutorials.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/pipelines/tdt_default/#fptools.preprocess.pipelines.tdt_default.tdt_default","title":"<code>tdt_default(session, block, signal_map, show_steps=True, plot_dir='', trim_extent='auto', downsample=None)</code>","text":"<p>A preprocess pipeline based on TDT tutorials.</p> <p>Pipeline steps: 1) Signals are trimmed to the optical system start. 2) Perform linear regression between sensor and isosbestic 3) calculate dFF</p>"},{"location":"reference/fptools/preprocess/pipelines/tdt_default/#fptools.preprocess.pipelines.tdt_default.tdt_default--4-signals-are-optionally-downsampled-factor-downsample","title":"4) Signals are optionally downsampled factor <code>downsample</code>","text":"<p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to populate.</p> </li> <li> <code>block</code>               (<code>Any</code>)           \u2013            <p>block data struct from <code>tdt.read_block()</code>.</p> </li> <li> <code>signal_map</code>               (<code>list[SignalMapping]</code>)           \u2013            <p>mapping of signals to perform</p> </li> <li> <code>show_steps</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if <code>True</code>, produce diagnostic plots of the preprocessing steps.</p> </li> <li> <code>plot_dir</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>path where diagnostic plots of the preprocessing steps should be saved.</p> </li> <li> <code>trim_extent</code>               (<code>Union[None, Literal['auto'], int, tuple[int, int]]</code>, default:                   <code>'auto'</code> )           \u2013            <p>specification for trimming. None disables trimming, auto uses the offset stored in <code>block.scalars.Fi1i.ts</code>, a single int trims that many samples from the beginning, a tuple of two ints specifies the number of samples from the beginning and end to trim, respectively.</p> </li> <li> <code>downsample</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>if not <code>None</code>, downsample signal by <code>downsample</code> factor.</p> </li> </ul> Source code in <code>fptools/preprocess/pipelines/tdt_default.py</code> <pre><code>def tdt_default(\n    session: Session,\n    block: Any,\n    signal_map: list[SignalMapping],\n    show_steps: bool = True,\n    plot_dir: str = \"\",\n    trim_extent: Union[None, Literal[\"auto\"], int, tuple[int, int]] = \"auto\",\n    downsample: Optional[int] = None,\n) -&gt; Session:\n    \"\"\"A preprocess pipeline based on TDT tutorials.\n\n    Pipeline steps:\n    1) Signals are trimmed to the optical system start.\n    2) Perform linear regression between sensor and isosbestic\n    3) calculate dFF\n    #4) Signals are optionally downsampled factor `downsample`\n\n    Args:\n        session: the session to populate.\n        block: block data struct from `tdt.read_block()`.\n        signal_map: mapping of signals to perform\n        show_steps: if `True`, produce diagnostic plots of the preprocessing steps.\n        plot_dir: path where diagnostic plots of the preprocessing steps should be saved.\n        trim_extent: specification for trimming. None disables trimming, auto uses the offset stored in `block.scalars.Fi1i.ts`, a single int trims that many samples from the beginning, a tuple of two ints specifies the number of samples from the beginning and end to trim, respectively.\n\n        downsample: if not `None`, downsample signal by `downsample` factor.\n    \"\"\"\n    try:\n        if show_steps:\n            fig, axs = plt.subplots(4, 1, figsize=(24, 6 * 4))\n            palette = sns.color_palette(\"colorblind\", n_colors=len(signal_map))\n\n        signals: list[Signal] = []\n        for sm in signal_map:\n            stream = block.streams[sm[\"tdt_name\"]]\n            signals.append(Signal(sm[\"dest_name\"], stream.data, fs=stream.fs))\n\n        if show_steps:\n            for i, sig in enumerate(signals):\n                axs[0].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n            axs[0].set_title(\"Raw signal\")\n            axs[0].legend()\n\n        # trim raw signal start to when the optical system came online\n        if trim_extent is not None:\n            if trim_extent == \"auto\":\n                trim_args = {\"begin\": int(block.scalars.Fi1i.ts[0] * sig.fs)}\n            elif isinstance(trim_extent, int):\n                trim_args = {\"begin\": trim_extent}\n            elif len(trim_extent) == 2:\n                trim_args = {\"begin\": trim_extent[0], \"end\": trim_extent[1]}\n\n            for sig in signals:\n                sig.signal, sig.time = trim(sig.signal, sig.time, **trim_args)\n\n            if show_steps:\n                for i, sig in enumerate(signals):\n                    axs[1].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n                axs[1].set_title(\"Trimmed Raw signal\")\n                axs[1].legend()\n        else:\n            axs[1].set_title(\"Trimming Disabled\")\n\n        ctrl_idx = next((i for i, sm in enumerate(signal_map) if sm[\"role\"] == \"control\"), None)\n        if ctrl_idx is None:\n            raise ValueError('at least one signal must be marked with `role`=\"Control\" in the `signal_map`!')\n        for sm, sig in zip(signal_map, signals):\n            if sm[\"role\"] == \"experimental\":\n\n                x = np.array(signals[ctrl_idx].signal)\n                y = np.array(sig.signal)\n                bls = np.polyfit(x, y, 1)\n                Y_fit_all = np.multiply(bls[0], x) + bls[1]\n                Y_dF_all = y - Y_fit_all\n                dFF = np.multiply(100, np.divide(Y_dF_all, Y_fit_all))\n                sig.signal = dFF\n                sig.units = \"\u0394F/F\"\n\n        if show_steps:\n            for i, sig in enumerate(signals):\n                axs[2].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n            axs[2].set_title(\"Normalized (dff)\")\n            axs[2].legend()\n\n        # construct Signals and add to the Session\n        for sig in signals:\n            session.add_signal(sig)\n\n        return session\n    except:\n        raise\n    finally:\n        if show_steps:\n            fig.savefig(os.path.join(plot_dir, f\"{block.info.blockname}.png\"), dpi=600)\n            fig.savefig(os.path.join(plot_dir, f\"{block.info.blockname}.pdf\"))\n            plt.close(fig)\n</code></pre>"},{"location":"reference/fptools/viz/","title":"viz","text":""},{"location":"reference/fptools/viz/#fptools.viz","title":"<code>fptools.viz</code>","text":"<p>Modules:</p> <ul> <li> <code>viz</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/viz/viz/","title":"viz","text":""},{"location":"reference/fptools/viz/viz/#fptools.viz.viz","title":"<code>fptools.viz.viz</code>","text":"<p>Functions:</p> <ul> <li> <code>plot_heatmap</code>             \u2013              <p>Plot a signal as a heatmap.</p> </li> <li> <code>plot_signal</code>             \u2013              <p>Plot a signal as a lineplot.</p> </li> <li> <code>sig_catplot</code>             \u2013              <p>Plot signals, similar to <code>seaborn.catplot()</code>.</p> </li> </ul>"},{"location":"reference/fptools/viz/viz/#fptools.viz.viz.plot_heatmap","title":"<code>plot_heatmap(signal, ax=None, cmap='viridis', vmin=None, vmax=None)</code>","text":"<p>Plot a signal as a heatmap.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>Signal</code>)           \u2013            <p>the signal to be plotted</p> </li> <li> <code>ax</code>               (<code>Optional[Axes]</code>, default:                   <code>None</code> )           \u2013            <p>optional axes to plot on. If not provided, a new figure with a single axes will be created</p> </li> <li> <code>cmap</code>           \u2013            <p>colormap to use for plotting</p> </li> <li> <code>vmin</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>minimum value mapping to colormap start. If None, will use the data minimum value</p> </li> <li> <code>vmax</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>maximum value mapping to colormap end. If None, will use the data maximum value</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Axes</code>           \u2013            <p>Axes</p> </li> </ul> Source code in <code>fptools/viz/viz.py</code> <pre><code>def plot_heatmap(\n    signal: Signal, ax: Optional[Axes] = None, cmap=\"viridis\", vmin: Optional[float] = None, vmax: Optional[float] = None\n) -&gt; Axes:\n    \"\"\"Plot a signal as a heatmap.\n\n    Args:\n        signal: the signal to be plotted\n        ax: optional axes to plot on. If not provided, a new figure with a single axes will be created\n        cmap: colormap to use for plotting\n        vmin: minimum value mapping to colormap start. If None, will use the data minimum value\n        vmax: maximum value mapping to colormap end. If None, will use the data maximum value\n\n    Returns:\n        Axes\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    cbar_kwargs = {\"label\": f\"{signal.name} ({signal.units})\"}\n\n    sns.heatmap(data=np.atleast_2d(signal.signal), ax=ax, cmap=cmap, vmin=vmin, vmax=vmax, cbar_kws=cbar_kwargs)\n\n    xticks = [0, signal.nsamples]\n    xticklabels = [f\"{signal.time[0]:0.0f}\", f\"{signal.time[-1]:0.0f}\"]\n    for m, t in signal.marks.items():\n        i = signal.tindex(t)\n        ax.axvline(i, c=\"w\", ls=\"--\")\n        xticks.append(i)\n        xticklabels.append(f\"{m}\")\n    order = np.argsort(xticks)\n    xticks = [xticks[i] for i in order]\n    xticklabels = [xticklabels[i] for i in order]\n    ax.set_xticks(xticks, labels=xticklabels, rotation=0)\n\n    ax.set_xlabel(\"Time, Reletive to Event (sec)\")\n\n    return ax\n</code></pre>"},{"location":"reference/fptools/viz/viz/#fptools.viz.viz.plot_signal","title":"<code>plot_signal(signal, ax=None, show_indv=True, color='k', indv_c='b', indv_alpha=0.1, indv_kwargs=None, agg_kwargs=None)</code>","text":"<p>Plot a signal as a lineplot.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>Signal</code>)           \u2013            <p>the signal to be plotted</p> </li> <li> <code>ax</code>               (<code>Optional[Axes]</code>, default:                   <code>None</code> )           \u2013            <p>optional axes to plot on. If not provided, a new figure with a single axes will be created</p> </li> <li> <code>show_indv</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if True, plot individual traces, otherwise only plot aggregate traces</p> </li> <li> <code>color</code>               (<code>ColorType</code>, default:                   <code>'k'</code> )           \u2013            <p>color of the aggregated trace</p> </li> <li> <code>indv_c</code>               (<code>ColorType</code>, default:                   <code>'b'</code> )           \u2013            <p>color of individual traces</p> </li> <li> <code>indv_alpha</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>alpha transparency for individual traces</p> </li> <li> <code>indv_kwargs</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>kwargs to pass to <code>seaborn.lineplot()</code> for individual traces</p> </li> <li> <code>agg_kwargs</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>kwarge to pass to <code>seaborn.lineplot()</code> for aggregate traces</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Axes</code>           \u2013            <p>Axes</p> </li> </ul> Source code in <code>fptools/viz/viz.py</code> <pre><code>def plot_signal(\n    signal: Signal,\n    ax: Optional[Axes] = None,\n    show_indv: bool = True,\n    color: ColorType = \"k\",\n    indv_c: ColorType = \"b\",\n    indv_alpha: float = 0.1,\n    indv_kwargs: Optional[dict] = None,\n    agg_kwargs: Optional[dict] = None,\n) -&gt; Axes:\n    \"\"\"Plot a signal as a lineplot.\n\n    Args:\n        signal: the signal to be plotted\n        ax: optional axes to plot on. If not provided, a new figure with a single axes will be created\n        show_indv: if True, plot individual traces, otherwise only plot aggregate traces\n        color: color of the aggregated trace\n        indv_c: color of individual traces\n        indv_alpha: alpha transparency for individual traces\n        indv_kwargs: kwargs to pass to `seaborn.lineplot()` for individual traces\n        agg_kwargs: kwarge to pass to `seaborn.lineplot()` for aggregate traces\n\n    Returns:\n        Axes\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    df = pd.DataFrame(signal.signal.T)\n    df.index = signal.time\n    df = df.melt(ignore_index=False)\n\n    _indv_kwargs = {\n        \"alpha\": indv_alpha,\n        \"color\": indv_c,\n    }\n    if indv_kwargs is not None:\n        _indv_kwargs.update(indv_kwargs)\n\n    if show_indv and signal.nobs &gt; 1:\n        for i in range(signal.signal.shape[0]):\n            sns.lineplot(data=None, x=signal.time, y=signal.signal[i, :], ax=ax, **_indv_kwargs)\n\n    _agg_kwargs = {\"color\": color}\n    if agg_kwargs is not None:\n        _agg_kwargs.update(agg_kwargs)\n\n    sns.lineplot(data=df, x=df.index, y=\"value\", ax=ax, **_agg_kwargs)\n\n    ax.set_xlabel(\"Time, Reletive to Event (s)\")\n    ax.set_ylabel(f\"{signal.name} ({signal.units})\")\n\n    xticks = ax.get_xticks()\n    xticklabels = ax.get_xticklabels()\n    if len(xticklabels) == 0:\n        xticklabels = [Text(text=f\"{xt}\") for xt in xticks]\n    for k, v in signal.marks.items():\n        # only annotate marks if they are within the time domain\n        if v &gt;= signal.time.min() and v &lt;= signal.time.max():\n            ax.axvline(v, c=\"gray\", ls=\"--\")\n            try:\n                xt = np.where(xticks == float(v))[0][0]\n                xticklabels[xt] = Text(text=k)\n            except:\n                xticks = np.append(xticks, float(v))\n                xticklabels.append(Text(text=k))\n                order = np.argsort(xticks)\n                xticks = xticks[order]\n                xticklabels = [xticklabels[i] for i in order]\n                pass\n    ax.set_xticks(xticks, [t.get_text() for t in xticklabels])\n\n    return ax\n</code></pre>"},{"location":"reference/fptools/viz/viz/#fptools.viz.viz.sig_catplot","title":"<code>sig_catplot(sessions, signal, col=None, col_order=None, row=None, row_order=None, palette=None, hue=None, hue_order=None, show_indv=False, indv_alpha=0.1, height=6, aspect=1.5, sharex=True, sharey=True, agg_method='mean', indv_kwargs=None, agg_kwargs=None)</code>","text":"<p>Plot signals, similar to <code>seaborn.catplot()</code>.</p> <p>You may provide more than one signal name to the <code>signal</code> parameter. In this case, you must also specify one facet (e.x. <code>col</code>, <code>row</code>, <code>hue</code>) as the string \"signal\".</p> <p>Parameters:</p> <ul> <li> <code>sessions</code>               (<code>SessionCollection</code>)           \u2013            <p>sessions to plot data from</p> </li> <li> <code>signal</code>               (<code>Union[str, list[str]]</code>)           \u2013            <p>name of the signal(s) to be plotted.</p> </li> <li> <code>col</code>               (<code>Union[Literal['signal'], str, None]</code>, default:                   <code>None</code> )           \u2013            <p>metadata column on which to form plot columns, or if multiple signal names are given to <code>signal</code>, you may specify \"signal\" here</p> </li> <li> <code>col_order</code>               (<code>Union[list[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>explicit ordering for columns</p> </li> <li> <code>row</code>               (<code>Union[Literal['signal'], str, None]</code>, default:                   <code>None</code> )           \u2013            <p>metadata column on which to form plot rows, or if multiple signal names are given to <code>signal</code>, you may specify \"signal\" here</p> </li> <li> <code>row_order</code>               (<code>Union[list[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>explicit ordering for rows</p> </li> <li> <code>palette</code>               (<code>Optional[Union[str, list, dict[Any, str]]]</code>, default:                   <code>None</code> )           \u2013            <p>palette to use for hue mapping. A dict[value, color], or something that sns.color_palette() understands</p> </li> <li> <code>hue</code>               (<code>Union[str, None]</code>, default:                   <code>None</code> )           \u2013            <p>metadata column on which to group and color, or if multiple signal names are given to <code>signal</code>, you may specify \"signal\" here</p> </li> <li> <code>hue_order</code>               (<code>Union[list[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>explicit ordering for hues</p> </li> <li> <code>show_indv</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if True, show individual session traces</p> </li> <li> <code>indv_alpha</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>alpha transparency level for individual session traces, in range (0, 1)</p> </li> <li> <code>height</code>               (<code>float</code>, default:                   <code>6</code> )           \u2013            <p>height of each facet</p> </li> <li> <code>aspect</code>               (<code>float</code>, default:                   <code>1.5</code> )           \u2013            <p>Aspect ratio of each facet, so that aspect * height gives the width of each facet</p> </li> <li> <code>sharex</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true, the facets will share x axes.</p> </li> <li> <code>sharey</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true, the facets will share y axes.</p> </li> <li> <code>agg_method</code>               (<code>str</code>, default:                   <code>'mean'</code> )           \u2013            <p>method to use for aggregation (see <code>SessionCollection.aggregate_signals()</code> for more details)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Figure, ndarray]</code>           \u2013            <p>Figure and array of axes</p> </li> </ul> Source code in <code>fptools/viz/viz.py</code> <pre><code>def sig_catplot(\n    sessions: SessionCollection,\n    signal: Union[str, list[str]],\n    col: Union[Literal[\"signal\"], str, None] = None,\n    col_order: Union[list[str], None] = None,\n    row: Union[Literal[\"signal\"], str, None] = None,\n    row_order: Union[list[str], None] = None,\n    palette: Optional[Union[str, list, dict[Any, str]]] = None,\n    hue: Union[str, None] = None,\n    hue_order: Union[list[str], None] = None,\n    show_indv: bool = False,\n    indv_alpha: float = 0.1,\n    height: float = 6,\n    aspect: float = 1.5,\n    sharex: bool = True,\n    sharey: bool = True,\n    agg_method: str = \"mean\",\n    indv_kwargs: Optional[dict] = None,\n    agg_kwargs: Optional[dict] = None,\n) -&gt; tuple[Figure, np.ndarray]:\n    \"\"\"Plot signals, similar to `seaborn.catplot()`.\n\n    You may provide more than one signal name to the `signal` parameter. In this case, you must also specify one\n    facet (e.x. `col`, `row`, `hue`) as the string \"signal\".\n\n    Args:\n        sessions: sessions to plot data from\n        signal: name of the signal(s) to be plotted.\n        col: metadata column on which to form plot columns, or if multiple signal names are given to `signal`, you may specify \"signal\" here\n        col_order: explicit ordering for columns\n        row: metadata column on which to form plot rows, or if multiple signal names are given to `signal`, you may specify \"signal\" here\n        row_order: explicit ordering for rows\n        palette: palette to use for hue mapping. A dict[value, color], or something that sns.color_palette() understands\n        hue: metadata column on which to group and color, or if multiple signal names are given to `signal`, you may specify \"signal\" here\n        hue_order: explicit ordering for hues\n        show_indv: if True, show individual session traces\n        indv_alpha: alpha transparency level for individual session traces, in range (0, 1)\n        height: height of each facet\n        aspect: Aspect ratio of each facet, so that aspect * height gives the width of each facet\n        sharex: If true, the facets will share x axes.\n        sharey: If true, the facets will share y axes.\n        agg_method: method to use for aggregation (see `SessionCollection.aggregate_signals()` for more details)\n\n    Returns:\n        Figure and array of axes\n    \"\"\"\n    metadata = sessions.metadata\n\n    _signals: list[str] = []\n    if isinstance(signal, str):\n        _signals = [signal]\n    else:\n        _signals = list(signal)\n\n    if len(_signals) &gt; 1:\n        # multiple signals provided, need to check a few things...\n        # 1) at least one facet should be \"signal\"\n        # 2) at most one facet should be \"signal\"\n        num_sig_facets = [(fname, fval) for fname, fval in [(\"col\", col), (\"row\", row), (\"hue\", hue)] if fval == \"signal\"]\n        if len(num_sig_facets) &lt;= 0:\n            raise ValueError('When providing multiple values to parameter `signal`, at least one facet must be set to \"signal\"!')\n        if len(num_sig_facets) &gt; 1:\n            raise ValueError('When providing multiple values to parameter `signal`, at most one facet may be set to \"signal\"!')\n\n    plot_cols: list[Any]\n    if col is not None:\n        if col == \"signal\":\n            plot_cols = [s for s in _signals]\n        elif col_order is None:\n            if pd.api.types.is_categorical_dtype(metadata[col]):\n                plot_cols = list(metadata[col].cat.categories.values)\n            else:\n                plot_cols = sorted(metadata[col].unique())\n        else:\n            avail_cols = list(metadata[col].unique())\n            plot_cols = [c for c in col_order if c in avail_cols]\n    else:\n        plot_cols = [None]\n\n    plot_rows: list[Any]\n    if row is not None:\n        if row == \"signal\":\n            plot_rows = [s for s in _signals]\n        elif row_order is None:\n            if pd.api.types.is_categorical_dtype(metadata[row]):\n                plot_rows = list(metadata[row].cat.categories.values)\n            else:\n                plot_rows = sorted(metadata[row].unique())\n        else:\n            avail_rows = list(metadata[row].unique())\n            plot_rows = [r for r in row_order if r in avail_rows]\n    else:\n        plot_rows = [None]\n\n    if hue is not None and hue_order is None:\n        if hue == \"signal\":\n            hue_order = [s for s in _signals]\n        elif pd.api.types.is_categorical_dtype(metadata[hue]):\n            hue_order = metadata[hue].cat.categories.values\n        else:\n            hue_order = sorted(metadata[hue].unique())\n\n    use_palette: list[str] = []\n    if hue_order is not None:\n        if palette is None:\n            # default palette\n            _palette = sns.color_palette(\"colorblind\", n_colors=len(hue_order))\n            use_palette = [_palette[i] for i in range(len(hue_order))]\n        elif isinstance(palette, Mapping):\n            # we got a dict-like of categories -&gt; colors\n            use_palette = [palette[item] for item in hue_order]\n        else:\n            # list or string, it's seaborn's problem now\n            _palette = sns.color_palette(palette, n_colors=len(hue_order))\n            use_palette = [_palette[i] for i in range(len(hue_order))]\n\n    fig, axs = plt.subplots(\n        len(plot_rows),\n        len(plot_cols),\n        figsize=(len(plot_cols) * (height * aspect), len(plot_rows) * height),\n        sharey=sharey,\n        sharex=sharex,\n        squeeze=False,\n    )\n\n    sig_to_plot = _signals[0]\n    for row_i, cur_row in enumerate(plot_rows):\n        if cur_row is not None:\n            if row == \"signal\":\n                row_criteria = np.ones(len(metadata.index), dtype=bool)\n                row_title = None\n                sig_to_plot = cur_row\n            else:\n                row_criteria = metadata[row] == cur_row\n                row_title = f\"{row} = {cur_row}\"\n        else:\n            row_criteria = np.ones(len(metadata.index), dtype=bool)\n            row_title = None\n\n        for col_i, cur_col in enumerate(plot_cols):\n            if cur_col is not None:\n                if col == \"signal\":\n                    col_criteria = np.ones(len(metadata.index), dtype=bool)\n                    col_title = None\n                    sig_to_plot = cur_col\n                else:\n                    col_criteria = metadata[col] == cur_col\n                    col_title = f\"{col} = {cur_col}\"\n            else:\n                col_criteria = np.ones(len(metadata.index), dtype=bool)\n                col_title = None\n\n            ax = axs[row_i, col_i]\n            ax.xaxis.set_tick_params(labelbottom=True)\n            ax.yaxis.set_tick_params(labelbottom=True)\n\n            if hue == \"signal\":\n                title = \"\"\n                if col_title is not None or row_title is not None:\n                    title += \" &amp; \".join([t for t in [col_title, row_title] if t is not None])\n                ax.set_title(title)\n            else:\n                title = f\"{sig_to_plot}\"\n                if col_title is not None or row_title is not None:\n                    title += \" at \" + \" &amp; \".join([t for t in [col_title, row_title] if t is not None])\n                ax.set_title(title)\n\n            if hue is None:\n                try:\n                    sig = sessions.select(row_criteria, col_criteria).aggregate_signals(sig_to_plot, method=agg_method)\n                    plot_signal(\n                        sig,\n                        ax=ax,\n                        show_indv=show_indv,\n                        color=use_palette[0],\n                        indv_c=use_palette[0],\n                        indv_alpha=indv_alpha,\n                        indv_kwargs=indv_kwargs,\n                        agg_kwargs=agg_kwargs,\n                    )\n                except:\n                    pass\n\n            elif hue_order is not None:\n                legend_items = []\n                legend_labels = []\n                for hi, curr_hue in enumerate(hue_order):\n                    try:\n                        if hue == \"signal\":\n                            sess_subset = sessions.select(row_criteria, col_criteria)\n                            sig_to_plot = curr_hue\n                        else:\n                            sess_subset = sessions.select(row_criteria, col_criteria, metadata[hue] == curr_hue)\n\n                        if len(sess_subset) &gt; 0:\n                            sig = sess_subset.aggregate_signals(sig_to_plot, method=agg_method)\n                            plot_signal(\n                                sig,\n                                ax=ax,\n                                show_indv=show_indv,\n                                color=use_palette[hi],\n                                indv_c=use_palette[hi],\n                                indv_kwargs=indv_kwargs,\n                                agg_kwargs=agg_kwargs,\n                            )\n\n                        legend_items.append(Line2D([0], [0], color=use_palette[hi]))\n                        legend_labels.append(f\"{curr_hue}, n={len(sess_subset)}\")\n\n                    except:\n                        raise\n\n                ax.legend(legend_items, legend_labels, loc=\"upper right\")\n                # sns.move_legend(ax, loc=\"upper left\", bbox_to_anchor=(1, 1))\n    return fig, axs\n</code></pre>"}]}