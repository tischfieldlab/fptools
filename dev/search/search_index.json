{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"fptools","text":""},{"location":"#tools-for-fiber-photometry-analysis","title":"Tools for Fiber Photometry Analysis","text":"<p>Codes/Notebooks to analyze Fiber Photometry and behavioral data</p> <p>See also related package for analyzing event data from TDT and MedAssociate systems: med-associates-utils</p>"},{"location":"#about","title":"About","text":"<p>This package allows you to load data produced by TDT Synapse software, and eases working with the resulting data. Also includes a growing library of analysis routines.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>read TDT tank files</li> <li>metadata management and propagation</li> <li>more being added with time</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>We recommend you use anaconda virtual environments.</p> <p>There are two main ways to install this package. The first, create the virtual environment, and install this package directly (more useful on \"production\" systems). The second, clone the repository, and then pass to anaconda the <code>environment.yaml</code> file during environment creation (more useful for development). <pre><code>conda create -n fptools python=3.12\npip install git+https://github.com/tischfieldlab/fptools.git\n</code></pre> OR <pre><code>git clone https://github.com/tischfieldlab/fptools.git\ncd fptools\nconda env create -f environment.yml\n</code></pre></p>"},{"location":"#support","title":"Support","text":"<p>For technical inquiries specific to this package, please open an Issue with a description of your problem or request.</p> <p>For general usage, see the main website.</p> <p>Other questions? Reach out to <code>thackray@rutgers.edu</code>.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"loading/","title":"Loading Data","text":"<p><code>fptools</code> offers robust data loading functionality for a variety of file types, including metadata injection, parallelism, caching, and preprocessing. Built-in are loaders for TDT tanks and Med-Associates files. You may also specify your own data loading functionality for arbitrary data which plugs into the <code>fptools</code> data loading infrastructure.</p>"},{"location":"loading/#signal-map","title":"Signal Map","text":"<p>Signal maps tell the system what data to load, the relationship between different streams, and allows renaming of data streams.</p>"},{"location":"loading/#manifest","title":"Manifest","text":"<p>Provide a tabular data file (ex: xlsx, csv, tsv), and the fields in that row will be added to a given sessions metadata.</p>"},{"location":"loading/#parallelism-and-caching","title":"Parallelism and Caching","text":"<p>Data loading can occur in parallel. Just specify the number of workers to use via the <code>max_workers</code> parameter. Each worker runs in a separate process, suitable for running preprocessing routines. The optimal number of workers would depend on the resources of the computer running the analysis.</p> <p>Preprocessed data can be cached for quick retrieval later, without needing to re-perform expensive operations. To enable caching, set the <code>cache</code> parameter to <code>True</code>, setting to <code>False</code> will disable the cache. Cached data needs to be stored someplace on disk, and can be controlled by providing a filesystem path to the <code>cache_dir</code> parameter to a directory to contain the cache.</p>"},{"location":"loading/#preprocessors","title":"Preprocessors","text":"<p>We offer several preprocessing routines you may choose from, or you may provide your own implementation; simply pass a function to the <code>preprocess</code>. If no preprocessor implementation is passed, the signals specified in the <code>signal_map</code> are simply added to the <code>Session</code> with no further changes.</p>"},{"location":"loading/#datalocators-loaders-and-datatypeadaptors","title":"DataLocators, Loaders and DataTypeAdaptors","text":"<p>The <code>load_data()</code> function takes a parameter <code>locator</code> which allows flexibility for finding a loading arbitrary data. For most users, the <code>locator</code> parameter can be set to the special strings <code>tdt</code>, <code>ma</code> or <code>auto</code> to find TDT blocks, med-associates data files, or a combination of the two, respectively.</p> <p>For more advanced use cases, one may supply a function that implements the <code>DataLocator</code> protocol. The purpose of a <code>DataLocator</code> is to locate data, returning a list of <code>DataTypeAdaptor</code>s, with each <code>DataTypeAdaptor</code> corresponding to one <code>Session</code>. The <code>DataTypeAdaptor</code> should be populated with a <code>name</code> for the eventually created <code>Session</code>, a <code>path</code> to the data, and finally a list of one or more functions implementing the <code>Loader</code> protocol. A <code>Loader</code> receives a <code>Session</code> and path (from the <code>DataTypeAdaptor</code>) and is responsible for reading data from that path and populating the <code>Session</code> with the loaded data.</p>"},{"location":"loading/#example","title":"Example","text":"<p>See the notebook <code>01_Data_Loading.ipynb</code> for an example of data loading.</p>"},{"location":"mkdocs/","title":"Mkdocs","text":""},{"location":"mkdocs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"mkdocs/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"session/","title":"Sessions and SessionCollections","text":"<p>A <code>Session</code> serves as the basic container for data. It typically would correspond to data from a single animal and a single bout of recording. One or more <code>Session</code>s can be added to a <code>SessionCollection</code>. A <code>SessionCollection</code> offers several convenience methods for working with many sessions. Many functions in this package accept a <code>Session</code> or <code>SessionCollection</code>.</p> <p><code>Session</code>s typically will contain the following types of data on the attributes:</p> <ul> <li><code>metadata</code>: arbitrary metadata for a given session</li> <li><code>signals</code>: continuous-valued time-series data sampled at fixed intervals</li> <li><code>epocs</code>: discrete event timestamps</li> <li><code>scalars</code>: single-value stores</li> </ul>"},{"location":"session/#metadata","title":"Metadata","text":"<p>Sessions can keep track of metadata about a recording as a simple dictionary, accessible via the <code>metadata</code> property. Metadata is typically populated when a session is loaded, but metadata can be added at anytime.</p> <p>When working with a <code>SessionCollection</code>, metadata for all sessions can be returned as a <code>pandas.DataFrame</code> from the <code>.metadata</code> property. There are several convenience methods that are useful for decorating sessions with addition metadata.</p>"},{"location":"session/#signals","title":"Signals","text":"<p>A <code>Signal</code> encapsulates continuous-valued time-series data sampled at fixed intervals, along with some metadata, such as <code>name</code>, <code>unit</code>, <code>marks</code>. A <code>Signal</code> can describe itself.</p> <pre><code>&gt; signal.describe()\nDopamine:\n    units = \u0394F/F\n    n_observations = 1\n    n_samples = 366051\n    duration = 0:59:58.417848\n    sample_rate = 101.72526245132634\n</code></pre>"},{"location":"session/#shape-of-signalsignal","title":"Shape of <code>Signal.signal</code>","text":"<p>Signal data can be described by the number of samples it contains (i.e. the length of the array), and is retrievable by <code>Signal.nsamples</code>. The duration, as a wall clock amount of time, can be retrieved as a <code>datetime.timedelta</code> by <code>Signal.duration</code>, and is equivalent to <code>Signal.nsamples * Signal.fs</code>. </p> <p>A <code>Signal</code> can contain data from a single observation (in this case <code>Signal.nobs == 1</code>), or from multiple observation (typical in the case of the return from <code>collect_signals()</code>, in this case <code>Signal.nobs &gt; 1</code>). In the case of a single observation, the shape of the signal will be 1D, while in the case of multiple observations, the signal will be 2D <code>(nobs, nsamples)</code>.</p>"},{"location":"session/#signal-aggregation","title":"Signal aggregation","text":"<p>In the case there are multiple observations, one would commonly want to aggregate to produce a mean/median signal representative of all observations. To do so, use the <code>aggregate()</code> method of the signal object. This method return a new <code>Signal</code> with propagated marks, units, sampling frequency and time. The new signal will be named according to this signal, with <code>#{func}</code> appended. The parameter <code>func</code> determines how the signal is aggregated, and can accept several types of argument.</p> <ul> <li>simple strings, like <code>mean</code>, <code>median</code>, <code>min</code>, <code>max</code> will invoke the <code>numpy</code> implementation (really any numpy function that is a <code>ufunc</code> and exists in the global <code>numpy</code> namespace)</li> <li>reference to a <code>numpy</code> <code>ufunc</code>, such as <code>numpy.mean</code></li> <li>any other arbitrary function that accepts and array and returns an array</li> </ul>"},{"location":"session/#epochs","title":"Epochs","text":"<p>Epochs contain behavioral data. Currently, these are stored as a simple dictionary on <code>Session</code> objects, with string keys as the event name, and the values as numpy arrays of relative event times, in seconds.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>fptools<ul> <li>io<ul> <li>common</li> <li>data_loader</li> <li>med_associates</li> <li>session</li> <li>signal</li> <li>tdt</li> <li>test</li> </ul> </li> <li>measure<ul> <li>peaks</li> <li>signal_collector</li> <li>snr</li> </ul> </li> <li>preprocess<ul> <li>common</li> <li>lib</li> <li>pipelines<ul> <li>dxp_motion_dff</li> <li>lowpass_dff</li> <li>tdt_default</li> </ul> </li> <li>steps<ul> <li>dbl_exp_fit</li> <li>dff</li> <li>downsample</li> <li>lowpass</li> <li>motion_correct</li> <li>rename</li> <li>trim_signals</li> <li>zscore</li> </ul> </li> </ul> </li> <li>viz<ul> <li>behavior<ul> <li>cumulative</li> <li>raster</li> </ul> </li> <li>common</li> <li>signal</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/fptools/","title":"fptools","text":""},{"location":"reference/fptools/#fptools","title":"<code>fptools</code>","text":"<p>Modules:</p> <ul> <li> <code>io</code>           \u2013            </li> <li> <code>measure</code>           \u2013            </li> <li> <code>preprocess</code>           \u2013            </li> <li> <code>viz</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/io/","title":"io","text":""},{"location":"reference/fptools/io/#fptools.io","title":"<code>fptools.io</code>","text":"<p>Modules:</p> <ul> <li> <code>common</code>           \u2013            </li> <li> <code>data_loader</code>           \u2013            </li> <li> <code>med_associates</code>           \u2013            </li> <li> <code>session</code>           \u2013            </li> <li> <code>signal</code>           \u2013            </li> <li> <code>tdt</code>           \u2013            </li> <li> <code>test</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/io/common/","title":"common","text":""},{"location":"reference/fptools/io/common/#fptools.io.common","title":"<code>fptools.io.common</code>","text":"<p>Classes:</p> <ul> <li> <code>DataLocator</code>           \u2013            </li> <li> <code>DataTypeAdaptor</code>           \u2013            </li> <li> <code>Loader</code>           \u2013            </li> <li> <code>Preprocessor</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/io/common/#fptools.io.common.DataLocator","title":"<code>DataLocator</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Data Locator Protocol.</p> </li> </ul> Source code in <code>fptools/io/common.py</code> <pre><code>class DataLocator(Protocol):\n    def __call__(self, path: str) -&gt; list[DataTypeAdaptor]:\n        \"\"\"Data Locator Protocol.\n\n        A data locator should take a path, and search that path for data files\n        to be loaded into a session. For each candidate, it should generate a DataTypeAdaptor\n\n        Args:\n            path: path to search for data files\n\n        Returns:\n            list of DataTypeAdaptor, each adaptor corresponding to one session, of data to be loaded\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/fptools/io/common/#fptools.io.common.DataLocator.__call__","title":"<code>__call__(path)</code>","text":"<p>Data Locator Protocol.</p> <p>A data locator should take a path, and search that path for data files to be loaded into a session. For each candidate, it should generate a DataTypeAdaptor</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to search for data files</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[DataTypeAdaptor]</code>           \u2013            <p>list of DataTypeAdaptor, each adaptor corresponding to one session, of data to be loaded</p> </li> </ul> Source code in <code>fptools/io/common.py</code> <pre><code>def __call__(self, path: str) -&gt; list[DataTypeAdaptor]:\n    \"\"\"Data Locator Protocol.\n\n    A data locator should take a path, and search that path for data files\n    to be loaded into a session. For each candidate, it should generate a DataTypeAdaptor\n\n    Args:\n        path: path to search for data files\n\n    Returns:\n        list of DataTypeAdaptor, each adaptor corresponding to one session, of data to be loaded\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fptools/io/common/#fptools.io.common.DataTypeAdaptor","title":"<code>DataTypeAdaptor</code>","text":"<p>Methods:</p> <ul> <li> <code>__init__</code>             \u2013              <p>Initialize this DataTypeAdaptor.</p> </li> </ul> Source code in <code>fptools/io/common.py</code> <pre><code>class DataTypeAdaptor:\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize this DataTypeAdaptor.\"\"\"\n        self.name: str\n        self.path: str\n        self.loaders: list[Loader] = []\n</code></pre>"},{"location":"reference/fptools/io/common/#fptools.io.common.DataTypeAdaptor.__init__","title":"<code>__init__()</code>","text":"<p>Initialize this DataTypeAdaptor.</p> Source code in <code>fptools/io/common.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize this DataTypeAdaptor.\"\"\"\n    self.name: str\n    self.path: str\n    self.loaders: list[Loader] = []\n</code></pre>"},{"location":"reference/fptools/io/common/#fptools.io.common.Loader","title":"<code>Loader</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Data Loader Protocol.</p> </li> </ul> Source code in <code>fptools/io/common.py</code> <pre><code>class Loader(Protocol):\n    def __call__(self, session: Session, path: str) -&gt; Session:\n        \"\"\"Data Loader Protocol.\n\n        A Loader receives a session instance and a path to data. It should load this data into\n        the Session instance and return back the session.\n\n        Args:\n            session: the session for data to be loaded into\n            path: path to some data that should be loaded\n\n        Returns:\n            Session object with data added\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/fptools/io/common/#fptools.io.common.Loader.__call__","title":"<code>__call__(session, path)</code>","text":"<p>Data Loader Protocol.</p> <p>A Loader receives a session instance and a path to data. It should load this data into the Session instance and return back the session.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session for data to be loaded into</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to some data that should be loaded</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session object with data added</p> </li> </ul> Source code in <code>fptools/io/common.py</code> <pre><code>def __call__(self, session: Session, path: str) -&gt; Session:\n    \"\"\"Data Loader Protocol.\n\n    A Loader receives a session instance and a path to data. It should load this data into\n    the Session instance and return back the session.\n\n    Args:\n        session: the session for data to be loaded into\n        path: path to some data that should be loaded\n\n    Returns:\n        Session object with data added\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fptools/io/common/#fptools.io.common.Preprocessor","title":"<code>Preprocessor</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Preprocessor protocol.</p> </li> </ul> Source code in <code>fptools/io/common.py</code> <pre><code>class Preprocessor(Protocol):\n    def __call__(self, session: Session, signal_map: list[SignalMapping], **kwargs) -&gt; Session:\n        \"\"\"Preprocessor protocol.\n\n        Args:\n            session: the session to operate upon\n            signal_map: mapping of signal information\n            **kwargs: additional kwargs a preprocessor might need\n\n        Returns:\n            Session object with preprocessed data added.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/fptools/io/common/#fptools.io.common.Preprocessor.__call__","title":"<code>__call__(session, signal_map, **kwargs)</code>","text":"<p>Preprocessor protocol.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to operate upon</p> </li> <li> <code>signal_map</code>               (<code>list[SignalMapping]</code>)           \u2013            <p>mapping of signal information</p> </li> <li> <code>**kwargs</code>           \u2013            <p>additional kwargs a preprocessor might need</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session object with preprocessed data added.</p> </li> </ul> Source code in <code>fptools/io/common.py</code> <pre><code>def __call__(self, session: Session, signal_map: list[SignalMapping], **kwargs) -&gt; Session:\n    \"\"\"Preprocessor protocol.\n\n    Args:\n        session: the session to operate upon\n        signal_map: mapping of signal information\n        **kwargs: additional kwargs a preprocessor might need\n\n    Returns:\n        Session object with preprocessed data added.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fptools/io/data_loader/","title":"data_loader","text":""},{"location":"reference/fptools/io/data_loader/#fptools.io.data_loader","title":"<code>fptools.io.data_loader</code>","text":"<p>Functions:</p> <ul> <li> <code>load_data</code>             \u2013              <p>Load blocks from <code>tank_path</code> and return a <code>SessionCollection</code>.</p> </li> <li> <code>load_manifest</code>             \u2013              <p>Load a manifest file, accepting most common tabular formats.</p> </li> </ul>"},{"location":"reference/fptools/io/data_loader/#fptools.io.data_loader._find_data","title":"<code>_find_data(path)</code>","text":"<p>Data locator for type \"auto\".</p> <p>Tries to find any type of data (TDT and med-associates currently supported)</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to search for data</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[DataTypeAdaptor]</code>           \u2013            <p>list of DataTypeAdaptor, each adaptor corresponding to one session, of data to be loaded</p> </li> </ul> Source code in <code>fptools/io/data_loader.py</code> <pre><code>def _find_data(path: str) -&gt; list[DataTypeAdaptor]:\n    \"\"\"Data locator for type \"auto\".\n\n    Tries to find any type of data (TDT and med-associates currently supported)\n\n    Args:\n        path: path to search for data\n\n    Returns:\n        list of DataTypeAdaptor, each adaptor corresponding to one session, of data to be loaded\n    \"\"\"\n    return [*find_tdt_blocks(path), *find_ma_blocks(path)]\n</code></pre>"},{"location":"reference/fptools/io/data_loader/#fptools.io.data_loader._get_locator","title":"<code>_get_locator(locator='auto')</code>","text":"<p>Translate a flexible locator argument to a concrete DataLoader implementation.</p> <p>Parameters:</p> <ul> <li> <code>locator</code>               (<code>Union[Literal['auto', 'tdt', 'ma'], DataLocator]</code>, default:                   <code>'auto'</code> )           \u2013            <p>type of locator to return. Several special strings supported or a concrete DataLoader implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLocator</code>           \u2013            <p>A concrete DataLoader</p> </li> </ul> Source code in <code>fptools/io/data_loader.py</code> <pre><code>def _get_locator(locator: Union[Literal[\"auto\", \"tdt\", \"ma\"], DataLocator] = \"auto\") -&gt; DataLocator:\n    \"\"\"Translate a flexible locator argument to a concrete DataLoader implementation.\n\n    Args:\n        locator: type of locator to return. Several special strings supported or a concrete DataLoader implementation.\n\n    Returns:\n        A concrete DataLoader\n    \"\"\"\n    if locator == \"auto\":\n        return _find_data\n    elif locator == \"tdt\":\n        return find_tdt_blocks\n    elif locator == \"ma\":\n        return find_ma_blocks\n    else:\n        return locator\n</code></pre>"},{"location":"reference/fptools/io/data_loader/#fptools.io.data_loader._load","title":"<code>_load(dset, preprocess=None, cache=True, cache_dir='cache')</code>","text":"<p>Load data for the given DataTypeAdaptor.</p> <p>Handles session instance creation, loading across possibly multiple loaders, caching, preprocessing</p> <p>Parameters:</p> <ul> <li> <code>dset</code>               (<code>DataTypeAdaptor</code>)           \u2013            <p>a DataTypeAdaptor instance describing what data and how to load it</p> </li> <li> <code>signal_map</code>           \u2013            <p>used to inform the preprocessor about data relationships</p> </li> <li> <code>preprocess</code>               (<code>Optional[Preprocessor]</code>, default:                   <code>None</code> )           \u2013            <p>preprocess routine to run on the data.</p> </li> <li> <code>cache</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, results will be cached for future use, or results will be loaded from the cache.</p> </li> <li> <code>cache_dir</code>               (<code>str</code>, default:                   <code>'cache'</code> )           \u2013            <p>path to the cache</p> </li> <li> <code>**kwargs</code>           \u2013            <p>additional keyword arguments to pass to the <code>preprocess</code> callable.</p> </li> </ul> Source code in <code>fptools/io/data_loader.py</code> <pre><code>def _load(\n    dset: DataTypeAdaptor,\n    preprocess: Optional[Preprocessor] = None,\n    cache: bool = True,\n    cache_dir: str = \"cache\",\n) -&gt; Session:\n    \"\"\"Load data for the given DataTypeAdaptor.\n\n    Handles session instance creation, loading across possibly multiple loaders, caching, preprocessing\n\n    Args:\n        dset: a DataTypeAdaptor instance describing what data and how to load it\n        signal_map: used to inform the preprocessor about data relationships\n        preprocess: preprocess routine to run on the data.\n        cache: If `True`, results will be cached for future use, or results will be loaded from the cache.\n        cache_dir: path to the cache\n        **kwargs: additional keyword arguments to pass to the `preprocess` callable.\n    \"\"\"\n    cache_path = os.path.join(cache_dir, f\"{dset.name}.h5\")\n\n    if cache and os.path.exists(cache_path):\n        # check if the cached version exists, if so, load and return that\n        print(f'loading cache: \"{cache_path}\"')\n        return Session.load(cache_path)\n\n    else:\n        # otherwise, we need to load the data from scratch\n\n        # load the data\n        session = Session()\n        session.name = dset.name\n        for loader in dset.loaders:\n            session = loader(session, dset.path)\n\n        # preprocess data if preprocessor is specified\n        if preprocess is not None:\n            session = preprocess(session)\n\n        # cache the session, if requested\n        if cache:\n            session.save(cache_path)\n\n        return session\n</code></pre>"},{"location":"reference/fptools/io/data_loader/#fptools.io.data_loader.load_data","title":"<code>load_data(tank_path, manifest_path=None, manifest_index='blockname', max_workers=None, locator='auto', preprocess=None, cache=True, cache_dir='cache')</code>","text":"<p>Load blocks from <code>tank_path</code> and return a <code>SessionCollection</code>.</p> <p>Loading will happen in parallel, split across <code>max_workers</code> worker processes.</p> <p>For quicker future loading, results may be cached. Caching is controlled by the <code>cache</code> parameter, and the location of cached files is controlled by the <code>cache_dir</code> parameter.</p> <p>You can specify a manifest (in TSV, CSV or XLSX formats) containing additional metadata to be injected into the loaded data. This manifest should have at minimum one column with header <code>blockname</code> containing each block's name. You may include any other arbitrary data columns you may wish. One special column name is <code>exclude</code> which should contain boolean <code>True</code> or <code>False</code>. If a block is marked with <code>True</code> in this column, then the block will not be loaded or returned in the resulting <code>SessionCollection</code>.</p> <p>You can also specify a preprocess routine to be applied to each block prior to being returned via the <code>preprocess</code> parameter. This should be a callable taking a <code>Session</code> as the first and only parameter. Your callable preprocess routine should attach any data to the passed <code>Session</code> object and return this <code>Session</code> object as it's sole return value. For example preprocessing routines, please see the implementations in the <code>tdt.preprocess.pipelines</code> module.</p> <p>Parameters:</p> <ul> <li> <code>tank_path</code>               (<code>str</code>)           \u2013            <p>path that will be recursively searched for blocks</p> </li> <li> <code>manifest_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>if provided, path to metadata in a tabular format, indexed with <code>blockname</code>. See above for more details</p> </li> <li> <code>manifest_index</code>               (<code>str</code>, default:                   <code>'blockname'</code> )           \u2013            <p>the name of the column to be set as the manifest DataFrame index.</p> </li> <li> <code>max_workers</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>number of workers in the process pool for loading blocks. If None, defaults to the number of CPUs on the machine.</p> </li> <li> <code>locator</code>               (<code>Union[Literal['auto', 'tdt', 'ma'], DataLocator]</code>, default:                   <code>'auto'</code> )           \u2013            <p>locator to use for finding data on <code>tank_path</code></p> </li> <li> <code>preprocess</code>               (<code>Optional[Preprocessor]</code>, default:                   <code>None</code> )           \u2013            <p>preprocess routine to run on the data. See above for more details.</p> </li> <li> <code>cache</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, results will be cached for future use, or results will be loaded from the cache.</p> </li> <li> <code>cache_dir</code>               (<code>str</code>, default:                   <code>'cache'</code> )           \u2013            <p>path to the cache</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SessionCollection</code>           \u2013            <p><code>SessionCollection</code> containing loaded data</p> </li> </ul> Source code in <code>fptools/io/data_loader.py</code> <pre><code>def load_data(\n    tank_path: str,\n    manifest_path: Optional[str] = None,\n    manifest_index: str = \"blockname\",\n    max_workers: Optional[int] = None,\n    locator: Union[Literal[\"auto\", \"tdt\", \"ma\"], DataLocator] = \"auto\",\n    preprocess: Optional[Preprocessor] = None,\n    cache: bool = True,\n    cache_dir: str = \"cache\",\n) -&gt; SessionCollection:\n    \"\"\"Load blocks from `tank_path` and return a `SessionCollection`.\n\n    Loading will happen in parallel, split across `max_workers` worker processes.\n\n    For quicker future loading, results may be cached. Caching is controlled by the `cache` parameter, and the location of cached\n    files is controlled by the `cache_dir` parameter.\n\n    You can specify a manifest (in TSV, CSV or XLSX formats) containing additional metadata to be injected into the loaded data.\n    This manifest should have at minimum one column with header `blockname` containing each block's name. You may include any other arbitrary\n    data columns you may wish. One special column name is `exclude` which should contain boolean `True` or `False`. If a block\n    is marked with `True` in this column, then the block will not be loaded or returned in the resulting `SessionCollection`.\n\n    You can also specify a preprocess routine to be applied to each block prior to being returned via the `preprocess` parameter. This\n    should be a callable taking a `Session` as the first and only parameter. Your callable preprocess routine should attach any data to the passed\n    `Session` object and return this `Session` object as it's sole return value.\n    For example preprocessing routines, please see the implementations in the `tdt.preprocess.pipelines` module.\n\n    Args:\n        tank_path: path that will be recursively searched for blocks\n        manifest_path: if provided, path to metadata in a tabular format, indexed with `blockname`. See above for more details\n        manifest_index: the name of the column to be set as the manifest DataFrame index.\n        max_workers: number of workers in the process pool for loading blocks. If None, defaults to the number of CPUs on the machine.\n        locator: locator to use for finding data on `tank_path`\n        preprocess: preprocess routine to run on the data. See above for more details.\n        cache: If `True`, results will be cached for future use, or results will be loaded from the cache.\n        cache_dir: path to the cache\n\n    Returns:\n        `SessionCollection` containing loaded data\n    \"\"\"\n    has_manifest = False\n    if manifest_path is not None:\n        manifest = load_manifest(manifest_path, index=manifest_index)\n        has_manifest = True\n\n    # if caching is enabled, make sure the cache directory exists\n    if cache:\n        os.makedirs(cache_dir, exist_ok=True)\n\n    futures: dict[concurrent.futures.Future[Session], str] = {}\n    sessions = SessionCollection()\n    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n\n        # iterate over all datasets found by the locator\n        for dset in _get_locator(locator)(tank_path):\n\n            # check if we were given a manifest. If so, try to load metadata from the manifest\n            # also perform some sanity checks along the way, and check some special flags (ex `exclude`)\n            if has_manifest:\n                try:\n                    block_meta = manifest.loc[dset.name]\n                except KeyError:\n                    # this block is not listed in the manifest! Err on the side of caution and exclude the block\n                    tqdm.write(f'WARNING: Excluding block \"{dset.name}\" because it is not listed in the manifest!!')\n                    continue\n\n                # possibly exclude the block, if flagged in the manifest\n                if \"exclude\" in block_meta and block_meta[\"exclude\"]:\n                    tqdm.write(f'Excluding block \"{dset.name}\" due to manifest exclude flag')\n                    continue\n\n            # submit the task to the pool\n            f = executor.submit(_load, dset, preprocess=preprocess, cache=cache, cache_dir=cache_dir)\n            futures[f] = dset.name\n\n        # collect completed tasks\n        for f in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n            try:\n                s = f.result()\n                if has_manifest:\n                    s.metadata.update(manifest.loc[futures[f]].to_dict())\n                sessions.append(s)\n            except Exception as e:\n                tqdm.write(\n                    f'Encountered problem loading data at \"{futures[f]}\":\\n{traceback.format_exc()}\\nData will be omitted from the final collection!\\n'\n                )\n                pass\n\n    return sessions\n</code></pre>"},{"location":"reference/fptools/io/data_loader/#fptools.io.data_loader.load_manifest","title":"<code>load_manifest(path, index=None)</code>","text":"<p>Load a manifest file, accepting most common tabular formats.</p> <p>*.tsv (tab-separated values), *.csv (comma-separated values), or *.xlsx (Excel) file extensions are supported.</p> <p>Optionally index the dataframe with one of the loaded columns.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to the file to load</p> </li> <li> <code>index</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>if not None, set this column to be the index of the DataFrame</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pandas.DataFrame containing the manifest data.</p> </li> </ul> Source code in <code>fptools/io/data_loader.py</code> <pre><code>def load_manifest(path: str, index: Optional[str] = None) -&gt; pd.DataFrame:\n    \"\"\"Load a manifest file, accepting most common tabular formats.\n\n    *.tsv (tab-separated values), *.csv (comma-separated values), or *.xlsx (Excel) file extensions are supported.\n\n    Optionally index the dataframe with one of the loaded columns.\n\n    Args:\n        path: path to the file to load\n        index: if not None, set this column to be the index of the DataFrame\n\n    Returns:\n        pandas.DataFrame containing the manifest data.\n    \"\"\"\n    ext = os.path.splitext(path)[1]\n\n    df: pd.DataFrame\n    if ext == \".tsv\":\n        df = pd.read_csv(path, sep=\"\\t\")\n    elif ext == \".csv\":\n        df = pd.read_csv(path)\n    elif ext == \".xlsx\":\n        df = pd.read_excel(path)\n    else:\n        raise ValueError(\n            f'Did not understand manifest format. Supported file extensions are *.tsv (tab-separated), *.csv (comma-separated), or *.xlsx (Excel), but got \"{ext}\"'\n        )\n\n    if index is not None:\n        if index in df.columns:\n            df.set_index(index, inplace=True)\n        else:\n            raise ValueError(f\"Cannot set manifest index to column {index}; available columns: {df.columns.values}\")\n\n    return df\n</code></pre>"},{"location":"reference/fptools/io/med_associates/","title":"med_associates","text":""},{"location":"reference/fptools/io/med_associates/#fptools.io.med_associates","title":"<code>fptools.io.med_associates</code>","text":"<p>Functions:</p> <ul> <li> <code>find_ma_blocks</code>             \u2013              <p>Data Locator for med-associates files.</p> </li> <li> <code>is_file_ma</code>             \u2013              <p>Test if a file looks like a med-associates data file.</p> </li> <li> <code>parse_ma_session</code>             \u2013              <p>Parse a session data file from MedAssociates.</p> </li> </ul>"},{"location":"reference/fptools/io/med_associates/#fptools.io.med_associates._parse_line","title":"<code>_parse_line(line)</code>","text":"<p>Parse a single session data file line.</p> <p>Do a regex search against all defined regexes and return the key and match result of the first matching regex</p> <p>Parameters:</p> <ul> <li> <code>line</code>               (<code>str</code>)           \u2013            <p>the line to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>(None, None) if no match was found, otherwise the key and match object</p> </li> </ul> Source code in <code>fptools/io/med_associates.py</code> <pre><code>def _parse_line(line: str):\n    \"\"\"Parse a single session data file line.\n\n    Do a regex search against all defined regexes and\n    return the key and match result of the first matching regex\n\n    Args:\n        line: the line to parse\n\n    Returns:\n        (None, None) if no match was found, otherwise the key and match object\n    \"\"\"\n    for key, rx in _rx_dict.items():\n        match = rx.search(line)\n        if match:\n            return key, match\n    # if there are no matches\n    return None, None\n</code></pre>"},{"location":"reference/fptools/io/med_associates/#fptools.io.med_associates.find_ma_blocks","title":"<code>find_ma_blocks(path, pattern='*.txt')</code>","text":"<p>Data Locator for med-associates files.</p> <p>Given a path to a directory, will search that path recursively for med-associates files.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to search for med-associates data files</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[DataTypeAdaptor]</code>           \u2013            <p>list of DataTypeAdaptor, each adaptor corresponding to one session, of data to be loaded</p> </li> </ul> Source code in <code>fptools/io/med_associates.py</code> <pre><code>def find_ma_blocks(path: str, pattern: str = \"*.txt\") -&gt; list[DataTypeAdaptor]:\n    \"\"\"Data Locator for med-associates files.\n\n    Given a path to a directory, will search that path recursively for med-associates files.\n\n    Args:\n        path: path to search for med-associates data files\n\n    Returns:\n        list of DataTypeAdaptor, each adaptor corresponding to one session, of data to be loaded\n    \"\"\"\n    found_files = list(glob.glob(os.path.join(path, \"**\", pattern), recursive=True))\n    filtered = list(filter(is_file_ma, found_files))\n    items_out = []\n    for f in filtered:\n        adapt = DataTypeAdaptor()\n        adapt.path = f  # path to the med-associates txt file\n        adapt.name = os.path.splitext(os.path.basename(adapt.path))[0]  # the basename of the file without file extensions\n        adapt.loaders.append(parse_ma_session)\n        items_out.append(adapt)\n    return items_out\n</code></pre>"},{"location":"reference/fptools/io/med_associates/#fptools.io.med_associates.is_file_ma","title":"<code>is_file_ma(path)</code>","text":"<p>Test if a file looks like a med-associates data file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to a file to test.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the file looks like a med-associates file, otherwise False.</p> </li> </ul> Source code in <code>fptools/io/med_associates.py</code> <pre><code>def is_file_ma(path: str) -&gt; bool:\n    \"\"\"Test if a file looks like a med-associates data file.\n\n    Args:\n        path: path to a file to test.\n\n    Returns:\n        True if the file looks like a med-associates file, otherwise False.\n    \"\"\"\n    with open(path, mode=\"r\") as f:\n        first_line = f.readline()\n        if first_line.startswith(\"File: \"):\n            return True\n    return False\n</code></pre>"},{"location":"reference/fptools/io/med_associates/#fptools.io.med_associates.parse_ma_session","title":"<code>parse_ma_session(session, path)</code>","text":"<p>Parse a session data file from MedAssociates.</p> <p>Adapted from https://github.com/matthewperkins/MPCdata, but fixes some issues.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Filepath for file_object to be parsed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>SessionCollection</p> </li> </ul> Source code in <code>fptools/io/med_associates.py</code> <pre><code>def parse_ma_session(session: Session, path: str) -&gt; Session:\n    \"\"\"Parse a session data file from MedAssociates.\n\n    Adapted from https://github.com/matthewperkins/MPCdata, but fixes some issues.\n\n    Args:\n        path: Filepath for file_object to be parsed\n\n    Returns:\n        SessionCollection\n    \"\"\"\n    # print(path)\n    MPCDateStringRe = re.compile(r\"\\s*(?P&lt;hour&gt;[0-9]+):(?P&lt;minute&gt;[0-9]{2}):(?P&lt;second&gt;[0-9]{2})\")\n    # open the file and read through it line by line\n    with open(path, \"r\", newline=\"\\n\") as file_object:\n        line = file_object.readline()\n        while line:\n            # at each line check for a match with a regex\n            key, match = _parse_line(line)\n\n            # start of data is '\\r\\r\\n'\n            if key == \"STARTOFDATA\":\n                pass\n\n            # extract start date\n            if key == \"StartDate\":\n                session.metadata[\"StartDate\"] = datetime.datetime.strptime(match.group(key), \"%m/%d/%y\").date()\n\n            # extract end date\n            if key == \"EndDate\":\n                session.metadata[\"EndDate\"] = datetime.datetime.strptime(match.group(key), \"%m/%d/%y\").date()\n\n            # extract start time\n            if key == \"StartTime\":\n                date_match = MPCDateStringRe.search(match.group(key))\n                if date_match is not None:\n                    (hour, min, sec) = [int(date_match.group(g)) for g in [\"hour\", \"minute\", \"second\"]]\n                    session.metadata[\"StartTime\"] = datetime.time(hour, min, sec)\n                    # date should be already read\n                    session.metadata[\"StartDateTime\"] = datetime.datetime.combine(\n                        session.metadata[\"StartDate\"], session.metadata[\"StartTime\"]\n                    )\n\n            # extract end time\n            if key == \"EndTime\":\n                date_match = MPCDateStringRe.search(match.group(key))\n                if date_match is not None:\n                    (hour, min, sec) = [int(date_match.group(g)) for g in [\"hour\", \"minute\", \"second\"]]\n                    session.metadata[\"EndTime\"] = datetime.time(hour, min, sec)\n                    # date should be already read\n                    session.metadata[\"EndDateTime\"] = datetime.datetime.combine(session.metadata[\"EndDate\"], session.metadata[\"EndTime\"])\n\n            # extract Subject\n            if key == \"Subject\":\n                session.metadata[\"Subject\"] = match.group(key)\n\n            # extract Experiment\n            if key == \"Experiment\":\n                session.metadata[\"Experiment\"] = match.group(key)\n\n            # extract Group\n            if key == \"Group\":\n                session.metadata[\"Group\"] = match.group(key)\n\n            # extract Box\n            if key == \"Box\":\n                session.metadata[\"Box\"] = int(match.group(key))\n\n            # extract MSN\n            if key == \"MSN\":\n                session.metadata[\"MSN\"] = match.group(key)\n\n            # extract scalars\n            if key == \"SCALAR\":\n                session.scalars[match.group(\"name\")] = np.array([float(match.group(\"value\"))])\n\n            # identify an array\n            if key == \"ARRAY\":\n                # print(f'This is the beginning of an Array:: \"{line}\"')\n                # have now have to step through the array\n                file_tell = file_object.tell()\n                subline = file_object.readline()\n                # print(f'This is the first line of the array:: \"{subline}\"')\n                items = []\n                while subline:\n                    m = _rx_dict[\"ARRAYidx\"].search(subline)\n                    if m is not None:\n                        items.extend([float(l) for l in m.group(\"list\").split()])\n\n                    else:\n                        # have to rewind\n                        # print(f'This is one line beyond the last line of the array:: \"{subline}\"')\n                        file_object.seek(file_tell)\n                        break\n                    file_tell = file_object.tell()\n                    subline = file_object.readline()\n                # print(f'Setting \"{match.group(\"name\")}\"={items}')\n                session.epocs[match.group(\"name\")] = np.array(items)\n            line = file_object.readline()\n    return session\n</code></pre>"},{"location":"reference/fptools/io/session/","title":"session","text":""},{"location":"reference/fptools/io/session/#fptools.io.session","title":"<code>fptools.io.session</code>","text":"<p>Classes:</p> <ul> <li> <code>Session</code>           \u2013            <p>Holds data and metadata for a single session.</p> </li> <li> <code>SessionCollection</code>           \u2013            <p>Collection of session data.</p> </li> </ul>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session","title":"<code>Session</code>","text":"<p>               Bases: <code>object</code></p> <p>Holds data and metadata for a single session.</p> <p>Methods:</p> <ul> <li> <code>__eq__</code>             \u2013              <p>Test this Session for equality to another Session.</p> </li> <li> <code>__init__</code>             \u2013              <p>Initialize this Session object.</p> </li> <li> <code>add_signal</code>             \u2013              <p>Add a signal to this Session.</p> </li> <li> <code>describe</code>             \u2013              <p>Describe this session.</p> </li> <li> <code>epoc_dataframe</code>             \u2013              <p>Produce a dataframe with epoc data and metadata.</p> </li> <li> <code>load</code>             \u2013              <p>Load a Session from an HDF5 file.</p> </li> <li> <code>rename_epoc</code>             \u2013              <p>Rename a epoc, from <code>old_name</code> to <code>new_name</code>.</p> </li> <li> <code>rename_scalar</code>             \u2013              <p>Rename a scalar, from <code>old_name</code> to <code>new_name</code>.</p> </li> <li> <code>rename_signal</code>             \u2013              <p>Rename a signal, from <code>old_name</code> to <code>new_name</code>.</p> </li> <li> <code>save</code>             \u2013              <p>Save this Session to and HDF5 file.</p> </li> <li> <code>scalar_dataframe</code>             \u2013              <p>Produce a dataframe with scalar data and metadata.</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>class Session(object):\n    \"\"\"Holds data and metadata for a single session.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize this Session object.\"\"\"\n        self.name: str = \"\"\n        self.metadata: dict[str, Any] = {}\n        self.signals: dict[str, Signal] = {}\n        self.epocs: dict[str, np.ndarray] = defaultdict(partial(np.ndarray, 0))\n        self.scalars: dict[str, np.ndarray] = defaultdict()\n\n    def describe(self, as_str: bool = False) -&gt; Union[str, None]:\n        \"\"\"Describe this session.\n\n        describes the metadata, scalars, and arrays contained in this session.\n\n        Args:\n            as_str: if True, return description as a string, otherwise print the description and return None\n\n        Returns:\n            `None` if `as_str` is `False`; if `as_str` is `True`, returns the description as a `str`\n        \"\"\"\n        buffer = f'Session with name \"{self.name}\"\\n\\n'\n\n        buffer += \"Metadata:\\n\"\n        if len(self.metadata) &gt; 0:\n            for k, v in self.metadata.items():\n                buffer += f\"    {k}: {v}\\n\"\n        else:\n            buffer += \"    &lt; No Metadata Available &gt;\\n\"\n        buffer += \"\\n\"\n\n        buffer += \"Epocs:\\n\"\n        if len(self.epocs) &gt; 0:\n            for k, v in self.epocs.items():\n                # buffer += f'    \"{k}\" with shape {v.shape}:\\n    {np.array2string(v, prefix=\"    \")}\\n\\n'\n                buffer += f\"    {k}:\\n\"\n                buffer += f\"        num_events = {v.shape}\\n\"\n                buffer += f\"        avg_rate = {datetime.timedelta(seconds=np.diff(v)[0])}\\n\"\n                buffer += f\"        earliest = {datetime.timedelta(seconds=v[0])}\\n\"\n                buffer += f\"        latest = {datetime.timedelta(seconds=v[-1])}\\n\"\n        else:\n            buffer += \"    &lt; No Epocs Available &gt;\\n\"\n        buffer += \"\\n\"\n\n        buffer += \"Scalars:\\n\"\n        if len(self.scalars) &gt; 0:\n            for k, v in self.scalars.items():\n                buffer += f\"    {k}:\\n\"\n                buffer += f\"        num_events = {v.shape}\\n\"\n                buffer += f\"        earliest = {datetime.timedelta(seconds=v[0])}\\n\"\n                buffer += f\"        latest = {datetime.timedelta(seconds=v[-1])}\\n\"\n        else:\n            buffer += \"    &lt; No Epocs Available &gt;\\n\"\n        buffer += \"\\n\"\n\n        buffer += \"Signals:\\n\"\n        if len(self.signals) &gt; 0:\n            for k, v in self.signals.items():\n                buffer += v.describe(as_str=True, prefix=\"    \")\n        else:\n            buffer += \"    &lt; No Signals Available &gt;\\n\"\n        buffer += \"\\n\"\n\n        if as_str:\n            return buffer\n        else:\n            print(buffer)\n            return None\n\n    def add_signal(self, signal: Signal, overwrite: bool = False) -&gt; None:\n        \"\"\"Add a signal to this Session.\n\n        Raises an error if the new signal name already exists and `overwrite` is not True.\n\n        Args:\n            signal: the signal to add to this Session\n            overwrite: if True, allow overwriting a pre-existing signal with the same name, if False, will raise instead.\n        \"\"\"\n        if signal.name in self.signals and not overwrite:\n            raise KeyError(f\"Key `{signal.name}` already exists in data!\")\n\n        self.signals[signal.name] = signal\n\n    def rename_signal(self, old_name: str, new_name: str) -&gt; None:\n        \"\"\"Rename a signal, from `old_name` to `new_name`.\n\n        Raises an error if the new signal name already exists.\n\n        Args:\n            old_name: the current name for the signal\n            new_name: the new name for the signal\n        \"\"\"\n        if new_name in self.signals:\n            raise KeyError(f\"Key `{new_name}` already exists in data!\")\n\n        self.signals[new_name] = self.signals[old_name]\n        self.signals.pop(old_name)\n\n    def rename_scalar(self, old_name: str, new_name: str):\n        \"\"\"Rename a scalar, from `old_name` to `new_name`.\n\n        Raises an error if the new scalar name already exists.\n\n        Args:\n        old_name: the current name for the scalar\n        new_name: the new name for the scalar\n        \"\"\"\n        if new_name in self.scalars:\n            raise KeyError(f\"Key `{new_name}` already exists in data!\")\n\n        self.scalars[new_name] = self.scalars[old_name]\n        self.scalars.pop(old_name)\n\n    def rename_epoc(self, old_name: str, new_name: str) -&gt; None:\n        \"\"\"Rename a epoc, from `old_name` to `new_name`.\n\n        Raises an error if the new epoc name already exists.\n\n        Args:\n            old_name: the current name for the epoc\n            new_name: the new name for the epoc\n        \"\"\"\n        if new_name in self.epocs:\n            raise KeyError(f\"Key `{new_name}` already exists in data!\")\n\n        self.epocs[new_name] = self.epocs[old_name]\n        self.epocs.pop(old_name)\n\n    def epoc_dataframe(self, include_epocs: FieldList = \"all\", include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n        \"\"\"Produce a dataframe with epoc data and metadata.\n\n        Args:\n            include_epocs: list of array names to include in the dataframe. Special str \"all\" is also accepted.\n            include_meta: list of metadata fields to include in the dataframe. Special str \"all\" is also accepted.\n\n        Returns:\n            DataFrame with data from this session\n        \"\"\"\n        # determine metadata fields to include\n        if include_meta == \"all\":\n            meta = self.metadata\n        else:\n            meta = {k: v for k, v in self.metadata.items() if k in include_meta}\n\n        # determine arrays to include\n        if include_epocs == \"all\":\n            epoc_names = list(self.epocs.keys())\n        else:\n            epoc_names = [k for k in self.epocs.keys() if k in include_epocs]\n\n        # iterate arrays and include any the user requested\n        # also add in any requested metadata\n        events = []\n        for k, v in self.epocs.items():\n            if k in epoc_names:\n                for value in v:\n                    events.append({**meta, \"event\": k, \"time\": value})\n\n        df = pd.DataFrame(events)\n\n        # sort the dataframe by time, but check that we have values, otherwise will raise keyerror\n        if len(df.index) &gt; 0:\n            df = df.sort_values(\"time\")\n\n        return df\n\n    def scalar_dataframe(self, include_scalars: FieldList = \"all\", include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n        \"\"\"Produce a dataframe with scalar data and metadata.\n\n        Args:\n            include_scalars: list of scalar names to include in the dataframe. Special str \"all\" is also accepted.\n            include_meta: list of metadata fields to include in the dataframe. Special str \"all\" is also accepted.\n\n        Returns:\n            DataFrame with data from this session\n        \"\"\"\n        # determine metadata fields to include\n        if include_meta == \"all\":\n            meta = self.metadata\n        else:\n            meta = {k: v for k, v in self.metadata.items() if k in include_meta}\n\n        # determine scalars to include\n        if include_scalars == \"all\":\n            scalar_names = list(self.scalars.keys())\n        else:\n            scalar_names = [k for k in self.scalars.keys() if k in include_scalars]\n\n        scalars = []\n        for sn in scalar_names:\n            scalars.append({**meta, \"scalar_name\": sn, \"scalar_value\": self.scalars[sn]})\n\n        return pd.DataFrame(scalars)\n\n    def __eq__(self, value: object) -&gt; bool:\n        \"\"\"Test this Session for equality to another Session.\n\n        Args:\n            value: value to test against for equality\n\n        Returns:\n            True if this Session is equal to value, False otherwise. Name, metadata, signals, epocs, scalars are considered for equality.\n        \"\"\"\n        # check we have a session instance to compare to\n        if not isinstance(value, Session):\n            return False\n\n        # check name for equality\n        if self.name != value.name:\n            return False\n\n        # check metadata for equality\n        if self.metadata.keys() != value.metadata.keys():\n            return False\n        for meta_key in self.metadata.keys():\n            val1 = self.metadata[meta_key]\n            val2 = value.metadata[meta_key]\n            if isinstance(val1, float) and isinstance(val2, float) and math.isnan(val1) and math.isnan(val2):\n                continue  # allow NaN values for the same key to be considered equal\n            elif val1 != val2:\n                return False\n\n        # check signals for equality\n        if self.signals.keys() != value.signals.keys():\n            return False\n        for k in self.signals.keys():\n            if self.signals[k] != value.signals[k]:\n                return False\n\n        # check epocs for equality\n        if self.epocs.keys() != value.epocs.keys():\n            return False\n        for k in self.epocs.keys():\n            if not np.array_equal(self.epocs[k], value.epocs[k]):\n                return False\n\n        # check scalars for equality\n        if self.scalars.keys() != value.scalars.keys():\n            return False\n        for k in self.scalars.keys():\n            if not np.array_equal(self.scalars[k], value.scalars[k]):\n                return False\n\n        return True\n\n    def save(self, path: str):\n        \"\"\"Save this Session to and HDF5 file.\n\n        Args:\n            path: path where the data should be saved\n        \"\"\"\n        with h5py.File(path, mode=\"w\") as h5:\n            # save name\n            h5.create_dataset(\"/name\", data=self.name)\n\n            # save signals\n            for k, sig in self.signals.items():\n                group = h5.create_group(f\"/signals/{k}\")\n                group.create_dataset(\"signal\", data=sig.signal, compression=\"gzip\")\n                group.create_dataset(\"time\", data=sig.time, compression=\"gzip\")\n                group.attrs[\"units\"] = sig.units\n                group.attrs[\"fs\"] = sig.fs\n                mark_group = group.create_group(\"marks\")\n                for mk, mv in sig.marks.items():\n                    mark_group.create_dataset(mk, data=mv)\n\n            # save epocs\n            for k, epoc in self.epocs.items():\n                h5.create_dataset(f\"/epocs/{k}\", data=epoc, compression=\"gzip\")\n\n            # save scalars\n            for k, scalar in self.scalars.items():\n                h5.create_dataset(f\"/scalars/{k}\", data=scalar, compression=\"gzip\")\n\n            # save metadata\n            meta_group = h5.create_group(\"/metadata\")\n            for k, v in self.metadata.items():\n                if isinstance(v, str):\n                    meta_group[k] = v\n                    meta_group[k].attrs[\"type\"] = \"str\"\n                elif isinstance(v, bool):\n                    meta_group[k] = v\n                    meta_group[k].attrs[\"type\"] = \"bool\"\n                elif isinstance(v, int):\n                    meta_group[k] = v\n                    meta_group[k].attrs[\"type\"] = \"int\"\n                elif isinstance(v, float):\n                    meta_group[k] = v\n                    meta_group[k].attrs[\"type\"] = \"float\"\n                elif isinstance(v, datetime.datetime):\n                    meta_group[k] = v.isoformat()\n                    meta_group[k].attrs[\"type\"] = \"datetime\"\n                elif isinstance(v, datetime.timedelta):\n                    meta_group[k] = v.total_seconds()\n                    meta_group[k].attrs[\"type\"] = \"timedelta\"\n                else:\n                    meta_group[k] = v\n\n    @classmethod\n    def load(cls, path: str) -&gt; \"Session\":\n        \"\"\"Load a Session from an HDF5 file.\n\n        Args:\n            path: path to the hdf5 file to load\n\n        Returns:\n            Session with data loaded\n        \"\"\"\n        session = cls()\n        with h5py.File(path, mode=\"r\") as h5:\n            # read name\n            session.name = h5[\"/name\"][()].decode(\"utf-8\")\n\n            # read signals\n            for signame in h5[\"/signals\"].keys():\n                sig_group = h5[f\"/signals/{signame}\"]\n                sig = Signal(\n                    signame, sig_group[\"signal\"][()], time=sig_group[\"time\"][()], fs=sig_group.attrs[\"fs\"], units=sig_group.attrs[\"units\"]\n                )\n                for mark_name in sig_group[\"marks\"].keys():\n                    sig.marks[mark_name] = sig_group[f\"marks/{mark_name}\"][()]\n                session.add_signal(sig)\n\n            # read epocs\n            for epoc_name in h5[\"/epocs\"].keys():\n                session.epocs[epoc_name] = h5[f\"/epocs/{epoc_name}\"][()]\n\n            # read scalars\n            for scalar_name in h5[\"/scalars\"].keys():\n                session.scalars[scalar_name] = h5[f\"/scalars/{scalar_name}\"][()]\n\n            # read metadata\n            for meta_name in h5[f\"/metadata\"].keys():\n                meta = h5[f\"/metadata/{meta_name}\"]\n                if \"type\" in meta.attrs:\n                    mtype = meta.attrs[\"type\"]\n                    if mtype == \"str\":\n                        session.metadata[meta_name] = meta[()].decode(\"utf-8\")\n                    elif mtype == \"bool\":\n                        session.metadata[meta_name] = bool(meta[()])\n                    elif mtype == \"int\":\n                        session.metadata[meta_name] = int(meta[()])\n                    elif mtype == \"float\":\n                        session.metadata[meta_name] = float(meta[()])\n                    elif mtype == \"datetime\":\n                        session.metadata[meta_name] = datetime.datetime.fromisoformat(meta[()].decode(\"utf-8\"))\n                    elif mtype == \"timedelta\":\n                        session.metadata[meta_name] = datetime.timedelta(seconds=meta[()])\n                    else:\n                        raise ValueError(f'Did not understand type {mtype} for metadata key \"{meta_name}\"')\n                else:\n                    session.metadata[meta_name] = meta[()]\n\n        return session\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.__eq__","title":"<code>__eq__(value)</code>","text":"<p>Test this Session for equality to another Session.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>object</code>)           \u2013            <p>value to test against for equality</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if this Session is equal to value, False otherwise. Name, metadata, signals, epocs, scalars are considered for equality.</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def __eq__(self, value: object) -&gt; bool:\n    \"\"\"Test this Session for equality to another Session.\n\n    Args:\n        value: value to test against for equality\n\n    Returns:\n        True if this Session is equal to value, False otherwise. Name, metadata, signals, epocs, scalars are considered for equality.\n    \"\"\"\n    # check we have a session instance to compare to\n    if not isinstance(value, Session):\n        return False\n\n    # check name for equality\n    if self.name != value.name:\n        return False\n\n    # check metadata for equality\n    if self.metadata.keys() != value.metadata.keys():\n        return False\n    for meta_key in self.metadata.keys():\n        val1 = self.metadata[meta_key]\n        val2 = value.metadata[meta_key]\n        if isinstance(val1, float) and isinstance(val2, float) and math.isnan(val1) and math.isnan(val2):\n            continue  # allow NaN values for the same key to be considered equal\n        elif val1 != val2:\n            return False\n\n    # check signals for equality\n    if self.signals.keys() != value.signals.keys():\n        return False\n    for k in self.signals.keys():\n        if self.signals[k] != value.signals[k]:\n            return False\n\n    # check epocs for equality\n    if self.epocs.keys() != value.epocs.keys():\n        return False\n    for k in self.epocs.keys():\n        if not np.array_equal(self.epocs[k], value.epocs[k]):\n            return False\n\n    # check scalars for equality\n    if self.scalars.keys() != value.scalars.keys():\n        return False\n    for k in self.scalars.keys():\n        if not np.array_equal(self.scalars[k], value.scalars[k]):\n            return False\n\n    return True\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.__init__","title":"<code>__init__()</code>","text":"<p>Initialize this Session object.</p> Source code in <code>fptools/io/session.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize this Session object.\"\"\"\n    self.name: str = \"\"\n    self.metadata: dict[str, Any] = {}\n    self.signals: dict[str, Signal] = {}\n    self.epocs: dict[str, np.ndarray] = defaultdict(partial(np.ndarray, 0))\n    self.scalars: dict[str, np.ndarray] = defaultdict()\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.add_signal","title":"<code>add_signal(signal, overwrite=False)</code>","text":"<p>Add a signal to this Session.</p> <p>Raises an error if the new signal name already exists and <code>overwrite</code> is not True.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>Signal</code>)           \u2013            <p>the signal to add to this Session</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if True, allow overwriting a pre-existing signal with the same name, if False, will raise instead.</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def add_signal(self, signal: Signal, overwrite: bool = False) -&gt; None:\n    \"\"\"Add a signal to this Session.\n\n    Raises an error if the new signal name already exists and `overwrite` is not True.\n\n    Args:\n        signal: the signal to add to this Session\n        overwrite: if True, allow overwriting a pre-existing signal with the same name, if False, will raise instead.\n    \"\"\"\n    if signal.name in self.signals and not overwrite:\n        raise KeyError(f\"Key `{signal.name}` already exists in data!\")\n\n    self.signals[signal.name] = signal\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.describe","title":"<code>describe(as_str=False)</code>","text":"<p>Describe this session.</p> <p>describes the metadata, scalars, and arrays contained in this session.</p> <p>Parameters:</p> <ul> <li> <code>as_str</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if True, return description as a string, otherwise print the description and return None</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, None]</code>           \u2013            <p><code>None</code> if <code>as_str</code> is <code>False</code>; if <code>as_str</code> is <code>True</code>, returns the description as a <code>str</code></p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def describe(self, as_str: bool = False) -&gt; Union[str, None]:\n    \"\"\"Describe this session.\n\n    describes the metadata, scalars, and arrays contained in this session.\n\n    Args:\n        as_str: if True, return description as a string, otherwise print the description and return None\n\n    Returns:\n        `None` if `as_str` is `False`; if `as_str` is `True`, returns the description as a `str`\n    \"\"\"\n    buffer = f'Session with name \"{self.name}\"\\n\\n'\n\n    buffer += \"Metadata:\\n\"\n    if len(self.metadata) &gt; 0:\n        for k, v in self.metadata.items():\n            buffer += f\"    {k}: {v}\\n\"\n    else:\n        buffer += \"    &lt; No Metadata Available &gt;\\n\"\n    buffer += \"\\n\"\n\n    buffer += \"Epocs:\\n\"\n    if len(self.epocs) &gt; 0:\n        for k, v in self.epocs.items():\n            # buffer += f'    \"{k}\" with shape {v.shape}:\\n    {np.array2string(v, prefix=\"    \")}\\n\\n'\n            buffer += f\"    {k}:\\n\"\n            buffer += f\"        num_events = {v.shape}\\n\"\n            buffer += f\"        avg_rate = {datetime.timedelta(seconds=np.diff(v)[0])}\\n\"\n            buffer += f\"        earliest = {datetime.timedelta(seconds=v[0])}\\n\"\n            buffer += f\"        latest = {datetime.timedelta(seconds=v[-1])}\\n\"\n    else:\n        buffer += \"    &lt; No Epocs Available &gt;\\n\"\n    buffer += \"\\n\"\n\n    buffer += \"Scalars:\\n\"\n    if len(self.scalars) &gt; 0:\n        for k, v in self.scalars.items():\n            buffer += f\"    {k}:\\n\"\n            buffer += f\"        num_events = {v.shape}\\n\"\n            buffer += f\"        earliest = {datetime.timedelta(seconds=v[0])}\\n\"\n            buffer += f\"        latest = {datetime.timedelta(seconds=v[-1])}\\n\"\n    else:\n        buffer += \"    &lt; No Epocs Available &gt;\\n\"\n    buffer += \"\\n\"\n\n    buffer += \"Signals:\\n\"\n    if len(self.signals) &gt; 0:\n        for k, v in self.signals.items():\n            buffer += v.describe(as_str=True, prefix=\"    \")\n    else:\n        buffer += \"    &lt; No Signals Available &gt;\\n\"\n    buffer += \"\\n\"\n\n    if as_str:\n        return buffer\n    else:\n        print(buffer)\n        return None\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.epoc_dataframe","title":"<code>epoc_dataframe(include_epocs='all', include_meta='all')</code>","text":"<p>Produce a dataframe with epoc data and metadata.</p> <p>Parameters:</p> <ul> <li> <code>include_epocs</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>list of array names to include in the dataframe. Special str \"all\" is also accepted.</p> </li> <li> <code>include_meta</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>list of metadata fields to include in the dataframe. Special str \"all\" is also accepted.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with data from this session</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def epoc_dataframe(self, include_epocs: FieldList = \"all\", include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n    \"\"\"Produce a dataframe with epoc data and metadata.\n\n    Args:\n        include_epocs: list of array names to include in the dataframe. Special str \"all\" is also accepted.\n        include_meta: list of metadata fields to include in the dataframe. Special str \"all\" is also accepted.\n\n    Returns:\n        DataFrame with data from this session\n    \"\"\"\n    # determine metadata fields to include\n    if include_meta == \"all\":\n        meta = self.metadata\n    else:\n        meta = {k: v for k, v in self.metadata.items() if k in include_meta}\n\n    # determine arrays to include\n    if include_epocs == \"all\":\n        epoc_names = list(self.epocs.keys())\n    else:\n        epoc_names = [k for k in self.epocs.keys() if k in include_epocs]\n\n    # iterate arrays and include any the user requested\n    # also add in any requested metadata\n    events = []\n    for k, v in self.epocs.items():\n        if k in epoc_names:\n            for value in v:\n                events.append({**meta, \"event\": k, \"time\": value})\n\n    df = pd.DataFrame(events)\n\n    # sort the dataframe by time, but check that we have values, otherwise will raise keyerror\n    if len(df.index) &gt; 0:\n        df = df.sort_values(\"time\")\n\n    return df\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load a Session from an HDF5 file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to the hdf5 file to load</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session with data loaded</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; \"Session\":\n    \"\"\"Load a Session from an HDF5 file.\n\n    Args:\n        path: path to the hdf5 file to load\n\n    Returns:\n        Session with data loaded\n    \"\"\"\n    session = cls()\n    with h5py.File(path, mode=\"r\") as h5:\n        # read name\n        session.name = h5[\"/name\"][()].decode(\"utf-8\")\n\n        # read signals\n        for signame in h5[\"/signals\"].keys():\n            sig_group = h5[f\"/signals/{signame}\"]\n            sig = Signal(\n                signame, sig_group[\"signal\"][()], time=sig_group[\"time\"][()], fs=sig_group.attrs[\"fs\"], units=sig_group.attrs[\"units\"]\n            )\n            for mark_name in sig_group[\"marks\"].keys():\n                sig.marks[mark_name] = sig_group[f\"marks/{mark_name}\"][()]\n            session.add_signal(sig)\n\n        # read epocs\n        for epoc_name in h5[\"/epocs\"].keys():\n            session.epocs[epoc_name] = h5[f\"/epocs/{epoc_name}\"][()]\n\n        # read scalars\n        for scalar_name in h5[\"/scalars\"].keys():\n            session.scalars[scalar_name] = h5[f\"/scalars/{scalar_name}\"][()]\n\n        # read metadata\n        for meta_name in h5[f\"/metadata\"].keys():\n            meta = h5[f\"/metadata/{meta_name}\"]\n            if \"type\" in meta.attrs:\n                mtype = meta.attrs[\"type\"]\n                if mtype == \"str\":\n                    session.metadata[meta_name] = meta[()].decode(\"utf-8\")\n                elif mtype == \"bool\":\n                    session.metadata[meta_name] = bool(meta[()])\n                elif mtype == \"int\":\n                    session.metadata[meta_name] = int(meta[()])\n                elif mtype == \"float\":\n                    session.metadata[meta_name] = float(meta[()])\n                elif mtype == \"datetime\":\n                    session.metadata[meta_name] = datetime.datetime.fromisoformat(meta[()].decode(\"utf-8\"))\n                elif mtype == \"timedelta\":\n                    session.metadata[meta_name] = datetime.timedelta(seconds=meta[()])\n                else:\n                    raise ValueError(f'Did not understand type {mtype} for metadata key \"{meta_name}\"')\n            else:\n                session.metadata[meta_name] = meta[()]\n\n    return session\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.rename_epoc","title":"<code>rename_epoc(old_name, new_name)</code>","text":"<p>Rename a epoc, from <code>old_name</code> to <code>new_name</code>.</p> <p>Raises an error if the new epoc name already exists.</p> <p>Parameters:</p> <ul> <li> <code>old_name</code>               (<code>str</code>)           \u2013            <p>the current name for the epoc</p> </li> <li> <code>new_name</code>               (<code>str</code>)           \u2013            <p>the new name for the epoc</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def rename_epoc(self, old_name: str, new_name: str) -&gt; None:\n    \"\"\"Rename a epoc, from `old_name` to `new_name`.\n\n    Raises an error if the new epoc name already exists.\n\n    Args:\n        old_name: the current name for the epoc\n        new_name: the new name for the epoc\n    \"\"\"\n    if new_name in self.epocs:\n        raise KeyError(f\"Key `{new_name}` already exists in data!\")\n\n    self.epocs[new_name] = self.epocs[old_name]\n    self.epocs.pop(old_name)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.rename_scalar","title":"<code>rename_scalar(old_name, new_name)</code>","text":"<p>Rename a scalar, from <code>old_name</code> to <code>new_name</code>.</p> <p>Raises an error if the new scalar name already exists.</p> <p>Args: old_name: the current name for the scalar new_name: the new name for the scalar</p> Source code in <code>fptools/io/session.py</code> <pre><code>def rename_scalar(self, old_name: str, new_name: str):\n    \"\"\"Rename a scalar, from `old_name` to `new_name`.\n\n    Raises an error if the new scalar name already exists.\n\n    Args:\n    old_name: the current name for the scalar\n    new_name: the new name for the scalar\n    \"\"\"\n    if new_name in self.scalars:\n        raise KeyError(f\"Key `{new_name}` already exists in data!\")\n\n    self.scalars[new_name] = self.scalars[old_name]\n    self.scalars.pop(old_name)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.rename_signal","title":"<code>rename_signal(old_name, new_name)</code>","text":"<p>Rename a signal, from <code>old_name</code> to <code>new_name</code>.</p> <p>Raises an error if the new signal name already exists.</p> <p>Parameters:</p> <ul> <li> <code>old_name</code>               (<code>str</code>)           \u2013            <p>the current name for the signal</p> </li> <li> <code>new_name</code>               (<code>str</code>)           \u2013            <p>the new name for the signal</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def rename_signal(self, old_name: str, new_name: str) -&gt; None:\n    \"\"\"Rename a signal, from `old_name` to `new_name`.\n\n    Raises an error if the new signal name already exists.\n\n    Args:\n        old_name: the current name for the signal\n        new_name: the new name for the signal\n    \"\"\"\n    if new_name in self.signals:\n        raise KeyError(f\"Key `{new_name}` already exists in data!\")\n\n    self.signals[new_name] = self.signals[old_name]\n    self.signals.pop(old_name)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.save","title":"<code>save(path)</code>","text":"<p>Save this Session to and HDF5 file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path where the data should be saved</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def save(self, path: str):\n    \"\"\"Save this Session to and HDF5 file.\n\n    Args:\n        path: path where the data should be saved\n    \"\"\"\n    with h5py.File(path, mode=\"w\") as h5:\n        # save name\n        h5.create_dataset(\"/name\", data=self.name)\n\n        # save signals\n        for k, sig in self.signals.items():\n            group = h5.create_group(f\"/signals/{k}\")\n            group.create_dataset(\"signal\", data=sig.signal, compression=\"gzip\")\n            group.create_dataset(\"time\", data=sig.time, compression=\"gzip\")\n            group.attrs[\"units\"] = sig.units\n            group.attrs[\"fs\"] = sig.fs\n            mark_group = group.create_group(\"marks\")\n            for mk, mv in sig.marks.items():\n                mark_group.create_dataset(mk, data=mv)\n\n        # save epocs\n        for k, epoc in self.epocs.items():\n            h5.create_dataset(f\"/epocs/{k}\", data=epoc, compression=\"gzip\")\n\n        # save scalars\n        for k, scalar in self.scalars.items():\n            h5.create_dataset(f\"/scalars/{k}\", data=scalar, compression=\"gzip\")\n\n        # save metadata\n        meta_group = h5.create_group(\"/metadata\")\n        for k, v in self.metadata.items():\n            if isinstance(v, str):\n                meta_group[k] = v\n                meta_group[k].attrs[\"type\"] = \"str\"\n            elif isinstance(v, bool):\n                meta_group[k] = v\n                meta_group[k].attrs[\"type\"] = \"bool\"\n            elif isinstance(v, int):\n                meta_group[k] = v\n                meta_group[k].attrs[\"type\"] = \"int\"\n            elif isinstance(v, float):\n                meta_group[k] = v\n                meta_group[k].attrs[\"type\"] = \"float\"\n            elif isinstance(v, datetime.datetime):\n                meta_group[k] = v.isoformat()\n                meta_group[k].attrs[\"type\"] = \"datetime\"\n            elif isinstance(v, datetime.timedelta):\n                meta_group[k] = v.total_seconds()\n                meta_group[k].attrs[\"type\"] = \"timedelta\"\n            else:\n                meta_group[k] = v\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.Session.scalar_dataframe","title":"<code>scalar_dataframe(include_scalars='all', include_meta='all')</code>","text":"<p>Produce a dataframe with scalar data and metadata.</p> <p>Parameters:</p> <ul> <li> <code>include_scalars</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>list of scalar names to include in the dataframe. Special str \"all\" is also accepted.</p> </li> <li> <code>include_meta</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>list of metadata fields to include in the dataframe. Special str \"all\" is also accepted.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with data from this session</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def scalar_dataframe(self, include_scalars: FieldList = \"all\", include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n    \"\"\"Produce a dataframe with scalar data and metadata.\n\n    Args:\n        include_scalars: list of scalar names to include in the dataframe. Special str \"all\" is also accepted.\n        include_meta: list of metadata fields to include in the dataframe. Special str \"all\" is also accepted.\n\n    Returns:\n        DataFrame with data from this session\n    \"\"\"\n    # determine metadata fields to include\n    if include_meta == \"all\":\n        meta = self.metadata\n    else:\n        meta = {k: v for k, v in self.metadata.items() if k in include_meta}\n\n    # determine scalars to include\n    if include_scalars == \"all\":\n        scalar_names = list(self.scalars.keys())\n    else:\n        scalar_names = [k for k in self.scalars.keys() if k in include_scalars]\n\n    scalars = []\n    for sn in scalar_names:\n        scalars.append({**meta, \"scalar_name\": sn, \"scalar_value\": self.scalars[sn]})\n\n    return pd.DataFrame(scalars)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection","title":"<code>SessionCollection</code>","text":"<p>               Bases: <code>list[Session]</code></p> <p>Collection of session data.</p> <p>Methods:</p> <ul> <li> <code>__init__</code>             \u2013              <p>Initialize this <code>SessionCollection</code>.</p> </li> <li> <code>add_metadata</code>             \u2013              <p>Set a metadata field on each session in this collection.</p> </li> <li> <code>aggregate_signals</code>             \u2013              <p>Aggregate signals across sessions in this collection for the signal name <code>name</code>.</p> </li> <li> <code>apply</code>             \u2013              <p>Apply a function to each session in this collection.</p> </li> <li> <code>describe</code>             \u2013              <p>Describe this collection of sessions.</p> </li> <li> <code>epoc_dataframe</code>             \u2013              <p>Produce a dataframe with epoc data and metadata across all the sessions in this collection.</p> </li> <li> <code>filter</code>             \u2013              <p>Filter the items in this collection, returning a new <code>SessionCollection</code> containing sessions which pass <code>predicate</code>.</p> </li> <li> <code>get_signal</code>             \u2013              <p>Get data across sessions in this collection for the signal named <code>name</code>.</p> </li> <li> <code>map</code>             \u2013              <p>Apply a function to each session in this collection, returning a new collection with the results.</p> </li> <li> <code>merge</code>             \u2013              <p>Merge session collections while preserving data.</p> </li> <li> <code>rename_epoc</code>             \u2013              <p>Rename an epoc on each session in this collection.</p> </li> <li> <code>rename_scalar</code>             \u2013              <p>Rename an scalar on each session in this collection.</p> </li> <li> <code>rename_signal</code>             \u2013              <p>Rename a signal on each session in this collection.</p> </li> <li> <code>save</code>             \u2013              <p>Save each Session in this SessionCollection to an hdf5 file.</p> </li> <li> <code>scalar_dataframe</code>             \u2013              <p>Produce a dataframe with scalar data and metadata across all the sessions in this collection.</p> </li> <li> <code>select</code>             \u2013              <p>Select sessions in this collection, returning a new <code>SessionCollection</code> containing sessions which all bool masks are true.</p> </li> <li> <code>set_metadata_props</code>             \u2013              <p>Set properties of a metadata column.</p> </li> <li> <code>signal_dataframe</code>             \u2013              <p>Get data for a given signal across sessions, also injecting metadata.</p> </li> <li> <code>update_metadata</code>             \u2013              <p>Set multiple metadata fields on each session in this collection.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>metadata</code>               (<code>DataFrame</code>)           \u2013            <p>Get a dataframe containing metadata across all sessions in this collection.</p> </li> <li> <code>metadata_keys</code>               (<code>list[str]</code>)           \u2013            <p>Get a list of the keys present in metadata across all sessions in this collection.</p> </li> <li> <code>signal_keys</code>               (<code>list[str]</code>)           \u2013            <p>Get a list of Signal keys in this SessionCollection.</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>class SessionCollection(list[Session]):\n    \"\"\"Collection of session data.\"\"\"\n\n    def __init__(self, *args) -&gt; None:\n        \"\"\"Initialize this `SessionCollection`.\"\"\"\n        super().__init__(*args)\n        self.__meta_meta: dict[str, dict[Literal[\"order\"], Any]] = {}\n\n    @property\n    def metadata(self) -&gt; pd.DataFrame:\n        \"\"\"Get a dataframe containing metadata across all sessions in this collection.\"\"\"\n        df = pd.DataFrame([item.metadata for item in self])\n\n        for k, v in self.__meta_meta.items():\n            if \"order\" in v:\n                df[k] = pd.Categorical(df[k], categories=v[\"order\"], ordered=True)\n\n        return df\n\n    @property\n    def metadata_keys(self) -&gt; list[str]:\n        \"\"\"Get a list of the keys present in metadata across all sessions in this collection.\"\"\"\n        return list(set([key for item in self for key in item.metadata.keys()]))\n\n    def add_metadata(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set a metadata field on each session in this collection.\n\n        Args:\n            key: name of the metadata field\n            value: value for the metadata field\n        \"\"\"\n        for item in self:\n            item.metadata[key] = value\n\n    def update_metadata(self, meta: dict[str, Any]) -&gt; None:\n        \"\"\"Set multiple metadata fields on each session in this collection.\n\n        Args:\n            meta: metadata information to set on each session\n        \"\"\"\n        for item in self:\n            item.metadata.update(meta)\n\n    def set_metadata_props(self, key: str, order: Optional[list[Any]] = None):\n        \"\"\"Set properties of a metadata column.\n\n        Args:\n            key: name of the metadata item, always required\n            order: optional, if specified will set the metadata column to be ordered categorical, according to `order`\n        \"\"\"\n        assert key in self.metadata_keys\n\n        if key not in self.__meta_meta:\n            self.__meta_meta[key] = {}\n\n        if order is not None:\n            self.__meta_meta[key][\"order\"] = order\n\n    def rename_signal(self, old_name: str, new_name: str) -&gt; None:\n        \"\"\"Rename a signal on each session in this collection.\n\n        Args:\n            old_name: current name of the signal\n            new_name: the new name for the signal\n        \"\"\"\n        for item in self:\n            item.rename_signal(old_name, new_name)\n\n    def rename_epoc(self, old_name: str, new_name: str) -&gt; None:\n        \"\"\"Rename an epoc on each session in this collection.\n\n        Args:\n            old_name: current name of the epoc\n            new_name: the new name for the epoc\n        \"\"\"\n        for item in self:\n            item.rename_epoc(old_name, new_name)\n\n    def rename_scalar(self, old_name: str, new_name: str) -&gt; None:\n        \"\"\"Rename an scalar on each session in this collection.\n\n        Args:\n            old_name: current name of the scalar\n            new_name: the new name for the scalar\n        \"\"\"\n        for item in self:\n            item.rename_scalar(old_name, new_name)\n\n    def filter(self, predicate: Callable[[Session], bool]) -&gt; \"SessionCollection\":\n        \"\"\"Filter the items in this collection, returning a new `SessionCollection` containing sessions which pass `predicate`.\n\n        Args:\n            predicate: a callable accepting a single session and returning bool.\n\n        Returns:\n            a new `SessionCollection` containing only items which pass `predicate`.\n        \"\"\"\n        sc = type(self)(item for item in self if predicate(item))\n        sc.__meta_meta.update(**copy.deepcopy(self.__meta_meta))\n        return sc\n\n    def select(self, *bool_masks: np.ndarray) -&gt; \"SessionCollection\":\n        \"\"\"Select sessions in this collection, returning a new `SessionCollection` containing sessions which all bool masks are true.\n\n        Args:\n            bool_masks: one or more boolean arrays, the reduced logical_and indicating which sessions to select\n\n        Returns:\n            a new `SessionCollection` containing only items which pass bool_masks.\n        \"\"\"\n        sc = type(self)(item for item, include in zip(self, np.logical_and.reduce(bool_masks)) if include)\n        sc.__meta_meta.update(**copy.deepcopy(self.__meta_meta))\n        return sc\n\n    def map(self, action: Callable[[Session], Session]) -&gt; \"SessionCollection\":\n        \"\"\"Apply a function to each session in this collection, returning a new collection with the results.\n\n        Args:\n            action: callable accepting a single session and returning a new session\n\n        Returns:\n            a new `SessionCollection` containing the results of `action`\n        \"\"\"\n        sc = type(self)(action(item) for item in self)\n        sc.__meta_meta.update(**copy.deepcopy(self.__meta_meta))\n        return sc\n\n    def apply(self, func: Callable[[Session], None]) -&gt; None:\n        \"\"\"Apply a function to each session in this collection.\n\n        Args:\n            func: callable accepting a single session and returning None\n        \"\"\"\n        for item in self:\n            func(item)\n\n    WHAT_LIST = Literal[\"all\", \"signal\", \"epocs\", \"metadata\"]\n\n    @staticmethod\n    def merge(\n        *session_collections: \"SessionCollection\", primary_key: str, what: Union[WHAT_LIST, list[WHAT_LIST]], prefixes: list[str]\n    ) -&gt; \"SessionCollection\":\n        \"\"\"Merge session collections while preserving data.\n\n        Args:\n            session_collections: SessionCollections to merge\n            primary_key: metadata key used to join sessions\n            what: the data within each session to merge\n            prefixes: list of prefixes, of the same length as the number of passed SessionCollections. each prefix will be prepended to signals to avoid overwriting\n        \"\"\"\n        available_whats = [\"signal\", \"epocs\", \"metadata\"]\n        use_what: list[str] = []\n        if isinstance(what, str):\n            if what == \"all\":\n                use_what.extend(available_whats)\n            else:\n                use_what.append(what)\n        else:\n            use_what.extend(what)\n\n        sorter: dict[str, list[Session]] = defaultdict(list[Session])\n        for collection in session_collections:\n            for session in collection:\n                sorter[session.metadata[primary_key]].append(session)\n\n        final = SessionCollection()\n        for k, v in sorter.items():\n            new_session = Session()\n            for i, old_session in enumerate(v):\n                if \"signal\" in use_what:\n                    for _, sig in old_session.signals.items():\n                        new_session.add_signal(sig.copy(f\"{prefixes[i]}{sig.name}\"))\n\n                if \"epocs\" in use_what:\n                    for name, epocs in old_session.epocs.items():\n                        new_session.epocs[name] = epocs\n\n                if \"metadata\" in use_what:\n                    new_session.metadata.update(old_session.metadata)\n            final.append(new_session)\n        return final\n\n    @property\n    def signal_keys(self) -&gt; list[str]:\n        \"\"\"Get a list of Signal keys in this SessionCollection.\"\"\"\n        return list(set([key for item in self for key in item.signals.keys()]))\n\n    def get_signal(self, name: str) -&gt; list[Signal]:\n        \"\"\"Get data across sessions in this collection for the signal named `name`.\n\n        Args:\n            name: Name of the signals to collect\n\n        Returns:\n            List of Signals, each corresponding to a single session\n        \"\"\"\n        return [item.signals[name] for item in self]\n\n    def epoc_dataframe(self, include_epocs: FieldList = \"all\", include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n        \"\"\"Produce a dataframe with epoc data and metadata across all the sessions in this collection.\n\n        Args:\n            include_epocs: list of epoc names to include in the dataframe. Special str \"all\" is also accepted.\n            include_meta: list of metadata fields to include in the dataframe. Special str \"all\" is also accepted.\n\n        Returns:\n            DataFrame with data from across this collection\n        \"\"\"\n        dfs = [session.epoc_dataframe(include_epocs=include_epocs, include_meta=include_meta) for session in self]\n        return pd.concat(dfs).sort_values(\"time\").reset_index(drop=True)\n\n    def scalar_dataframe(self, include_scalars: FieldList = \"all\", include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n        \"\"\"Produce a dataframe with scalar data and metadata across all the sessions in this collection.\n\n        Args:\n            include_scalars: list of scalar names to include in the dataframe. Special str \"all\" is also accepted.\n            include_meta: list of metadata fields to include in the dataframe. Special str \"all\" is also accepted.\n\n        Returns:\n            DataFrame with data from across this collection\n        \"\"\"\n        dfs = [session.scalar_dataframe(include_scalars=include_scalars, include_meta=include_meta) for session in self]\n        return pd.concat(dfs).reset_index(drop=True)\n\n    def signal_dataframe(self, signal: str, include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n        \"\"\"Get data for a given signal across sessions, also injecting metadata.\n\n        See also: `Signal.to_dataframe()`\n\n        Args:\n            signal: Name of the signal to collect\n            include_meta: include_meta: metadata fields to include in the final output. Special string \"all\" will include all metadata fields\n\n        Returns:\n            signal across sessions as a `pandas.DataFrame`\n        \"\"\"\n        dfs = []\n        for session in self:\n            if include_meta == \"all\":\n                meta = session.metadata\n            else:\n                meta = {k: v for k, v in session.metadata.items() if k in include_meta}\n\n            sig = session.signals[signal]\n            df = sig.to_dataframe()\n\n            df[\"obs\"] = list(range(sig.nobs))\n\n            for k, v in meta.items():\n                df[k] = v\n\n            # reorder columns\n            df = df[\n                [c for c in df.columns.values if not str(c).startswith(\"Y.\")] + [c for c in df.columns.values if str(c).startswith(\"Y.\")]\n            ]\n\n            dfs.append(df)\n\n        return pd.concat(dfs, ignore_index=True)\n\n    def aggregate_signals(self, name: str, method: Union[str, np.ufunc, Callable[[np.ndarray], np.ndarray]] = \"median\") -&gt; Signal:\n        \"\"\"Aggregate signals across sessions in this collection for the signal name `name`.\n\n        Args:\n            name: name of the signal to aggregate\n            method: the method used for aggregation\n\n        Returns:\n            Aggregated `Signal`\n        \"\"\"\n        signals = self.get_signal(name)\n        if len(signals) &lt;= 0:\n            raise ValueError(\"No signals were passed!\")\n\n        # check all signals have the same number of samples\n        assert np.all(np.equal([s.nsamples for s in signals], signals[0].nsamples))\n\n        if method is not None:\n            signals = [s.aggregate(method) for s in signals]\n\n        s = Signal(signals[0].name, np.vstack([s.signal for s in signals]), time=signals[0].time, units=signals[0].units)\n        s.marks.update(signals[0].marks)\n        return s\n\n    def describe(self, as_str: bool = False) -&gt; Union[str, None]:\n        \"\"\"Describe this collection of sessions.\n\n        Args:\n            as_str: if True, return description as a string, otherwise print the description and return None\n\n        Returns:\n            `None` if `as_str` is `False`; if `as_str` is `True`, returns the description as a `str`\n        \"\"\"\n        buffer = \"\"\n\n        buffer += f\"Number of sessions: {len(self)}\\n\\n\"\n\n        signals = Counter([item for session in self for item in session.signals.keys()])\n        buffer += \"Signals present in data with counts:\\n\"\n        for k, v in signals.items():\n            buffer += f'({v}) \"{k}\"\\n'\n        buffer += \"\\n\"\n\n        epocs = Counter([item for session in self for item in session.epocs.keys()])\n        buffer += \"Epocs present in data with counts:\\n\"\n        for k, v in epocs.items():\n            buffer += f'({v}) \"{k}\"\\n'\n        buffer += \"\\n\"\n\n        scalars = Counter([item for session in self for item in session.scalars.keys()])\n        buffer += \"Scalars present in data with counts:\\n\"\n        for k, v in scalars.items():\n            buffer += f'({v}) \"{k}\"\\n'\n        buffer += \"\\n\"\n\n        if as_str:\n            return buffer\n        else:\n            print(buffer)\n            return None\n\n    def save(self, path: str):\n        \"\"\"Save each Session in this SessionCollection to an hdf5 file.\n\n        Each Session will be named as the `Session.name` with an \".h5\" file extension\n\n        Args:\n            path: path to a directory\n        \"\"\"\n        # ensure the directory exists\n        os.makedirs(path, exist_ok=True)\n\n        for session in self:\n            session.save(os.path.join(path, f\"{session.name}.h5\"))\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>Get a dataframe containing metadata across all sessions in this collection.</p>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.metadata_keys","title":"<code>metadata_keys</code>  <code>property</code>","text":"<p>Get a list of the keys present in metadata across all sessions in this collection.</p>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.signal_keys","title":"<code>signal_keys</code>  <code>property</code>","text":"<p>Get a list of Signal keys in this SessionCollection.</p>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.__init__","title":"<code>__init__(*args)</code>","text":"<p>Initialize this <code>SessionCollection</code>.</p> Source code in <code>fptools/io/session.py</code> <pre><code>def __init__(self, *args) -&gt; None:\n    \"\"\"Initialize this `SessionCollection`.\"\"\"\n    super().__init__(*args)\n    self.__meta_meta: dict[str, dict[Literal[\"order\"], Any]] = {}\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.add_metadata","title":"<code>add_metadata(key, value)</code>","text":"<p>Set a metadata field on each session in this collection.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>name of the metadata field</p> </li> <li> <code>value</code>               (<code>Any</code>)           \u2013            <p>value for the metadata field</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def add_metadata(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set a metadata field on each session in this collection.\n\n    Args:\n        key: name of the metadata field\n        value: value for the metadata field\n    \"\"\"\n    for item in self:\n        item.metadata[key] = value\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.aggregate_signals","title":"<code>aggregate_signals(name, method='median')</code>","text":"<p>Aggregate signals across sessions in this collection for the signal name <code>name</code>.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>name of the signal to aggregate</p> </li> <li> <code>method</code>               (<code>Union[str, ufunc, Callable[[ndarray], ndarray]]</code>, default:                   <code>'median'</code> )           \u2013            <p>the method used for aggregation</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Signal</code>           \u2013            <p>Aggregated <code>Signal</code></p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def aggregate_signals(self, name: str, method: Union[str, np.ufunc, Callable[[np.ndarray], np.ndarray]] = \"median\") -&gt; Signal:\n    \"\"\"Aggregate signals across sessions in this collection for the signal name `name`.\n\n    Args:\n        name: name of the signal to aggregate\n        method: the method used for aggregation\n\n    Returns:\n        Aggregated `Signal`\n    \"\"\"\n    signals = self.get_signal(name)\n    if len(signals) &lt;= 0:\n        raise ValueError(\"No signals were passed!\")\n\n    # check all signals have the same number of samples\n    assert np.all(np.equal([s.nsamples for s in signals], signals[0].nsamples))\n\n    if method is not None:\n        signals = [s.aggregate(method) for s in signals]\n\n    s = Signal(signals[0].name, np.vstack([s.signal for s in signals]), time=signals[0].time, units=signals[0].units)\n    s.marks.update(signals[0].marks)\n    return s\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.apply","title":"<code>apply(func)</code>","text":"<p>Apply a function to each session in this collection.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Callable[[Session], None]</code>)           \u2013            <p>callable accepting a single session and returning None</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def apply(self, func: Callable[[Session], None]) -&gt; None:\n    \"\"\"Apply a function to each session in this collection.\n\n    Args:\n        func: callable accepting a single session and returning None\n    \"\"\"\n    for item in self:\n        func(item)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.describe","title":"<code>describe(as_str=False)</code>","text":"<p>Describe this collection of sessions.</p> <p>Parameters:</p> <ul> <li> <code>as_str</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if True, return description as a string, otherwise print the description and return None</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, None]</code>           \u2013            <p><code>None</code> if <code>as_str</code> is <code>False</code>; if <code>as_str</code> is <code>True</code>, returns the description as a <code>str</code></p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def describe(self, as_str: bool = False) -&gt; Union[str, None]:\n    \"\"\"Describe this collection of sessions.\n\n    Args:\n        as_str: if True, return description as a string, otherwise print the description and return None\n\n    Returns:\n        `None` if `as_str` is `False`; if `as_str` is `True`, returns the description as a `str`\n    \"\"\"\n    buffer = \"\"\n\n    buffer += f\"Number of sessions: {len(self)}\\n\\n\"\n\n    signals = Counter([item for session in self for item in session.signals.keys()])\n    buffer += \"Signals present in data with counts:\\n\"\n    for k, v in signals.items():\n        buffer += f'({v}) \"{k}\"\\n'\n    buffer += \"\\n\"\n\n    epocs = Counter([item for session in self for item in session.epocs.keys()])\n    buffer += \"Epocs present in data with counts:\\n\"\n    for k, v in epocs.items():\n        buffer += f'({v}) \"{k}\"\\n'\n    buffer += \"\\n\"\n\n    scalars = Counter([item for session in self for item in session.scalars.keys()])\n    buffer += \"Scalars present in data with counts:\\n\"\n    for k, v in scalars.items():\n        buffer += f'({v}) \"{k}\"\\n'\n    buffer += \"\\n\"\n\n    if as_str:\n        return buffer\n    else:\n        print(buffer)\n        return None\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.epoc_dataframe","title":"<code>epoc_dataframe(include_epocs='all', include_meta='all')</code>","text":"<p>Produce a dataframe with epoc data and metadata across all the sessions in this collection.</p> <p>Parameters:</p> <ul> <li> <code>include_epocs</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>list of epoc names to include in the dataframe. Special str \"all\" is also accepted.</p> </li> <li> <code>include_meta</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>list of metadata fields to include in the dataframe. Special str \"all\" is also accepted.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with data from across this collection</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def epoc_dataframe(self, include_epocs: FieldList = \"all\", include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n    \"\"\"Produce a dataframe with epoc data and metadata across all the sessions in this collection.\n\n    Args:\n        include_epocs: list of epoc names to include in the dataframe. Special str \"all\" is also accepted.\n        include_meta: list of metadata fields to include in the dataframe. Special str \"all\" is also accepted.\n\n    Returns:\n        DataFrame with data from across this collection\n    \"\"\"\n    dfs = [session.epoc_dataframe(include_epocs=include_epocs, include_meta=include_meta) for session in self]\n    return pd.concat(dfs).sort_values(\"time\").reset_index(drop=True)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.filter","title":"<code>filter(predicate)</code>","text":"<p>Filter the items in this collection, returning a new <code>SessionCollection</code> containing sessions which pass <code>predicate</code>.</p> <p>Parameters:</p> <ul> <li> <code>predicate</code>               (<code>Callable[[Session], bool]</code>)           \u2013            <p>a callable accepting a single session and returning bool.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SessionCollection</code>           \u2013            <p>a new <code>SessionCollection</code> containing only items which pass <code>predicate</code>.</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def filter(self, predicate: Callable[[Session], bool]) -&gt; \"SessionCollection\":\n    \"\"\"Filter the items in this collection, returning a new `SessionCollection` containing sessions which pass `predicate`.\n\n    Args:\n        predicate: a callable accepting a single session and returning bool.\n\n    Returns:\n        a new `SessionCollection` containing only items which pass `predicate`.\n    \"\"\"\n    sc = type(self)(item for item in self if predicate(item))\n    sc.__meta_meta.update(**copy.deepcopy(self.__meta_meta))\n    return sc\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.get_signal","title":"<code>get_signal(name)</code>","text":"<p>Get data across sessions in this collection for the signal named <code>name</code>.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of the signals to collect</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Signal]</code>           \u2013            <p>List of Signals, each corresponding to a single session</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def get_signal(self, name: str) -&gt; list[Signal]:\n    \"\"\"Get data across sessions in this collection for the signal named `name`.\n\n    Args:\n        name: Name of the signals to collect\n\n    Returns:\n        List of Signals, each corresponding to a single session\n    \"\"\"\n    return [item.signals[name] for item in self]\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.map","title":"<code>map(action)</code>","text":"<p>Apply a function to each session in this collection, returning a new collection with the results.</p> <p>Parameters:</p> <ul> <li> <code>action</code>               (<code>Callable[[Session], Session]</code>)           \u2013            <p>callable accepting a single session and returning a new session</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SessionCollection</code>           \u2013            <p>a new <code>SessionCollection</code> containing the results of <code>action</code></p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def map(self, action: Callable[[Session], Session]) -&gt; \"SessionCollection\":\n    \"\"\"Apply a function to each session in this collection, returning a new collection with the results.\n\n    Args:\n        action: callable accepting a single session and returning a new session\n\n    Returns:\n        a new `SessionCollection` containing the results of `action`\n    \"\"\"\n    sc = type(self)(action(item) for item in self)\n    sc.__meta_meta.update(**copy.deepcopy(self.__meta_meta))\n    return sc\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.merge","title":"<code>merge(*session_collections, primary_key, what, prefixes)</code>  <code>staticmethod</code>","text":"<p>Merge session collections while preserving data.</p> <p>Parameters:</p> <ul> <li> <code>session_collections</code>               (<code>SessionCollection</code>, default:                   <code>()</code> )           \u2013            <p>SessionCollections to merge</p> </li> <li> <code>primary_key</code>               (<code>str</code>)           \u2013            <p>metadata key used to join sessions</p> </li> <li> <code>what</code>               (<code>Union[WHAT_LIST, list[WHAT_LIST]]</code>)           \u2013            <p>the data within each session to merge</p> </li> <li> <code>prefixes</code>               (<code>list[str]</code>)           \u2013            <p>list of prefixes, of the same length as the number of passed SessionCollections. each prefix will be prepended to signals to avoid overwriting</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>@staticmethod\ndef merge(\n    *session_collections: \"SessionCollection\", primary_key: str, what: Union[WHAT_LIST, list[WHAT_LIST]], prefixes: list[str]\n) -&gt; \"SessionCollection\":\n    \"\"\"Merge session collections while preserving data.\n\n    Args:\n        session_collections: SessionCollections to merge\n        primary_key: metadata key used to join sessions\n        what: the data within each session to merge\n        prefixes: list of prefixes, of the same length as the number of passed SessionCollections. each prefix will be prepended to signals to avoid overwriting\n    \"\"\"\n    available_whats = [\"signal\", \"epocs\", \"metadata\"]\n    use_what: list[str] = []\n    if isinstance(what, str):\n        if what == \"all\":\n            use_what.extend(available_whats)\n        else:\n            use_what.append(what)\n    else:\n        use_what.extend(what)\n\n    sorter: dict[str, list[Session]] = defaultdict(list[Session])\n    for collection in session_collections:\n        for session in collection:\n            sorter[session.metadata[primary_key]].append(session)\n\n    final = SessionCollection()\n    for k, v in sorter.items():\n        new_session = Session()\n        for i, old_session in enumerate(v):\n            if \"signal\" in use_what:\n                for _, sig in old_session.signals.items():\n                    new_session.add_signal(sig.copy(f\"{prefixes[i]}{sig.name}\"))\n\n            if \"epocs\" in use_what:\n                for name, epocs in old_session.epocs.items():\n                    new_session.epocs[name] = epocs\n\n            if \"metadata\" in use_what:\n                new_session.metadata.update(old_session.metadata)\n        final.append(new_session)\n    return final\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.rename_epoc","title":"<code>rename_epoc(old_name, new_name)</code>","text":"<p>Rename an epoc on each session in this collection.</p> <p>Parameters:</p> <ul> <li> <code>old_name</code>               (<code>str</code>)           \u2013            <p>current name of the epoc</p> </li> <li> <code>new_name</code>               (<code>str</code>)           \u2013            <p>the new name for the epoc</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def rename_epoc(self, old_name: str, new_name: str) -&gt; None:\n    \"\"\"Rename an epoc on each session in this collection.\n\n    Args:\n        old_name: current name of the epoc\n        new_name: the new name for the epoc\n    \"\"\"\n    for item in self:\n        item.rename_epoc(old_name, new_name)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.rename_scalar","title":"<code>rename_scalar(old_name, new_name)</code>","text":"<p>Rename an scalar on each session in this collection.</p> <p>Parameters:</p> <ul> <li> <code>old_name</code>               (<code>str</code>)           \u2013            <p>current name of the scalar</p> </li> <li> <code>new_name</code>               (<code>str</code>)           \u2013            <p>the new name for the scalar</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def rename_scalar(self, old_name: str, new_name: str) -&gt; None:\n    \"\"\"Rename an scalar on each session in this collection.\n\n    Args:\n        old_name: current name of the scalar\n        new_name: the new name for the scalar\n    \"\"\"\n    for item in self:\n        item.rename_scalar(old_name, new_name)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.rename_signal","title":"<code>rename_signal(old_name, new_name)</code>","text":"<p>Rename a signal on each session in this collection.</p> <p>Parameters:</p> <ul> <li> <code>old_name</code>               (<code>str</code>)           \u2013            <p>current name of the signal</p> </li> <li> <code>new_name</code>               (<code>str</code>)           \u2013            <p>the new name for the signal</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def rename_signal(self, old_name: str, new_name: str) -&gt; None:\n    \"\"\"Rename a signal on each session in this collection.\n\n    Args:\n        old_name: current name of the signal\n        new_name: the new name for the signal\n    \"\"\"\n    for item in self:\n        item.rename_signal(old_name, new_name)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.save","title":"<code>save(path)</code>","text":"<p>Save each Session in this SessionCollection to an hdf5 file.</p> <p>Each Session will be named as the <code>Session.name</code> with an \".h5\" file extension</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to a directory</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def save(self, path: str):\n    \"\"\"Save each Session in this SessionCollection to an hdf5 file.\n\n    Each Session will be named as the `Session.name` with an \".h5\" file extension\n\n    Args:\n        path: path to a directory\n    \"\"\"\n    # ensure the directory exists\n    os.makedirs(path, exist_ok=True)\n\n    for session in self:\n        session.save(os.path.join(path, f\"{session.name}.h5\"))\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.scalar_dataframe","title":"<code>scalar_dataframe(include_scalars='all', include_meta='all')</code>","text":"<p>Produce a dataframe with scalar data and metadata across all the sessions in this collection.</p> <p>Parameters:</p> <ul> <li> <code>include_scalars</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>list of scalar names to include in the dataframe. Special str \"all\" is also accepted.</p> </li> <li> <code>include_meta</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>list of metadata fields to include in the dataframe. Special str \"all\" is also accepted.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with data from across this collection</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def scalar_dataframe(self, include_scalars: FieldList = \"all\", include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n    \"\"\"Produce a dataframe with scalar data and metadata across all the sessions in this collection.\n\n    Args:\n        include_scalars: list of scalar names to include in the dataframe. Special str \"all\" is also accepted.\n        include_meta: list of metadata fields to include in the dataframe. Special str \"all\" is also accepted.\n\n    Returns:\n        DataFrame with data from across this collection\n    \"\"\"\n    dfs = [session.scalar_dataframe(include_scalars=include_scalars, include_meta=include_meta) for session in self]\n    return pd.concat(dfs).reset_index(drop=True)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.select","title":"<code>select(*bool_masks)</code>","text":"<p>Select sessions in this collection, returning a new <code>SessionCollection</code> containing sessions which all bool masks are true.</p> <p>Parameters:</p> <ul> <li> <code>bool_masks</code>               (<code>ndarray</code>, default:                   <code>()</code> )           \u2013            <p>one or more boolean arrays, the reduced logical_and indicating which sessions to select</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SessionCollection</code>           \u2013            <p>a new <code>SessionCollection</code> containing only items which pass bool_masks.</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def select(self, *bool_masks: np.ndarray) -&gt; \"SessionCollection\":\n    \"\"\"Select sessions in this collection, returning a new `SessionCollection` containing sessions which all bool masks are true.\n\n    Args:\n        bool_masks: one or more boolean arrays, the reduced logical_and indicating which sessions to select\n\n    Returns:\n        a new `SessionCollection` containing only items which pass bool_masks.\n    \"\"\"\n    sc = type(self)(item for item, include in zip(self, np.logical_and.reduce(bool_masks)) if include)\n    sc.__meta_meta.update(**copy.deepcopy(self.__meta_meta))\n    return sc\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.set_metadata_props","title":"<code>set_metadata_props(key, order=None)</code>","text":"<p>Set properties of a metadata column.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>name of the metadata item, always required</p> </li> <li> <code>order</code>               (<code>Optional[list[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>optional, if specified will set the metadata column to be ordered categorical, according to <code>order</code></p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def set_metadata_props(self, key: str, order: Optional[list[Any]] = None):\n    \"\"\"Set properties of a metadata column.\n\n    Args:\n        key: name of the metadata item, always required\n        order: optional, if specified will set the metadata column to be ordered categorical, according to `order`\n    \"\"\"\n    assert key in self.metadata_keys\n\n    if key not in self.__meta_meta:\n        self.__meta_meta[key] = {}\n\n    if order is not None:\n        self.__meta_meta[key][\"order\"] = order\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.signal_dataframe","title":"<code>signal_dataframe(signal, include_meta='all')</code>","text":"<p>Get data for a given signal across sessions, also injecting metadata.</p> <p>See also: <code>Signal.to_dataframe()</code></p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>str</code>)           \u2013            <p>Name of the signal to collect</p> </li> <li> <code>include_meta</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>include_meta: metadata fields to include in the final output. Special string \"all\" will include all metadata fields</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>signal across sessions as a <code>pandas.DataFrame</code></p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def signal_dataframe(self, signal: str, include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n    \"\"\"Get data for a given signal across sessions, also injecting metadata.\n\n    See also: `Signal.to_dataframe()`\n\n    Args:\n        signal: Name of the signal to collect\n        include_meta: include_meta: metadata fields to include in the final output. Special string \"all\" will include all metadata fields\n\n    Returns:\n        signal across sessions as a `pandas.DataFrame`\n    \"\"\"\n    dfs = []\n    for session in self:\n        if include_meta == \"all\":\n            meta = session.metadata\n        else:\n            meta = {k: v for k, v in session.metadata.items() if k in include_meta}\n\n        sig = session.signals[signal]\n        df = sig.to_dataframe()\n\n        df[\"obs\"] = list(range(sig.nobs))\n\n        for k, v in meta.items():\n            df[k] = v\n\n        # reorder columns\n        df = df[\n            [c for c in df.columns.values if not str(c).startswith(\"Y.\")] + [c for c in df.columns.values if str(c).startswith(\"Y.\")]\n        ]\n\n        dfs.append(df)\n\n    return pd.concat(dfs, ignore_index=True)\n</code></pre>"},{"location":"reference/fptools/io/session/#fptools.io.session.SessionCollection.update_metadata","title":"<code>update_metadata(meta)</code>","text":"<p>Set multiple metadata fields on each session in this collection.</p> <p>Parameters:</p> <ul> <li> <code>meta</code>               (<code>dict[str, Any]</code>)           \u2013            <p>metadata information to set on each session</p> </li> </ul> Source code in <code>fptools/io/session.py</code> <pre><code>def update_metadata(self, meta: dict[str, Any]) -&gt; None:\n    \"\"\"Set multiple metadata fields on each session in this collection.\n\n    Args:\n        meta: metadata information to set on each session\n    \"\"\"\n    for item in self:\n        item.metadata.update(meta)\n</code></pre>"},{"location":"reference/fptools/io/signal/","title":"signal","text":""},{"location":"reference/fptools/io/signal/#fptools.io.signal","title":"<code>fptools.io.signal</code>","text":"<p>Classes:</p> <ul> <li> <code>Signal</code>           \u2013            <p>Represents a real valued signal with fixed interval sampling.</p> </li> </ul>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal","title":"<code>Signal</code>","text":"<p>               Bases: <code>object</code></p> <p>Represents a real valued signal with fixed interval sampling.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name for this Signal</p> </li> <li> <code>signal</code>               (<code>ndarray</code>)           \u2013            <p>Array of signal values</p> </li> <li> <code>units</code>               (<code>str</code>)           \u2013            <p>units of measurement for this signal</p> </li> <li> <code>marks</code>               (<code>dict[str, float]</code>)           \u2013            <p>dictionary of timestamp labels</p> </li> <li> <code>time</code>               (<code>ndarray</code>)           \u2013            <p>Array of relative time values</p> </li> <li> <code>fs</code>               (<code>float</code>)           \u2013            <p>Sampling frequency, in Hz</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__add__</code>             \u2013              <p>Add another signal to this signal, returning a new signal.</p> </li> <li> <code>__eq__</code>             \u2013              <p>Test if this Signal is equal to another Signal.</p> </li> <li> <code>__init__</code>             \u2013              <p>Initialize a this Signal Object.</p> </li> <li> <code>__mul__</code>             \u2013              <p>Multiply another signal against this signal, returning a new signal.</p> </li> <li> <code>__sub__</code>             \u2013              <p>Subtract another signal from this signal, returning a new signal.</p> </li> <li> <code>__truediv__</code>             \u2013              <p>Divide this signal by another signal, returning a new signal.</p> </li> <li> <code>aggregate</code>             \u2013              <p>Aggregate this signal.</p> </li> <li> <code>copy</code>             \u2013              <p>Return a deep copy of this signal.</p> </li> <li> <code>describe</code>             \u2013              <p>Describe this Signal.</p> </li> <li> <code>tindex</code>             \u2013              <p>Get the sample index closest to time <code>t</code>.</p> </li> <li> <code>to_dataframe</code>             \u2013              <p>Get the signal data as a <code>pandas.DataFrame</code>.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>duration</code>               (<code>timedelta</code>)           \u2013            <p>Get the duration of this Signal.</p> </li> <li> <code>nobs</code>               (<code>int</code>)           \u2013            <p>Get the number of observations for this Signal (i.e. number of trials).</p> </li> <li> <code>nsamples</code>               (<code>int</code>)           \u2013            <p>Get the number of samples for this Signal (i.e. length of signal).</p> </li> </ul> Source code in <code>fptools/io/signal.py</code> <pre><code>class Signal(object):\n    \"\"\"Represents a real valued signal with fixed interval sampling.\n\n    Attributes:\n        name: Name for this Signal\n        signal: Array of signal values\n        units: units of measurement for this signal\n        marks: dictionary of timestamp labels\n        time: Array of relative time values\n        fs: Sampling frequency, in Hz\n    \"\"\"\n\n    def __init__(\n        self, name: str, signal: np.ndarray, time: Optional[np.ndarray] = None, fs: Optional[float] = None, units: str = \"AU\"\n    ) -&gt; None:\n        \"\"\"Initialize a this Signal Object.\n\n        At least one of `time` or `fs` must be provided. If `time` is provided, the sampling frequency (`fs`) will be estimated\n        from `time`. If `fs` is provided, the sampled timepoints (`time`) will be estimated. If both are provided, the values\n        will be checked against one another, and if they do not match, a ValueError will be raised.\n\n        Args:\n            name: Name of this signal\n            signal: Array of signal values\n            time: Array of sampled timepoints\n            fs: Sampling frequency, in Hz\n            units: units of this signal\n        \"\"\"\n        self.name: str = name\n        self.signal: np.ndarray = signal\n        self.units: str = units\n        self.marks: dict[str, float] = {}\n        self.time: np.ndarray\n        self.fs: float\n\n        if time is not None and fs is not None:\n            # both time and sampling frequency provided\n            self.time = time\n            self.fs = fs\n            # just do a sanity check that the two pieces of information make sense\n            if not np.isclose(self.fs, 1 / np.median(np.diff(time))):\n                raise ValueError(\n                    f\"Both `time` and `fs` were provided, but they do not match!\\n  fs={fs}\\ntime={1 / np.median(np.diff(time))}\"\n                )\n\n        elif time is None and fs is not None:\n            # sampling frequency is provided, infer time from fs\n            self.fs = fs\n            self.time = np.linspace(1, signal.shape[-1], signal.shape[-1]) / self.fs\n\n        elif fs is None and time is not None:\n            # time is provided, so lets estimate the sampling frequency\n            self.time = time\n            self.fs = 1 / np.median(np.diff(time))\n\n        else:\n            # neither time or sampling frequency provided, we need at least one!\n            raise ValueError(\"Both `time` and `fs` cannot be `None`, one must be supplied!\")\n\n        if not self.signal.shape[-1] == self.time.shape[0]:\n            raise ValueError(f\"Signal and time must have the same length! signal.shape={self.signal.shape}; time.shape={self.time.shape}\")\n\n    @property\n    def nobs(self) -&gt; int:\n        \"\"\"Get the number of observations for this Signal (i.e. number of trials).\"\"\"\n        return self.signal.shape[0] if len(self.signal.shape) &gt; 1 else 1\n\n    @property\n    def nsamples(self) -&gt; int:\n        \"\"\"Get the number of samples for this Signal (i.e. length of signal).\"\"\"\n        return self.signal.shape[-1]\n\n    @property\n    def duration(self) -&gt; datetime.timedelta:\n        \"\"\"Get the duration of this Signal.\"\"\"\n        return datetime.timedelta(seconds=self.time[-1] - self.time[0])\n\n    def tindex(self, t: float) -&gt; int:\n        \"\"\"Get the sample index closest to time `t`.\"\"\"\n        return int((np.abs(self.time - t)).argmin())\n\n    def copy(self, new_name: Optional[str] = None) -&gt; \"Signal\":\n        \"\"\"Return a deep copy of this signal.\n\n        Args:\n            new_name: if not None, assign the copy this new name\n\n        Returns:\n            A copy of this Signal\n        \"\"\"\n        if new_name is None:\n            new_name = self.name\n        s = type(self)(new_name, self.signal.copy(), time=self.time.copy(), fs=self.fs, units=self.units)\n        s.marks.update(**self.marks)\n        return s\n\n    def _check_other_compatible(self, other: \"Signal\") -&gt; bool:\n        return self.nsamples == other.nsamples and self.nobs == self.nobs and self.fs == other.fs\n\n    def __eq__(self, value: object) -&gt; bool:\n        \"\"\"Test if this Signal is equal to another Signal.\n\n        Args:\n            value: value to test against for equality\n\n        Returns:\n            True if value is equal to self, False otherwise. Equality is checked for FS, units, signal, time, and marks data.\n        \"\"\"\n        if not isinstance(value, Signal):\n            return False\n\n        if self.fs != value.fs:\n            return False\n\n        if self.units != value.units:\n            return False\n\n        if not np.allclose(self.signal, value.signal, equal_nan=True):\n            return False\n\n        if not np.array_equal(self.time, value.time):\n            return False\n\n        if not self.marks == value.marks:\n            return False\n\n        return True\n\n    def __add__(self, other: Any) -&gt; \"Signal\":\n        \"\"\"Add another signal to this signal, returning a new signal.\n\n        Args:\n            other: the other signal to be added to this signal\n\n        Return:\n            A new Signal with the addition result.\n        \"\"\"\n        s = self.copy()\n        if isinstance(other, Signal):\n            assert self._check_other_compatible(other)\n            s.signal += other.signal\n        elif isinstance(other, (int, float)):\n            s.signal += other\n        else:\n            raise NotImplementedError(f\"Add is not defined for type {type(other)}\")\n        return s\n\n    def __sub__(self, other: Any) -&gt; \"Signal\":\n        \"\"\"Subtract another signal from this signal, returning a new signal.\n\n        Args:\n            other: the other signal to be subtracted from this signal\n\n        Return:\n            A new Signal with the subtraction result.\n        \"\"\"\n        s = self.copy()\n        if isinstance(other, Signal):\n            assert self._check_other_compatible(other)\n            s.signal -= other.signal\n        elif isinstance(other, (int, float)):\n            s.signal -= other\n        else:\n            raise NotImplementedError(f\"Subtract is not defined for type {type(other)}\")\n        return s\n\n    def __mul__(self, other: Any) -&gt; \"Signal\":\n        \"\"\"Multiply another signal against this signal, returning a new signal.\n\n        Args:\n            other: the other signal to be multiplied by this signal\n\n        Return:\n            A new Signal with the multiplication result.\n        \"\"\"\n        s = self.copy()\n        if isinstance(other, Signal):\n            assert self._check_other_compatible(other)\n            s.signal *= other.signal\n        elif isinstance(other, (int, float)):\n            s.signal *= other\n        else:\n            raise NotImplementedError(f\"Multiply is not defined for type {type(other)}\")\n        return s\n\n    def __truediv__(self, other: Any) -&gt; \"Signal\":\n        \"\"\"Divide this signal by another signal, returning a new signal.\n\n        Args:\n            other: the other signal to divide this signal by\n\n        Return:\n            A new Signal with the division result.\n        \"\"\"\n        s = self.copy()\n        if isinstance(other, Signal):\n            assert self._check_other_compatible(other)\n            s.signal /= other.signal\n        elif isinstance(other, (int, float)):\n            s.signal /= other\n        else:\n            raise NotImplementedError(f\"Division is not defined for type {type(other)}\")\n        return s\n\n    def aggregate(self, func: Union[str, np.ufunc, Callable[[np.ndarray], np.ndarray]]) -&gt; \"Signal\":\n        \"\"\"Aggregate this signal.\n\n        If there is only a single observation, that observation is returned unchanged, otherwise  observations will\n        be aggregated by `func` along axis=0.\n\n        Marks, units, and time will be propagated. The new signal will be named according to this signal, with `#{func_name}` appended.\n\n        Args:\n            func: string or callable that take a (nobs x nsamples) array and returns a (nsamples,) shaped array. If a string\n                will be interpreted as the name of a numpy function (e.x. mean, median, etc)\n\n        Returns:\n            aggregated `Signal`\n        \"\"\"\n        if self.nobs == 1:\n            return self  # maybe we should raise??\n\n        else:\n            f: Callable[[np.ndarray], np.ndarray]\n            if isinstance(func, str):\n                f = partial(cast(np.ufunc, getattr(np, func)), axis=0)\n                f_name = func\n\n            elif isinstance(func, np.ufunc):\n                f = partial(func, axis=0)\n                f_name = func.__name__\n\n            else:\n                f = func\n                f_name = func.__name__\n\n            s = Signal(f\"{self.name}#{f_name}\", f(self.signal), time=self.time, units=self.units)\n            s.marks.update(self.marks)\n            return s\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Get the signal data as a `pandas.DataFrame`.\n\n        Observations are across rows, and samples are across columns. Each sample column is named with\n        the pattern `Y.{i+1}` where i is the sample index. This also implies 1-based indexing on the output.\n        \"\"\"\n        return pd.DataFrame(np.atleast_2d(self.signal), columns=[f\"Y.{i+1}\" for i in range(self.nsamples)])\n\n    def describe(self, as_str: bool = False, prefix: str = \"\") -&gt; Union[str, None]:\n        \"\"\"Describe this Signal.\n\n        Args:\n            as_str: if True, return description as a string, otherwise print the description and return None\n            prefix: a string to prepend to each line of output\n\n        Returns:\n            `None` if `as_str` is `False`; if `as_str` is `True`, returns the description as a `str`\n        \"\"\"\n        buffer = f\"{prefix}{self.name}:\\n\"\n        buffer += f\"{prefix}    units = {self.units}\\n\"\n        buffer += f\"{prefix}    n_observations = {self.nobs}\\n\"\n        buffer += f\"{prefix}    n_samples = {self.nsamples}\\n\"\n        buffer += f\"{prefix}    duration = {self.duration}\\n\"\n        buffer += f\"{prefix}    sample_rate = {self.fs}\\n\"\n        buffer += f\"{prefix}    min|mean|max = {self.signal.min():.2f}|{self.signal.mean():.2f}|{self.signal.max():.2f}\\n\"\n        if len(self.marks) &gt; 0:\n            buffer += f\"{prefix}    marks ({len(self.marks)}):\\n\"\n            for k, v in self.marks.items():\n                buffer += f\"{prefix}        {datetime.timedelta(seconds=v)} {k}\\n\"\n\n        if as_str:\n            return buffer\n        else:\n            print(buffer)\n            return None\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.duration","title":"<code>duration</code>  <code>property</code>","text":"<p>Get the duration of this Signal.</p>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.nobs","title":"<code>nobs</code>  <code>property</code>","text":"<p>Get the number of observations for this Signal (i.e. number of trials).</p>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.nsamples","title":"<code>nsamples</code>  <code>property</code>","text":"<p>Get the number of samples for this Signal (i.e. length of signal).</p>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.__add__","title":"<code>__add__(other)</code>","text":"<p>Add another signal to this signal, returning a new signal.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Any</code>)           \u2013            <p>the other signal to be added to this signal</p> </li> </ul> Return <p>A new Signal with the addition result.</p> Source code in <code>fptools/io/signal.py</code> <pre><code>def __add__(self, other: Any) -&gt; \"Signal\":\n    \"\"\"Add another signal to this signal, returning a new signal.\n\n    Args:\n        other: the other signal to be added to this signal\n\n    Return:\n        A new Signal with the addition result.\n    \"\"\"\n    s = self.copy()\n    if isinstance(other, Signal):\n        assert self._check_other_compatible(other)\n        s.signal += other.signal\n    elif isinstance(other, (int, float)):\n        s.signal += other\n    else:\n        raise NotImplementedError(f\"Add is not defined for type {type(other)}\")\n    return s\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.__eq__","title":"<code>__eq__(value)</code>","text":"<p>Test if this Signal is equal to another Signal.</p> <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>object</code>)           \u2013            <p>value to test against for equality</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if value is equal to self, False otherwise. Equality is checked for FS, units, signal, time, and marks data.</p> </li> </ul> Source code in <code>fptools/io/signal.py</code> <pre><code>def __eq__(self, value: object) -&gt; bool:\n    \"\"\"Test if this Signal is equal to another Signal.\n\n    Args:\n        value: value to test against for equality\n\n    Returns:\n        True if value is equal to self, False otherwise. Equality is checked for FS, units, signal, time, and marks data.\n    \"\"\"\n    if not isinstance(value, Signal):\n        return False\n\n    if self.fs != value.fs:\n        return False\n\n    if self.units != value.units:\n        return False\n\n    if not np.allclose(self.signal, value.signal, equal_nan=True):\n        return False\n\n    if not np.array_equal(self.time, value.time):\n        return False\n\n    if not self.marks == value.marks:\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.__init__","title":"<code>__init__(name, signal, time=None, fs=None, units='AU')</code>","text":"<p>Initialize a this Signal Object.</p> <p>At least one of <code>time</code> or <code>fs</code> must be provided. If <code>time</code> is provided, the sampling frequency (<code>fs</code>) will be estimated from <code>time</code>. If <code>fs</code> is provided, the sampled timepoints (<code>time</code>) will be estimated. If both are provided, the values will be checked against one another, and if they do not match, a ValueError will be raised.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of this signal</p> </li> <li> <code>signal</code>               (<code>ndarray</code>)           \u2013            <p>Array of signal values</p> </li> <li> <code>time</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>Array of sampled timepoints</p> </li> <li> <code>fs</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Sampling frequency, in Hz</p> </li> <li> <code>units</code>               (<code>str</code>, default:                   <code>'AU'</code> )           \u2013            <p>units of this signal</p> </li> </ul> Source code in <code>fptools/io/signal.py</code> <pre><code>def __init__(\n    self, name: str, signal: np.ndarray, time: Optional[np.ndarray] = None, fs: Optional[float] = None, units: str = \"AU\"\n) -&gt; None:\n    \"\"\"Initialize a this Signal Object.\n\n    At least one of `time` or `fs` must be provided. If `time` is provided, the sampling frequency (`fs`) will be estimated\n    from `time`. If `fs` is provided, the sampled timepoints (`time`) will be estimated. If both are provided, the values\n    will be checked against one another, and if they do not match, a ValueError will be raised.\n\n    Args:\n        name: Name of this signal\n        signal: Array of signal values\n        time: Array of sampled timepoints\n        fs: Sampling frequency, in Hz\n        units: units of this signal\n    \"\"\"\n    self.name: str = name\n    self.signal: np.ndarray = signal\n    self.units: str = units\n    self.marks: dict[str, float] = {}\n    self.time: np.ndarray\n    self.fs: float\n\n    if time is not None and fs is not None:\n        # both time and sampling frequency provided\n        self.time = time\n        self.fs = fs\n        # just do a sanity check that the two pieces of information make sense\n        if not np.isclose(self.fs, 1 / np.median(np.diff(time))):\n            raise ValueError(\n                f\"Both `time` and `fs` were provided, but they do not match!\\n  fs={fs}\\ntime={1 / np.median(np.diff(time))}\"\n            )\n\n    elif time is None and fs is not None:\n        # sampling frequency is provided, infer time from fs\n        self.fs = fs\n        self.time = np.linspace(1, signal.shape[-1], signal.shape[-1]) / self.fs\n\n    elif fs is None and time is not None:\n        # time is provided, so lets estimate the sampling frequency\n        self.time = time\n        self.fs = 1 / np.median(np.diff(time))\n\n    else:\n        # neither time or sampling frequency provided, we need at least one!\n        raise ValueError(\"Both `time` and `fs` cannot be `None`, one must be supplied!\")\n\n    if not self.signal.shape[-1] == self.time.shape[0]:\n        raise ValueError(f\"Signal and time must have the same length! signal.shape={self.signal.shape}; time.shape={self.time.shape}\")\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.__mul__","title":"<code>__mul__(other)</code>","text":"<p>Multiply another signal against this signal, returning a new signal.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Any</code>)           \u2013            <p>the other signal to be multiplied by this signal</p> </li> </ul> Return <p>A new Signal with the multiplication result.</p> Source code in <code>fptools/io/signal.py</code> <pre><code>def __mul__(self, other: Any) -&gt; \"Signal\":\n    \"\"\"Multiply another signal against this signal, returning a new signal.\n\n    Args:\n        other: the other signal to be multiplied by this signal\n\n    Return:\n        A new Signal with the multiplication result.\n    \"\"\"\n    s = self.copy()\n    if isinstance(other, Signal):\n        assert self._check_other_compatible(other)\n        s.signal *= other.signal\n    elif isinstance(other, (int, float)):\n        s.signal *= other\n    else:\n        raise NotImplementedError(f\"Multiply is not defined for type {type(other)}\")\n    return s\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.__sub__","title":"<code>__sub__(other)</code>","text":"<p>Subtract another signal from this signal, returning a new signal.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Any</code>)           \u2013            <p>the other signal to be subtracted from this signal</p> </li> </ul> Return <p>A new Signal with the subtraction result.</p> Source code in <code>fptools/io/signal.py</code> <pre><code>def __sub__(self, other: Any) -&gt; \"Signal\":\n    \"\"\"Subtract another signal from this signal, returning a new signal.\n\n    Args:\n        other: the other signal to be subtracted from this signal\n\n    Return:\n        A new Signal with the subtraction result.\n    \"\"\"\n    s = self.copy()\n    if isinstance(other, Signal):\n        assert self._check_other_compatible(other)\n        s.signal -= other.signal\n    elif isinstance(other, (int, float)):\n        s.signal -= other\n    else:\n        raise NotImplementedError(f\"Subtract is not defined for type {type(other)}\")\n    return s\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.__truediv__","title":"<code>__truediv__(other)</code>","text":"<p>Divide this signal by another signal, returning a new signal.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Any</code>)           \u2013            <p>the other signal to divide this signal by</p> </li> </ul> Return <p>A new Signal with the division result.</p> Source code in <code>fptools/io/signal.py</code> <pre><code>def __truediv__(self, other: Any) -&gt; \"Signal\":\n    \"\"\"Divide this signal by another signal, returning a new signal.\n\n    Args:\n        other: the other signal to divide this signal by\n\n    Return:\n        A new Signal with the division result.\n    \"\"\"\n    s = self.copy()\n    if isinstance(other, Signal):\n        assert self._check_other_compatible(other)\n        s.signal /= other.signal\n    elif isinstance(other, (int, float)):\n        s.signal /= other\n    else:\n        raise NotImplementedError(f\"Division is not defined for type {type(other)}\")\n    return s\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.aggregate","title":"<code>aggregate(func)</code>","text":"<p>Aggregate this signal.</p> <p>If there is only a single observation, that observation is returned unchanged, otherwise  observations will be aggregated by <code>func</code> along axis=0.</p> <p>Marks, units, and time will be propagated. The new signal will be named according to this signal, with <code>#{func_name}</code> appended.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Union[str, ufunc, Callable[[ndarray], ndarray]]</code>)           \u2013            <p>string or callable that take a (nobs x nsamples) array and returns a (nsamples,) shaped array. If a string will be interpreted as the name of a numpy function (e.x. mean, median, etc)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Signal</code>           \u2013            <p>aggregated <code>Signal</code></p> </li> </ul> Source code in <code>fptools/io/signal.py</code> <pre><code>def aggregate(self, func: Union[str, np.ufunc, Callable[[np.ndarray], np.ndarray]]) -&gt; \"Signal\":\n    \"\"\"Aggregate this signal.\n\n    If there is only a single observation, that observation is returned unchanged, otherwise  observations will\n    be aggregated by `func` along axis=0.\n\n    Marks, units, and time will be propagated. The new signal will be named according to this signal, with `#{func_name}` appended.\n\n    Args:\n        func: string or callable that take a (nobs x nsamples) array and returns a (nsamples,) shaped array. If a string\n            will be interpreted as the name of a numpy function (e.x. mean, median, etc)\n\n    Returns:\n        aggregated `Signal`\n    \"\"\"\n    if self.nobs == 1:\n        return self  # maybe we should raise??\n\n    else:\n        f: Callable[[np.ndarray], np.ndarray]\n        if isinstance(func, str):\n            f = partial(cast(np.ufunc, getattr(np, func)), axis=0)\n            f_name = func\n\n        elif isinstance(func, np.ufunc):\n            f = partial(func, axis=0)\n            f_name = func.__name__\n\n        else:\n            f = func\n            f_name = func.__name__\n\n        s = Signal(f\"{self.name}#{f_name}\", f(self.signal), time=self.time, units=self.units)\n        s.marks.update(self.marks)\n        return s\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.copy","title":"<code>copy(new_name=None)</code>","text":"<p>Return a deep copy of this signal.</p> <p>Parameters:</p> <ul> <li> <code>new_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>if not None, assign the copy this new name</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Signal</code>           \u2013            <p>A copy of this Signal</p> </li> </ul> Source code in <code>fptools/io/signal.py</code> <pre><code>def copy(self, new_name: Optional[str] = None) -&gt; \"Signal\":\n    \"\"\"Return a deep copy of this signal.\n\n    Args:\n        new_name: if not None, assign the copy this new name\n\n    Returns:\n        A copy of this Signal\n    \"\"\"\n    if new_name is None:\n        new_name = self.name\n    s = type(self)(new_name, self.signal.copy(), time=self.time.copy(), fs=self.fs, units=self.units)\n    s.marks.update(**self.marks)\n    return s\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.describe","title":"<code>describe(as_str=False, prefix='')</code>","text":"<p>Describe this Signal.</p> <p>Parameters:</p> <ul> <li> <code>as_str</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if True, return description as a string, otherwise print the description and return None</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>a string to prepend to each line of output</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, None]</code>           \u2013            <p><code>None</code> if <code>as_str</code> is <code>False</code>; if <code>as_str</code> is <code>True</code>, returns the description as a <code>str</code></p> </li> </ul> Source code in <code>fptools/io/signal.py</code> <pre><code>def describe(self, as_str: bool = False, prefix: str = \"\") -&gt; Union[str, None]:\n    \"\"\"Describe this Signal.\n\n    Args:\n        as_str: if True, return description as a string, otherwise print the description and return None\n        prefix: a string to prepend to each line of output\n\n    Returns:\n        `None` if `as_str` is `False`; if `as_str` is `True`, returns the description as a `str`\n    \"\"\"\n    buffer = f\"{prefix}{self.name}:\\n\"\n    buffer += f\"{prefix}    units = {self.units}\\n\"\n    buffer += f\"{prefix}    n_observations = {self.nobs}\\n\"\n    buffer += f\"{prefix}    n_samples = {self.nsamples}\\n\"\n    buffer += f\"{prefix}    duration = {self.duration}\\n\"\n    buffer += f\"{prefix}    sample_rate = {self.fs}\\n\"\n    buffer += f\"{prefix}    min|mean|max = {self.signal.min():.2f}|{self.signal.mean():.2f}|{self.signal.max():.2f}\\n\"\n    if len(self.marks) &gt; 0:\n        buffer += f\"{prefix}    marks ({len(self.marks)}):\\n\"\n        for k, v in self.marks.items():\n            buffer += f\"{prefix}        {datetime.timedelta(seconds=v)} {k}\\n\"\n\n    if as_str:\n        return buffer\n    else:\n        print(buffer)\n        return None\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.tindex","title":"<code>tindex(t)</code>","text":"<p>Get the sample index closest to time <code>t</code>.</p> Source code in <code>fptools/io/signal.py</code> <pre><code>def tindex(self, t: float) -&gt; int:\n    \"\"\"Get the sample index closest to time `t`.\"\"\"\n    return int((np.abs(self.time - t)).argmin())\n</code></pre>"},{"location":"reference/fptools/io/signal/#fptools.io.signal.Signal.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Get the signal data as a <code>pandas.DataFrame</code>.</p> <p>Observations are across rows, and samples are across columns. Each sample column is named with the pattern <code>Y.{i+1}</code> where i is the sample index. This also implies 1-based indexing on the output.</p> Source code in <code>fptools/io/signal.py</code> <pre><code>def to_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Get the signal data as a `pandas.DataFrame`.\n\n    Observations are across rows, and samples are across columns. Each sample column is named with\n    the pattern `Y.{i+1}` where i is the sample index. This also implies 1-based indexing on the output.\n    \"\"\"\n    return pd.DataFrame(np.atleast_2d(self.signal), columns=[f\"Y.{i+1}\" for i in range(self.nsamples)])\n</code></pre>"},{"location":"reference/fptools/io/tdt/","title":"tdt","text":""},{"location":"reference/fptools/io/tdt/#fptools.io.tdt","title":"<code>fptools.io.tdt</code>","text":"<p>Functions:</p> <ul> <li> <code>find_tdt_blocks</code>             \u2013              <p>Data Locator for TDT blocks.</p> </li> <li> <code>load_tdt_block</code>             \u2013              <p>Data Loader for TDT blocks.</p> </li> </ul>"},{"location":"reference/fptools/io/tdt/#fptools.io.tdt.find_tdt_blocks","title":"<code>find_tdt_blocks(path)</code>","text":"<p>Data Locator for TDT blocks.</p> <p>Given a path to a directory, will search that path recursively for TDT blocks.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to search for TDT blocks</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[DataTypeAdaptor]</code>           \u2013            <p>list of DataTypeAdaptor, each adaptor corresponding to one session, of data to be loaded</p> </li> </ul> Source code in <code>fptools/io/tdt.py</code> <pre><code>def find_tdt_blocks(path: str) -&gt; list[DataTypeAdaptor]:\n    \"\"\"Data Locator for TDT blocks.\n\n    Given a path to a directory, will search that path recursively for TDT blocks.\n\n    Args:\n        path: path to search for TDT blocks\n\n    Returns:\n        list of DataTypeAdaptor, each adaptor corresponding to one session, of data to be loaded\n    \"\"\"\n    tbk_files = glob.glob(os.path.join(path, \"**/*.[tT][bB][kK]\"), recursive=True)\n    items_out = []\n    for tbk in tbk_files:\n        adapt = DataTypeAdaptor()\n        adapt.path = os.path.dirname(tbk)  # the directory for the block\n        adapt.name = os.path.basename(adapt.path)  # the name of the directory\n        adapt.loaders.append(load_tdt_block)\n        items_out.append(adapt)\n    return items_out\n</code></pre>"},{"location":"reference/fptools/io/tdt/#fptools.io.tdt.load_tdt_block","title":"<code>load_tdt_block(session, path)</code>","text":"<p>Data Loader for TDT blocks.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session for data to be loaded into</p> </li> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to a TDT block folder</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session object with data added</p> </li> </ul> Source code in <code>fptools/io/tdt.py</code> <pre><code>def load_tdt_block(session: Session, path: str) -&gt; Session:\n    \"\"\"Data Loader for TDT blocks.\n\n    Args:\n        session: the session for data to be loaded into\n        path: path to a TDT block folder\n\n    Returns:\n        Session object with data added\n    \"\"\"\n    # read the block\n    block = tdt.read_block(path)\n\n    # add metadata from the block\n    session.metadata.update(dict(block.info.items()))\n\n    # add streams (as signals)\n    for k in block.streams.keys():\n        stream = block.streams[k]\n        session.add_signal(Signal(k, stream.data, fs=stream.fs, units=\"mV\"))\n\n    # add epocs\n    for k in block.epocs.keys():\n        session.epocs[k] = block.epocs[k].onset\n\n    # add scalars\n    for k in block.scalars.keys():\n        session.scalars[k] = block.scalars[k].ts\n\n    return session\n</code></pre>"},{"location":"reference/fptools/io/test/","title":"test","text":""},{"location":"reference/fptools/io/test/#fptools.io.test","title":"<code>fptools.io.test</code>","text":"<p>Functions:</p> <ul> <li> <code>download_test_data</code>             \u2013              <p>Download test data to a directory.</p> </li> <li> <code>list_datasets</code>             \u2013              <p>List the datasets on path.</p> </li> </ul>"},{"location":"reference/fptools/io/test/#fptools.io.test._get_default_test_data_location","title":"<code>_get_default_test_data_location()</code>","text":"<p>Get the default user test data location, in the users home directory.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>string path to the default user test data location.</p> </li> </ul> Source code in <code>fptools/io/test.py</code> <pre><code>def _get_default_test_data_location() -&gt; str:\n    \"\"\"Get the default user test data location, in the users home directory.\n\n    Returns:\n        string path to the default user test data location.\n    \"\"\"\n    return os.path.join(os.path.expanduser(\"~\"), \"fptools_test_data\")\n</code></pre>"},{"location":"reference/fptools/io/test/#fptools.io.test.download_test_data","title":"<code>download_test_data(dest=None)</code>","text":"<p>Download test data to a directory.</p> <p>Parameters:</p> <ul> <li> <code>dest</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>path to a directory where test data should be saved to. If None, defaults to a folder in the current users home directory named \"fptools_test_data\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>path to a directory containing the test data</p> </li> </ul> Source code in <code>fptools/io/test.py</code> <pre><code>def download_test_data(dest: Optional[str] = None) -&gt; str:\n    \"\"\"Download test data to a directory.\n\n    Args:\n        dest: path to a directory where test data should be saved to. If None, defaults to a folder in the current users home directory named \"fptools_test_data\".\n\n    Returns:\n        path to a directory containing the test data\n    \"\"\"\n    # this is the URL to a zip file containing the test data\n    test_data_link = r\"https://rutgers.box.com/shared/static/pd6pl4ieo9je5ahh2f22z3t0yssxjern.zip\"\n\n    if dest is None:\n        dest = _get_default_test_data_location()\n\n    # ensure the destination directory exists\n    os.makedirs(dest, exist_ok=True)\n\n    # check if the data appears to be already in place\n    if len(glob.glob(os.path.join(dest, \"*\"))) &lt;= 0:\n        print(f'Downloading test data and unpacking to \"{dest}\"')\n\n        try:\n            # location where the zip file will be saved\n            zip_path = os.path.join(dest, 'test_data.zip')\n\n            # initiate the request for the data file, check status and calculate the size\n            response = requests.get(test_data_link, stream=True)\n            response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n            total_size = int(response.headers.get('content-length', 0))\n            block_size = 1024\n\n            # commence with downloading the file, providing progress feedback along the way\n            with open(zip_path, 'wb') as zip_file, tqdm(\n                desc=\"Downloading\",\n                total=total_size,\n                unit='iB',\n                unit_scale=True,\n                unit_divisor=1024,\n            ) as bar:\n                for data in response.iter_content(block_size):\n                    bar.update(len(data))\n                    zip_file.write(data)\n\n            # now unpack the zip file, providing progress feedback along the way\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref, tqdm(\n                desc=\"Extracting\",\n                total=len(zip_ref.namelist()),\n                unit=\"file\",\n            ) as bar:\n                for file in zip_ref.namelist():\n                    zip_ref.extract(file, path=dest)\n                    bar.update(1)\n\n            # finally, remove the zip file\n            os.remove(zip_path)\n\n        except requests.exceptions.RequestException as e:\n            print(f\"Request error: {e}\")\n        except zipfile.BadZipFile as e:\n            print(f\"Zip file error: {e}\")\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n\n    else:\n        print(f'Test data appears to already be in place at \"{dest}\".')\n\n    return dest\n</code></pre>"},{"location":"reference/fptools/io/test/#fptools.io.test.list_datasets","title":"<code>list_datasets(path=None)</code>","text":"<p>List the datasets on path.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>the path to look on for datasets. If None, uses the default test data location.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list of dataset names.</p> </li> </ul> Source code in <code>fptools/io/test.py</code> <pre><code>def list_datasets(path: Optional[str] = None) -&gt; list[str]:\n    \"\"\"List the datasets on path.\n\n    Args:\n        path: the path to look on for datasets. If None, uses the default test data location.\n\n    Returns:\n        list of dataset names.\n    \"\"\"\n    if path is None:\n        path = _get_default_test_data_location()\n\n    return [entry for entry in os.listdir(path) if os.path.isdir(os.path.join(path, entry))]\n</code></pre>"},{"location":"reference/fptools/measure/","title":"measure","text":""},{"location":"reference/fptools/measure/#fptools.measure","title":"<code>fptools.measure</code>","text":"<p>Modules:</p> <ul> <li> <code>peaks</code>           \u2013            </li> <li> <code>signal_collector</code>           \u2013            </li> <li> <code>snr</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/measure/peaks/","title":"peaks","text":""},{"location":"reference/fptools/measure/peaks/#fptools.measure.peaks","title":"<code>fptools.measure.peaks</code>","text":"<p>Classes:</p> <ul> <li> <code>PeakFilterProvider</code>           \u2013            </li> </ul> <p>Functions:</p> <ul> <li> <code>measure_peaks</code>             \u2013              <p>Measure peaks within a signal.</p> </li> </ul>"},{"location":"reference/fptools/measure/peaks/#fptools.measure.peaks.PeakFilterProvider","title":"<code>PeakFilterProvider</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Peak Filter Provider protocol.</p> </li> </ul> Source code in <code>fptools/measure/peaks.py</code> <pre><code>@runtime_checkable\nclass PeakFilterProvider(Protocol):\n    def __call__(self, session: Session, signal: Signal, trial: int, trail_data: np.ndarray) -&gt; PeakFilter:\n        \"\"\"Peak Filter Provider protocol.\n\n        A Peak Filter Provider allows one to dynamically provide filter values for `scipy.signal.find_peaks()`.\n\n        Args:\n            session: the current `Session` being operated upon\n            signal: the current `Signal` being operated upon\n            trial: the current index of signal\n            trail_data: array of signal values for the current trial\n\n        Returns:\n            The return value should be something that `scipy.signal.find_peaks()` can use for the property you wish to filter.\n            Typically, this means one of the following: `None`, a scalar float, or a tuple containing some combination of None or scalar floats\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/fptools/measure/peaks/#fptools.measure.peaks.PeakFilterProvider.__call__","title":"<code>__call__(session, signal, trial, trail_data)</code>","text":"<p>Peak Filter Provider protocol.</p> <p>A Peak Filter Provider allows one to dynamically provide filter values for <code>scipy.signal.find_peaks()</code>.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the current <code>Session</code> being operated upon</p> </li> <li> <code>signal</code>               (<code>Signal</code>)           \u2013            <p>the current <code>Signal</code> being operated upon</p> </li> <li> <code>trial</code>               (<code>int</code>)           \u2013            <p>the current index of signal</p> </li> <li> <code>trail_data</code>               (<code>ndarray</code>)           \u2013            <p>array of signal values for the current trial</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PeakFilter</code>           \u2013            <p>The return value should be something that <code>scipy.signal.find_peaks()</code> can use for the property you wish to filter. Typically, this means one of the following: <code>None</code>, a scalar float, or a tuple containing some combination of None or scalar floats</p> </li> </ul> Source code in <code>fptools/measure/peaks.py</code> <pre><code>def __call__(self, session: Session, signal: Signal, trial: int, trail_data: np.ndarray) -&gt; PeakFilter:\n    \"\"\"Peak Filter Provider protocol.\n\n    A Peak Filter Provider allows one to dynamically provide filter values for `scipy.signal.find_peaks()`.\n\n    Args:\n        session: the current `Session` being operated upon\n        signal: the current `Signal` being operated upon\n        trial: the current index of signal\n        trail_data: array of signal values for the current trial\n\n    Returns:\n        The return value should be something that `scipy.signal.find_peaks()` can use for the property you wish to filter.\n        Typically, this means one of the following: `None`, a scalar float, or a tuple containing some combination of None or scalar floats\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/fptools/measure/peaks/#fptools.measure.peaks.measure_peaks","title":"<code>measure_peaks(sessions, signal, include_meta='all', include_detection_params=False, **kwargs)</code>","text":"<p>Measure peaks within a signal.</p> <p>Default detection parameters are as follows:</p> <p>These parameters are designed to not filter any peaks, but also provoke <code>scipy.signal.find_peaks()</code> into returning all peak measurement fields.</p> <p>You may override any of these parameters to <code>scipy.signal.find_peaks()</code> via this functions <code>**kwargs</code>.</p> <p>Also allowed for any valid detection parameter is a callable (see <code>PeakFilterProvider</code> for signature) that is given the session, signal, trial index and trial data, and should return valid detection parameter values (i.e. None, float, or tuple of the preceding). The callable is evaluated for each observation in the signal in a given session.</p> <p>The returned dataframe will contain the following information (one row corresponds to one peak) - any metadata from the Session, as specified by the <code>include_meta</code> parameter - trial: index of the observation from the signal data that the peak was found - peak_num: which peak, within a given trial, for the case of multiple found peaks within a single trial - peak_index: the index along the signal where the peak was found - peak_time: this is <code>peak_index</code> converted to the time domain (i.e. relative seconds)</p> <p>The following measurements are reported by <code>scipy.signal.find_peaks()</code>: - peak_heights: this is the height of the peak, as returned by <code>scipy.signal.find_peaks()</code> - left_thresholds, right_thresholds: this is the peak vertical distance to its neighboring samples, as returned by <code>scipy.signal.find_peaks()</code> - prominences: this is the prominence of the peak, as returned by <code>scipy.signal.find_peaks()</code> - left_bases, right_bases: The peak's bases as indices in x to the left and right of each peak, as returned by <code>scipy.signal.find_peaks()</code>. The higher base of each pair is a peak's lowest contour line. - widths: the width of the peak, as returned by <code>scipy.signal.find_peaks()</code>. - width_heights: The height of the contour lines at which the widths where evaluated, as returned by <code>scipy.signal.find_peaks()</code>.. - left_ips, right_ips: Interpolated positions of left and right intersection points of a horizontal line at the respective evaluation height, as returned by <code>scipy.signal.find_peaks()</code>.. - auc: area under the curve of the given trial, as returned by <code>sklearn.metrics.auc()</code></p> <p>Parameters:</p> <ul> <li> <code>sessions</code>               (<code>SessionCollection</code>)           \u2013            <p>collection of sessions to work on</p> </li> <li> <code>signal</code>               (<code>str</code>)           \u2013            <p>name of the signal to measure</p> </li> <li> <code>include_meta</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>metadata fields to include in the final output. Special string \"all\" will include all metadata fields</p> </li> <li> <code>include_detection_params</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if True, include detection parameters as columns in the output dataframe</p> </li> <li> <code>**kwargs</code>           \u2013            <p>additional kwargs to pass to <code>scipy.signal.find_peaks()</code></p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pandas <code>DataFrame</code> with peak measurements.</p> </li> </ul> Source code in <code>fptools/measure/peaks.py</code> <pre><code>def measure_peaks(\n    sessions: SessionCollection, signal: str, include_meta: FieldList = \"all\", include_detection_params: bool = False, **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"Measure peaks within a signal.\n\n    Default detection parameters are as follows:\n    {\"height\": (None, None), \"threshold\": (None, None), \"distance\": None, \"prominence\": (None, None), \"width\": (None, None) }\n\n    These parameters are designed to not filter any peaks, but also provoke `scipy.signal.find_peaks()` into returning all peak measurement fields.\n\n    You may override any of these parameters to `scipy.signal.find_peaks()` via this functions `**kwargs`.\n\n    Also allowed for any valid detection parameter is a callable (see `PeakFilterProvider` for signature) that is given the session, signal, trial index and trial\n    data, and should return valid detection parameter values (i.e. None, float, or tuple of the preceding). The callable is evaluated for each observation in the\n    signal in a given session.\n\n    The returned dataframe will contain the following information (one row corresponds to one peak)\n    - any metadata from the Session, as specified by the `include_meta` parameter\n    - trial: index of the observation from the signal data that the peak was found\n    - peak_num: which peak, within a given trial, for the case of multiple found peaks within a single trial\n    - peak_index: the index along the signal where the peak was found\n    - peak_time: this is `peak_index` converted to the time domain (i.e. relative seconds)\n\n    The following measurements are reported by `scipy.signal.find_peaks()`:\n    - peak_heights: this is the height of the peak, as returned by `scipy.signal.find_peaks()`\n    - left_thresholds, right_thresholds: this is the peak vertical distance to its neighboring samples, as returned by `scipy.signal.find_peaks()`\n    - prominences: this is the prominence of the peak, as returned by `scipy.signal.find_peaks()`\n    - left_bases, right_bases: The peak's bases as indices in x to the left and right of each peak, as returned by `scipy.signal.find_peaks()`. The higher base of each pair is a peak's lowest contour line.\n    - widths: the width of the peak, as returned by `scipy.signal.find_peaks()`.\n    - width_heights: The height of the contour lines at which the widths where evaluated, as returned by `scipy.signal.find_peaks()`..\n    - left_ips, right_ips: Interpolated positions of left and right intersection points of a horizontal line at the respective evaluation height, as returned by `scipy.signal.find_peaks()`..\n    - auc: area under the curve of the given trial, as returned by `sklearn.metrics.auc()`\n\n    Args:\n        sessions: collection of sessions to work on\n        signal: name of the signal to measure\n        include_meta: metadata fields to include in the final output. Special string \"all\" will include all metadata fields\n        include_detection_params: if True, include detection parameters as columns in the output dataframe\n        **kwargs: additional kwargs to pass to `scipy.signal.find_peaks()`\n\n    Returns:\n        pandas `DataFrame` with peak measurements.\n    \"\"\"\n    # these serve as the base detection params. Designed to evoke all the extra results from `find_peaks()`\n    # without actually filtering anything. These can be overridden by the user via kwargs\n    detection_params: dict[str, Union[PeakFilter, PeakFilterProvider]] = {\n        \"height\": (None, None),\n        \"threshold\": (None, None),\n        \"distance\": None,\n        \"prominence\": (None, None),\n        \"width\": (None, None),\n    }\n    # update the detection params with any the user provided.\n    detection_params.update(**kwargs)\n\n    peak_data = []\n    for session in sessions:\n        # determine metadata fields to include\n        if include_meta == \"all\":\n            meta = session.metadata\n        else:\n            meta = {k: v for k, v in session.metadata.items() if k in include_meta}\n\n        # fetch time and signal data\n        t = session.signals[signal].time\n        sig = np.atleast_2d(session.signals[signal].signal)\n        for i in range(sig.shape[0]):\n            # setup detection params for the current trial.\n            # the user could possibly provide us a callable to dynamically determine detection parameter values\n            current_detection_params = detection_params.copy()\n            for key in current_detection_params.keys():\n                param = current_detection_params[key]\n                if isinstance(param, PeakFilterProvider):\n                    current_detection_params[key] = param(session, session.signals[signal], i, sig[i, :])\n\n            # detect peaks\n            peaks, props = scipy.signal.find_peaks(sig[i, :], **current_detection_params)\n\n            # for each peak, prepare a \"row\" for the output dataframe\n            for peak_i in range(len(peaks)):\n                peak_slice = slice(props[\"left_bases\"][peak_i], props[\"right_bases\"][peak_i])\n                result = {\n                    **meta,\n                    \"trial\": i,\n                    \"peak_num\": peak_i,\n                    \"peak_index\": peaks[peak_i],\n                    \"peak_time\": t[peaks[peak_i]],\n                    **{k: v[peak_i] for k, v in props.items()},\n                    \"auc\": metrics.auc(t[peak_slice], sig[i, peak_slice]),\n                }\n\n                # include any detection params if the user requested that feature\n                if include_detection_params:\n                    result.update({f\"param_{k}\": v for k, v in current_detection_params.items()})\n\n                # collect the result\n                peak_data.append(result)\n\n    # convert results to a dataframe and return\n    peak_data = pd.DataFrame(peak_data)\n    return peak_data\n</code></pre>"},{"location":"reference/fptools/measure/signal_collector/","title":"signal_collector","text":""},{"location":"reference/fptools/measure/signal_collector/#fptools.measure.signal_collector","title":"<code>fptools.measure.signal_collector</code>","text":"<p>Functions:</p> <ul> <li> <code>collect_signals</code>             \u2013              <p>Collect a signal from a session around an event.</p> </li> <li> <code>collect_signals_2event</code>             \u2013              <p>Collect a signal from a session around two events.</p> </li> </ul>"},{"location":"reference/fptools/measure/signal_collector/#fptools.measure.signal_collector.collect_signals","title":"<code>collect_signals(session, event, signal, start=-1.0, stop=3.0, out_name=None)</code>","text":"<p>Collect a signal from a session around an event.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the Session to operate on</p> </li> <li> <code>event</code>               (<code>str</code>)           \u2013            <p>the name of the event to use</p> </li> <li> <code>signal</code>               (<code>str</code>)           \u2013            <p>the name of the signal to collect</p> </li> <li> <code>start</code>               (<code>float</code>, default:                   <code>-1.0</code> )           \u2013            <p>start of the collection interval, in seconds, relative to each event. Negative values imply prior to event, positive values imply after the event</p> </li> <li> <code>stop</code>               (<code>float</code>, default:                   <code>3.0</code> )           \u2013            <p>end of the collection interval, in seconds, relative to each event. Negative values imply prior to event, positive values imply after the event</p> </li> <li> <code>out_name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>if not None, the name of the returned <code>Signal</code> object. Otherwise, the new <code>Signal</code> object's name will be generated as <code>{signal}@{event}</code></p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Signal</code>           \u2013            <p>the collected Signal</p> </li> </ul> Source code in <code>fptools/measure/signal_collector.py</code> <pre><code>def collect_signals(\n    session: Session, event: str, signal: str, start: float = -1.0, stop: float = 3.0, out_name: Optional[str] = None\n) -&gt; Signal:\n    \"\"\"Collect a signal from a session around an event.\n\n    Args:\n        session: the Session to operate on\n        event: the name of the event to use\n        signal: the name of the signal to collect\n        start: start of the collection interval, in seconds, relative to each event. Negative values imply prior to event, positive values imply after the event\n        stop: end of the collection interval, in seconds, relative to each event. Negative values imply prior to event, positive values imply after the event\n        out_name: if not None, the name of the returned `Signal` object. Otherwise, the new `Signal` object's name will be generated as `{signal}@{event}`\n\n    Returns:\n        the collected Signal\n    \"\"\"\n    assert start &lt; stop\n\n    sig = session.signals[signal]\n    events = session.epocs[event]\n\n    n_samples = int(np.rint((stop - start) * sig.fs))\n    offset = int(np.rint(start * sig.fs))\n    new_time = fs2t(sig.fs, n_samples) + start\n\n    accum = np.zeros_like(sig.signal, shape=(events.shape[0], n_samples))\n    padding = abs(offset) + n_samples\n    padded_signal = np.pad(sig.signal, (padding, padding), mode=\"constant\", constant_values=0)\n    for ei, evt in enumerate(events):\n        event_idx = sig.tindex(evt) + padding  # add padding\n        accum[ei, :] = padded_signal[(event_idx + offset) : (event_idx + offset + n_samples)]\n\n    if out_name is None:\n        out_name = f\"{signal}@{event}\"\n\n    s = Signal(out_name, accum, time=new_time, units=sig.units)\n    s.marks[event] = 0\n    return s\n</code></pre>"},{"location":"reference/fptools/measure/signal_collector/#fptools.measure.signal_collector.collect_signals_2event","title":"<code>collect_signals_2event(session, event1, event2, signal, pre=2.0, inter=2.0, post=2.0, out_name=None)</code>","text":"<p>Collect a signal from a session around two events.</p> <p>Collects a fixed amount of time before event1 and after event2. The \"real\" time between event1 and event2 is scaled to a \"meta\" time specified by <code>inter</code>. This is done by resampling the inter-event time using linear interpolation.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the Session to operate on</p> </li> <li> <code>event1</code>               (<code>str</code>)           \u2013            <p>the name of the first event to use</p> </li> <li> <code>event2</code>               (<code>str</code>)           \u2013            <p>the name of the second event to use</p> </li> <li> <code>signal</code>               (<code>str</code>)           \u2013            <p>the name of the signal to collect</p> </li> <li> <code>pre</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>amount of time, in seconds, to collect prior to each event</p> </li> <li> <code>inter</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>amount of \"meta\" time, in seconds, to collect between events</p> </li> <li> <code>post</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>amount of time, in seconds, to collect after each event</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Signal</code>           \u2013            <p>the collected Signal</p> </li> </ul> Source code in <code>fptools/measure/signal_collector.py</code> <pre><code>def collect_signals_2event(\n    session: Session,\n    event1: str,\n    event2: str,\n    signal: str,\n    pre: float = 2.0,\n    inter: float = 2.0,\n    post: float = 2.0,\n    out_name: Optional[str] = None,\n) -&gt; Signal:\n    \"\"\"Collect a signal from a session around two events.\n\n    Collects a fixed amount of time before event1 and after event2. The \"real\" time between event1 and event2\n    is scaled to a \"meta\" time specified by `inter`. This is done by resampling the inter-event time using\n    linear interpolation.\n\n    Args:\n        session: the Session to operate on\n        event1: the name of the first event to use\n        event2: the name of the second event to use\n        signal: the name of the signal to collect\n        pre: amount of time, in seconds, to collect prior to each event\n        inter: amount of \"meta\" time, in seconds, to collect between events\n        post: amount of time, in seconds, to collect after each event\n\n    Returns:\n        the collected Signal\n    \"\"\"\n    # unpack arguments\n    sig = session.signals[signal]\n    events_1 = session.epocs[event1]\n    events_2 = session.epocs[event2]\n    # in a rare case, a stray event2 before event1, this will prune those\n    # not sure if it's the best idea to do this............\n    events_2 = events_2[events_2 &gt; events_1.min()]\n    # assert len(events_1) == len(events_2)\n\n    # calculate index offsets etc\n    pre_idxs = int(np.rint(pre * sig.fs))\n    inter_idxs = int(np.rint(inter * sig.fs))\n    post_idxs = int(np.rint(post * sig.fs))\n    n_samples = pre_idxs + inter_idxs + post_idxs\n    new_time = fs2t(sig.fs, n_samples) - pre\n\n    # destination slices in the final signal\n    slice1 = slice(0, pre_idxs)\n    slice2 = slice(pre_idxs, pre_idxs + inter_idxs)\n    slice3 = slice(pre_idxs + inter_idxs, pre_idxs + inter_idxs + post_idxs)\n\n    accum = np.zeros_like(sig.signal, shape=(events_1.shape[0], n_samples))\n    padded_signal = np.pad(sig.signal, (pre_idxs, post_idxs), mode=\"constant\", constant_values=0)\n    for ei, (evt1, evt2) in enumerate(zip(events_1, events_2)):\n        event1_idx = sig.tindex(evt1)\n        event2_idx = sig.tindex(evt2)\n\n        # collect the first third (pre to event1)\n        accum[ei, slice1] = padded_signal[event1_idx : (event1_idx + pre_idxs)]\n\n        # collect the second third (event1 to event2)\n        # idea here is train a scipy.interpolate.interp1d() object with our real data\n        # and then resample `inter_idxs` number of points from `inter_time`\n        inter_time = sig.time[event1_idx:event2_idx]\n        inter_sig = padded_signal[(event1_idx + pre_idxs) : (event2_idx + pre_idxs)]\n        inter_source_intp = scipy.interpolate.interp1d(inter_time, inter_sig)\n        inter_time_query = np.linspace(inter_time[0], inter_time[-1], inter_idxs, endpoint=True)\n        accum[ei, slice2] = inter_source_intp(inter_time_query)\n\n        # collect the final third (event 2 to post)\n        accum[ei, slice3] = padded_signal[(event2_idx + pre_idxs) : (event2_idx + pre_idxs + post_idxs)]\n\n    # construct the new signal object, and copy over proper metadata and add marks\n    if out_name is None:\n        out_name = f\"{signal}@{event1}&gt;{event2}\"\n\n    s = Signal(out_name, accum, time=new_time, units=sig.units)\n    s.marks[event1] = 0\n    s.marks[event2] = inter\n\n    # return the collected signal\n    return s\n</code></pre>"},{"location":"reference/fptools/measure/snr/","title":"snr","text":""},{"location":"reference/fptools/measure/snr/#fptools.measure.snr","title":"<code>fptools.measure.snr</code>","text":"<p>Functions:</p> <ul> <li> <code>measure_snr_event</code>             \u2013              <p>Measure Signal to Noise ratio (SNR) in signals surrounding events.</p> </li> <li> <code>measure_snr_overall</code>             \u2013              <p>Measure Signal to Noise ratio (SNR) of the overall stream.</p> </li> </ul>"},{"location":"reference/fptools/measure/snr/#fptools.measure.snr.measure_snr_event","title":"<code>measure_snr_event(sessions, signals, events, noise_range, signal_range, include_meta='all')</code>","text":"<p>Measure Signal to Noise ratio (SNR) in signals surrounding events.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>Union[str, list[str]]</code>)           \u2013            <p>one or more signals to measure</p> </li> <li> <code>events</code>               (<code>Union[str, list[str]]</code>)           \u2013            <p>one or more events at which to measure signals</p> </li> <li> <code>noise_range</code>               (<code>Union[tuple[float, float], list[tuple[float, float]]]</code>)           \u2013            <p>tuple(s) of start/stop times relative to the event to consider as noise</p> </li> <li> <code>signal_range</code>               (<code>Union[tuple[float, float], list[tuple[float, float]]]</code>)           \u2013            <p>tuple(s) of start/stop times relative to the event to consider as signal</p> </li> <li> <code>include_meta</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>metadata fields to include in output. if \"all\" then all fields will be included</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pandas.DataFrame with collected SNR data</p> </li> </ul> Source code in <code>fptools/measure/snr.py</code> <pre><code>def measure_snr_event(\n    sessions: SessionCollection,\n    signals: Union[str, list[str]],\n    events: Union[str, list[str]],\n    noise_range: Union[tuple[float, float], list[tuple[float, float]]],\n    signal_range: Union[tuple[float, float], list[tuple[float, float]]],\n    include_meta: FieldList = \"all\",\n) -&gt; pd.DataFrame:\n    \"\"\"Measure Signal to Noise ratio (SNR) in signals surrounding events.\n\n    Args:\n        signals: one or more signals to measure\n        events: one or more events at which to measure signals\n        noise_range: tuple(s) of start/stop times relative to the event to consider as noise\n        signal_range: tuple(s) of start/stop times relative to the event to consider as signal\n        include_meta: metadata fields to include in output. if \"all\" then all fields will be included\n\n    Returns:\n        pandas.DataFrame with collected SNR data\n    \"\"\"\n    sigs_to_measure = []\n    if isinstance(signals, str):\n        sigs_to_measure.append(signals)\n    else:\n        sigs_to_measure.extend(signals)\n\n    events_to_measure = []\n    if isinstance(events, str):\n        events_to_measure.append(events)\n    else:\n        events_to_measure.extend(events)\n\n    nrs: list[tuple[float, float]] = []\n    if len(np.array(noise_range).shape) == 1:\n        nrs = [cast(tuple, noise_range)] * len(events_to_measure)\n    else:\n        nrs.extend(cast(list, noise_range))\n\n    srs: list[tuple[float, float]] = []\n    if len(np.array(signal_range).shape) == 1:\n        srs = [cast(tuple, signal_range)] * len(events_to_measure)\n    else:\n        srs.extend(cast(list, signal_range))\n\n    data = []\n    for session in sessions:\n        # determine metadata fields to include\n        if include_meta == \"all\":\n            meta = session.metadata\n        else:\n            meta = {k: v for k, v in session.metadata.items() if k in include_meta}\n\n        for sig_name in sigs_to_measure:\n            for ei, event_name in enumerate(events_to_measure):\n                n = collect_signals(session, event_name, sig_name, start=nrs[ei][0], stop=nrs[ei][1])\n                s = collect_signals(session, event_name, sig_name, start=srs[ei][0], stop=srs[ei][1])\n                data.append(\n                    {\n                        **meta,\n                        \"signal\": sig_name,\n                        \"snr\": np.median((s.signal.max(axis=1) - s.signal.min(axis=1)) ** 2 / n.signal.std(axis=1) ** 2),\n                    }\n                )\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"reference/fptools/measure/snr/#fptools.measure.snr.measure_snr_overall","title":"<code>measure_snr_overall(sessions, signals, include_meta='all')</code>","text":"<p>Measure Signal to Noise ratio (SNR) of the overall stream.</p> <p>SNR defined as log10(mean(signal)^2 / std(signal)^2), and expressed in decibels (dB)</p> <p>Parameters:</p> <ul> <li> <code>sessions</code>               (<code>SessionCollection</code>)           \u2013            <p>sessions to pull signals from</p> </li> <li> <code>signals</code>               (<code>Union[str, list[str]]</code>)           \u2013            <p>one or more signal names to operate on\"</p> </li> <li> <code>include_meta</code>               (<code>FieldList</code>, default:                   <code>'all'</code> )           \u2013            <p>metadata fields to include in the resulting dataframe. If \"all\", include all metadata fields</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pandas.DataFrame with calculated SNR</p> </li> </ul> Source code in <code>fptools/measure/snr.py</code> <pre><code>def measure_snr_overall(sessions: SessionCollection, signals: Union[str, list[str]], include_meta: FieldList = \"all\") -&gt; pd.DataFrame:\n    \"\"\"Measure Signal to Noise ratio (SNR) of the overall stream.\n\n    SNR defined as log10(mean(signal)^2 / std(signal)^2), and expressed in decibels (dB)\n\n    Args:\n        sessions: sessions to pull signals from\n        signals: one or more signal names to operate on\"\n        include_meta: metadata fields to include in the resulting dataframe. If \"all\", include all metadata fields\n\n    Returns:\n        pandas.DataFrame with calculated SNR\n    \"\"\"\n    sigs_to_measure = []\n    if isinstance(signals, str):\n        sigs_to_measure.append(signals)\n    else:\n        sigs_to_measure.extend(signals)\n\n    data = []\n    for session in sessions:\n        # determine metadata fields to include\n        if include_meta == \"all\":\n            meta = session.metadata\n        else:\n            meta = {k: v for k, v in session.metadata.items() if k in include_meta}\n\n        for sig_name in sigs_to_measure:\n            sig = session.signals[sig_name]\n            data.append(\n                {\n                    **meta,\n                    \"signal\": sig_name,\n                    \"snr\": np.log10(np.power(np.mean(sig.signal), 2) / np.power(np.std(sig.signal), 2)),\n                }\n            )\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"reference/fptools/preprocess/","title":"preprocess","text":""},{"location":"reference/fptools/preprocess/#fptools.preprocess","title":"<code>fptools.preprocess</code>","text":"<p>Modules:</p> <ul> <li> <code>common</code>           \u2013            </li> <li> <code>lib</code>           \u2013            </li> <li> <code>pipelines</code>           \u2013            </li> <li> <code>steps</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/preprocess/common/","title":"common","text":""},{"location":"reference/fptools/preprocess/common/#fptools.preprocess.common","title":"<code>fptools.preprocess.common</code>","text":"<p>Classes:</p> <ul> <li> <code>Pipeline</code>           \u2013            </li> <li> <code>Preprocessor</code>           \u2013            <p>Abstract Preprocessor.</p> </li> <li> <code>PreprocessorStep</code>           \u2013            <p>Abstract Preprocessor.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/common/#fptools.preprocess.common.Pipeline","title":"<code>Pipeline</code>","text":"<p>               Bases: <code>Preprocessor</code></p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Run this pipeline on a Session.</p> </li> <li> <code>__init__</code>             \u2013              <p>Initialize this pipeline.</p> </li> </ul> Source code in <code>fptools/preprocess/common.py</code> <pre><code>class Pipeline(Preprocessor):\n    def __init__(self, steps: Optional[list[Preprocessor]] = None, plot: bool = True, plot_dir: Optional[str] = None):\n        \"\"\"Initialize this pipeline.\n\n        Args:\n            steps: list of preprocessors to run on a given Session\n            plot: whether to plot the results of each step\n            plot_dir: directory to save plots to, if None, will save to current working directory\n        \"\"\"\n        self.steps: list[Preprocessor]\n        if steps is None:\n            self.steps = []\n        else:\n            self.steps = steps\n\n        self.plot: bool = plot\n        self.plot_dir: str = plot_dir or os.getcwd()\n\n    def __call__(self, session: Session) -&gt; Session:\n        \"\"\"Run this pipeline on a Session.\n\n        Args:\n            session: the session to operate on\n\n        Returns:\n            Session with Preprocessors applied\n        \"\"\"\n        try:\n            if self.plot:\n                # allocate a plot:\n                # - each step receives one row to plot on\n                fig, axs = plt.subplots(len(self.steps), 1, figsize=(24, 6 * len(self.steps)))\n\n            for i, step in enumerate(self.steps):\n                session = step(session)\n                if self.plot:\n                    if hasattr(step, \"plot\"):\n                        step.plot(session, axs[i])\n                    else:\n                        if hasattr(step, \"__class__\"):\n                            step_name = step.__class__.__name__  # this handles the case where the step is a class\n                        elif hasattr(step, \"__name__\"):\n                            step_name = step.__name__  # this handles the case where the step is a function\n                        elif hasattr(step, \"func\"):\n                            step_name = step.func.__name__  # this handles the case where the step is a partial\n                        else:\n                            step_name = \"Unknown\"  # this handles the case where we cannot figure out a reasonable name for the step\n\n                        message = f'Step #{i+1} \"{step_name}\" has no plot method, skipping plotting for this step.'\n                        axs[i].text(0.5, 0.5, message, ha=\"center\", va=\"center\", transform=axs[i].transAxes)\n                        axs[i].axis(\"off\")\n\n            return session\n        except:\n            raise\n        finally:\n            if self.plot:\n                fig.savefig(os.path.join(self.plot_dir, f\"{session.name}.png\"), dpi=300)\n                fig.savefig(os.path.join(self.plot_dir, f\"{session.name}.pdf\"))\n                plt.close(fig)\n</code></pre>"},{"location":"reference/fptools/preprocess/common/#fptools.preprocess.common.Pipeline.__call__","title":"<code>__call__(session)</code>","text":"<p>Run this pipeline on a Session.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to operate on</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session with Preprocessors applied</p> </li> </ul> Source code in <code>fptools/preprocess/common.py</code> <pre><code>def __call__(self, session: Session) -&gt; Session:\n    \"\"\"Run this pipeline on a Session.\n\n    Args:\n        session: the session to operate on\n\n    Returns:\n        Session with Preprocessors applied\n    \"\"\"\n    try:\n        if self.plot:\n            # allocate a plot:\n            # - each step receives one row to plot on\n            fig, axs = plt.subplots(len(self.steps), 1, figsize=(24, 6 * len(self.steps)))\n\n        for i, step in enumerate(self.steps):\n            session = step(session)\n            if self.plot:\n                if hasattr(step, \"plot\"):\n                    step.plot(session, axs[i])\n                else:\n                    if hasattr(step, \"__class__\"):\n                        step_name = step.__class__.__name__  # this handles the case where the step is a class\n                    elif hasattr(step, \"__name__\"):\n                        step_name = step.__name__  # this handles the case where the step is a function\n                    elif hasattr(step, \"func\"):\n                        step_name = step.func.__name__  # this handles the case where the step is a partial\n                    else:\n                        step_name = \"Unknown\"  # this handles the case where we cannot figure out a reasonable name for the step\n\n                    message = f'Step #{i+1} \"{step_name}\" has no plot method, skipping plotting for this step.'\n                    axs[i].text(0.5, 0.5, message, ha=\"center\", va=\"center\", transform=axs[i].transAxes)\n                    axs[i].axis(\"off\")\n\n        return session\n    except:\n        raise\n    finally:\n        if self.plot:\n            fig.savefig(os.path.join(self.plot_dir, f\"{session.name}.png\"), dpi=300)\n            fig.savefig(os.path.join(self.plot_dir, f\"{session.name}.pdf\"))\n            plt.close(fig)\n</code></pre>"},{"location":"reference/fptools/preprocess/common/#fptools.preprocess.common.Pipeline.__init__","title":"<code>__init__(steps=None, plot=True, plot_dir=None)</code>","text":"<p>Initialize this pipeline.</p> <p>Parameters:</p> <ul> <li> <code>steps</code>               (<code>Optional[list[Preprocessor]]</code>, default:                   <code>None</code> )           \u2013            <p>list of preprocessors to run on a given Session</p> </li> <li> <code>plot</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to plot the results of each step</p> </li> <li> <code>plot_dir</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>directory to save plots to, if None, will save to current working directory</p> </li> </ul> Source code in <code>fptools/preprocess/common.py</code> <pre><code>def __init__(self, steps: Optional[list[Preprocessor]] = None, plot: bool = True, plot_dir: Optional[str] = None):\n    \"\"\"Initialize this pipeline.\n\n    Args:\n        steps: list of preprocessors to run on a given Session\n        plot: whether to plot the results of each step\n        plot_dir: directory to save plots to, if None, will save to current working directory\n    \"\"\"\n    self.steps: list[Preprocessor]\n    if steps is None:\n        self.steps = []\n    else:\n        self.steps = steps\n\n    self.plot: bool = plot\n    self.plot_dir: str = plot_dir or os.getcwd()\n</code></pre>"},{"location":"reference/fptools/preprocess/common/#fptools.preprocess.common.Preprocessor","title":"<code>Preprocessor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract Preprocessor.</p> <p>Implementors should implement the <code>__call__</code> method.</p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Effect this preprocessing step.</p> </li> </ul> Source code in <code>fptools/preprocess/common.py</code> <pre><code>class Preprocessor(ABC):\n    \"\"\"Abstract Preprocessor.\n\n    Implementors should implement the `__call__` method.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, session: Session) -&gt; Session:\n        \"\"\"Effect this preprocessing step.\n\n        Args:\n            session: the session to operate upon\n\n        Returns:\n            Session with the preprocessing step applied\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/fptools/preprocess/common/#fptools.preprocess.common.Preprocessor.__call__","title":"<code>__call__(session)</code>  <code>abstractmethod</code>","text":"<p>Effect this preprocessing step.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to operate upon</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session with the preprocessing step applied</p> </li> </ul> Source code in <code>fptools/preprocess/common.py</code> <pre><code>@abstractmethod\ndef __call__(self, session: Session) -&gt; Session:\n    \"\"\"Effect this preprocessing step.\n\n    Args:\n        session: the session to operate upon\n\n    Returns:\n        Session with the preprocessing step applied\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/fptools/preprocess/common/#fptools.preprocess.common.PreprocessorStep","title":"<code>PreprocessorStep</code>","text":"<p>               Bases: <code>Preprocessor</code></p> <p>Abstract Preprocessor.</p> <p>Implementors should implement the <code>__call__</code> and <code>plot</code> methods.</p> <p>Methods:</p> <ul> <li> <code>plot</code>             \u2013              <p>Plot the effects of this preprocessing step.</p> </li> </ul> Source code in <code>fptools/preprocess/common.py</code> <pre><code>class PreprocessorStep(Preprocessor):\n    \"\"\"Abstract Preprocessor.\n\n    Implementors should implement the `__call__` and `plot` methods.\n    \"\"\"\n\n    @abstractmethod\n    def plot(self, session: Session, ax: Axes) -&gt; None:\n        \"\"\"Plot the effects of this preprocessing step.\n\n        Args:\n            session: the session being operated upon\n            ax: matplotlib Axes for plotting onto\n        \"\"\"\n        raise NotImplementedError()\n\n    def _resolve_signal_names(self, session: Session, signals: SignalList) -&gt; list[str]:\n        \"\"\"Resolve signal names, including special monikers, i.e. \"all\".\"\"\"\n        if signals == \"all\":\n            return list(session.signals.keys())\n        else:\n            return [s for s in signals if s in session.signals.keys()]\n</code></pre>"},{"location":"reference/fptools/preprocess/common/#fptools.preprocess.common.PreprocessorStep._resolve_signal_names","title":"<code>_resolve_signal_names(session, signals)</code>","text":"<p>Resolve signal names, including special monikers, i.e. \"all\".</p> Source code in <code>fptools/preprocess/common.py</code> <pre><code>def _resolve_signal_names(self, session: Session, signals: SignalList) -&gt; list[str]:\n    \"\"\"Resolve signal names, including special monikers, i.e. \"all\".\"\"\"\n    if signals == \"all\":\n        return list(session.signals.keys())\n    else:\n        return [s for s in signals if s in session.signals.keys()]\n</code></pre>"},{"location":"reference/fptools/preprocess/common/#fptools.preprocess.common.PreprocessorStep.plot","title":"<code>plot(session, ax)</code>  <code>abstractmethod</code>","text":"<p>Plot the effects of this preprocessing step.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session being operated upon</p> </li> <li> <code>ax</code>               (<code>Axes</code>)           \u2013            <p>matplotlib Axes for plotting onto</p> </li> </ul> Source code in <code>fptools/preprocess/common.py</code> <pre><code>@abstractmethod\ndef plot(self, session: Session, ax: Axes) -&gt; None:\n    \"\"\"Plot the effects of this preprocessing step.\n\n    Args:\n        session: the session being operated upon\n        ax: matplotlib Axes for plotting onto\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/","title":"lib","text":""},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib","title":"<code>fptools.preprocess.lib</code>","text":"<p>Functions:</p> <ul> <li> <code>are_arrays_same_length</code>             \u2013              <p>Check if all arrays are the same shape in the first axis.</p> </li> <li> <code>detrend_double_exponential</code>             \u2013              <p>Detrend a signal by fitting and subtracting a double exponential curve to the data.</p> </li> <li> <code>double_exponential</code>             \u2013              <p>Compute a double exponential function with constant offset.</p> </li> <li> <code>downsample</code>             \u2013              <p>Downsample one or more signals by factor across windows of size <code>window</code>.</p> </li> <li> <code>estimate_motion</code>             \u2013              <p>Estimate the contribution of motion artifacts in <code>signal</code>.</p> </li> <li> <code>fit_double_exponential</code>             \u2013              <p>Run the fitting procedure for a double exponential curve.</p> </li> <li> <code>fs2t</code>             \u2013              <p>Generate a time array given a sample frequency and number of samples.</p> </li> <li> <code>lowpass_filter</code>             \u2013              <p>zero-phase lowpass filter a signal.</p> </li> <li> <code>t2fs</code>             \u2013              <p>Estimate the sample frequency given a time array.</p> </li> <li> <code>trim</code>             \u2013              <p>Trim samples from the beginning or end of a signal.</p> </li> <li> <code>zscore_signals</code>             \u2013              <p>Z-score one or more signals.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.are_arrays_same_length","title":"<code>are_arrays_same_length(*arrays)</code>","text":"<p>Check if all arrays are the same shape in the first axis.</p> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def are_arrays_same_length(*arrays: np.ndarray) -&gt; bool:\n    \"\"\"Check if all arrays are the same shape in the first axis.\"\"\"\n    lengths = [arr.shape[0] for arr in arrays]\n    return bool(np.all(np.array(lengths) == lengths[0]))\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.detrend_double_exponential","title":"<code>detrend_double_exponential(time, signal)</code>","text":"<p>Detrend a signal by fitting and subtracting a double exponential curve to the data.</p> <p>See <code>double_exponential</code> for the underlying curve design, and <code>fit_double_exponential</code> for the curve fitting procedure.</p> <p>Parameters:</p> <ul> <li> <code>time</code>               (<code>ndarray</code>)           \u2013            <p>array of sample/observation times</p> </li> <li> <code>signal</code>               (<code>ndarray</code>)           \u2013            <p>array of samples</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ndarray]</code>           \u2013            <p>Tuple of (detrended_signal, signal_fit)</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def detrend_double_exponential(time: np.ndarray, signal: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Detrend a signal by fitting and subtracting a double exponential curve to the data.\n\n    See `double_exponential` for the underlying curve design, and `fit_double_exponential` for the curve fitting procedure.\n\n    Args:\n        time: array of sample/observation times\n        signal: array of samples\n\n    Returns:\n        Tuple of (detrended_signal, signal_fit)\n    \"\"\"\n    signal_fit = fit_double_exponential(time, signal)\n    return signal - signal_fit, signal_fit\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.double_exponential","title":"<code>double_exponential(t, const, amp_fast, amp_slow, tau_slow, tau_multiplier)</code>","text":"<p>Compute a double exponential function with constant offset.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>ndarray</code>)           \u2013            <p>Time vector in seconds.</p> </li> <li> <code>const</code>               (<code>float</code>)           \u2013            <p>Amplitude of the constant offset.</p> </li> <li> <code>amp_fast</code>               (<code>float</code>)           \u2013            <p>Amplitude of the fast component.</p> </li> <li> <code>amp_slow</code>               (<code>float</code>)           \u2013            <p>Amplitude of the slow component.</p> </li> <li> <code>tau_slow</code>               (<code>float</code>)           \u2013            <p>Time constant of slow component in seconds.</p> </li> <li> <code>tau_multiplier</code>               (<code>float</code>)           \u2013            <p>Time constant of fast component relative to slow.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>dependent values evaluated over <code>t</code> given the remaining parameters.</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def double_exponential(t: np.ndarray, const: float, amp_fast: float, amp_slow: float, tau_slow: float, tau_multiplier: float) -&gt; np.ndarray:\n    \"\"\"Compute a double exponential function with constant offset.\n\n    Args:\n        t: Time vector in seconds.\n        const: Amplitude of the constant offset.\n        amp_fast: Amplitude of the fast component.\n        amp_slow: Amplitude of the slow component.\n        tau_slow: Time constant of slow component in seconds.\n        tau_multiplier: Time constant of fast component relative to slow.\n\n    Returns:\n        dependent values evaluated over `t` given the remaining parameters.\n    \"\"\"\n    tau_fast = tau_slow * tau_multiplier\n    return const + amp_slow * np.exp(-t / tau_slow) + amp_fast * np.exp(-t / tau_fast)\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.downsample","title":"<code>downsample(*signals, window=10, factor=10)</code>","text":"<p>Downsample one or more signals by factor across windows of size <code>window</code>.</p> <p>performs a moving window average using windows of size <code>window</code>, then takes every n-th observation as given by <code>factor</code>.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>ndarray</code>, default:                   <code>()</code> )           \u2013            <p>one or more signals to downsample</p> </li> <li> <code>window</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>size of the window used for averaging</p> </li> <li> <code>factor</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>step size for taking the final downsampled signal</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ...]</code>           \u2013            <p>downsampled signal(s)</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def downsample(*signals: np.ndarray, window: int = 10, factor: int = 10) -&gt; tuple[np.ndarray, ...]:\n    \"\"\"Downsample one or more signals by factor across windows of size `window`.\n\n    performs a moving window average using windows of size `window`, then takes every\n    n-th observation as given by `factor`.\n\n    Args:\n        signals: one or more signals to downsample\n        window: size of the window used for averaging\n        factor: step size for taking the final downsampled signal\n\n    Returns:\n        downsampled signal(s)\n    \"\"\"\n    # assert are_arrays_same_length(*signals)\n    return tuple(np.convolve(sig, np.ones(window) / window, mode=\"valid\")[::factor] for sig in signals)\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.estimate_motion","title":"<code>estimate_motion(signal, control)</code>","text":"<p>Estimate the contribution of motion artifacts in <code>signal</code>.</p> <p>Performs linear regression of control (x, independent) vs signal (y, dependent)</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>ndarray</code>)           \u2013            <p>the signal to correct for motion artifacts</p> </li> <li> <code>control</code>               (<code>ndarray</code>)           \u2013            <p>signal to use for background signal</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ndarray]</code>           \u2013            <p>tuple of (corrected_signal, estimated_motion)</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def estimate_motion(signal: np.ndarray, control: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Estimate the contribution of motion artifacts in `signal`.\n\n    Performs linear regression of control (x, independent) vs signal (y, dependent)\n\n    Args:\n        signal: the signal to correct for motion artifacts\n        control: signal to use for background signal\n\n    Returns:\n        tuple of (corrected_signal, estimated_motion)\n    \"\"\"\n    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x=control, y=signal)\n    est_motion = intercept + slope * (control)\n    return signal - est_motion, est_motion\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.fit_double_exponential","title":"<code>fit_double_exponential(time, signal)</code>","text":"<p>Run the fitting procedure for a double exponential curve.</p> <p>Parameters:</p> <ul> <li> <code>time</code>               (<code>ndarray</code>)           \u2013            <p>array of sample times</p> </li> <li> <code>signal</code>               (<code>ndarray</code>)           \u2013            <p>array of sample values</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>array of values from the fitted double exponential curve, samples at the times in <code>time</code>.</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def fit_double_exponential(time: np.ndarray, signal: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Run the fitting procedure for a double exponential curve.\n\n    Args:\n        time: array of sample times\n        signal: array of sample values\n\n    Returns:\n        array of values from the fitted double exponential curve, samples at the times in `time`.\n    \"\"\"\n    max_sig = np.max(signal)\n    initial_params = [max_sig / 2, max_sig / 4, max_sig / 4, 3600, 0.1]\n    bounds = ([0, 0, 0, 600, 0], [max_sig, max_sig, max_sig, 36000, 1])\n    parm_opt, parm_cov = scipy.optimize.curve_fit(double_exponential, time, signal, p0=initial_params, bounds=bounds, maxfev=1000)\n    return double_exponential(time, *parm_opt)\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.fs2t","title":"<code>fs2t(fs, length)</code>","text":"<p>Generate a time array given a sample frequency and number of samples.</p> <p>Parameters:</p> <ul> <li> <code>fs</code>               (<code>float</code>)           \u2013            <p>sampling rate, in Hz</p> </li> <li> <code>length</code>               (<code>int</code>)           \u2013            <p>number of samples to generate</p> </li> </ul> <p>Returns     array of time values, in seconds</p> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def fs2t(fs: float, length: int) -&gt; np.ndarray:\n    \"\"\"Generate a time array given a sample frequency and number of samples.\n\n    Args:\n        fs: sampling rate, in Hz\n        length: number of samples to generate\n\n    Returns\n        array of time values, in seconds\n    \"\"\"\n    return np.linspace(1, length, length) / fs\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.lowpass_filter","title":"<code>lowpass_filter(signal, fs, Wn=10)</code>","text":"<p>zero-phase lowpass filter a signal.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>ndarray</code>)           \u2013            <p>array to be filtered</p> </li> <li> <code>fs</code>               (<code>float</code>)           \u2013            <p>sampling frequency of the signal, in Hz</p> </li> <li> <code>Wn</code>               (<code>float</code>, default:                   <code>10</code> )           \u2013            <p>critical frequency, see <code>scipy.signal.butter()</code></p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>lowpass filtered signal</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def lowpass_filter(signal: np.ndarray, fs: float, Wn: float = 10) -&gt; np.ndarray:\n    \"\"\"zero-phase lowpass filter a signal.\n\n    Args:\n        signal: array to be filtered\n        fs: sampling frequency of the signal, in Hz\n        Wn: critical frequency, see `scipy.signal.butter()`\n\n    Returns:\n        lowpass filtered signal\n    \"\"\"\n    b, a = scipy.signal.butter(2, Wn, btype=\"lowpass\", fs=fs)\n    return scipy.signal.filtfilt(b, a, signal)\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.t2fs","title":"<code>t2fs(time)</code>","text":"<p>Estimate the sample frequency given a time array.</p> <p>Parameters:</p> <ul> <li> <code>time</code>               (<code>ndarray</code>)           \u2013            <p>array of time values, in seconds, from which to estimate the sample frequency</p> </li> </ul> <p>Returns     The estimated sample frequency, in Hz</p> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def t2fs(time: np.ndarray) -&gt; float:\n    \"\"\"Estimate the sample frequency given a time array.\n\n    Args:\n        time: array of time values, in seconds, from which to estimate the sample frequency\n\n    Returns\n        The estimated sample frequency, in Hz\n    \"\"\"\n    return 1 / np.median(np.diff(time))\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.trim","title":"<code>trim(*signals, begin=None, end=None)</code>","text":"<p>Trim samples from the beginning or end of a signal.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>ndarray</code>, default:                   <code>()</code> )           \u2013            <p>one or more signals to be trimmed</p> </li> <li> <code>begin</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>number of samples to trim from the beginning</p> </li> <li> <code>end</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>number of samples to trim from the end</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ...]</code>           \u2013            <p>tuple of trimmed signals</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def trim(*signals: np.ndarray, begin: Optional[int] = None, end: Optional[int] = None) -&gt; tuple[np.ndarray, ...]:\n    \"\"\"Trim samples from the beginning or end of a signal.\n\n    Args:\n        signals: one or more signals to be trimmed\n        begin: number of samples to trim from the beginning\n        end: number of samples to trim from the end\n\n    Returns:\n        tuple of trimmed signals\n    \"\"\"\n    assert are_arrays_same_length(*signals)\n    if begin is None:\n        begin = 0\n    if end is None:\n        end = signals[0].shape[0]\n    return tuple(sig[begin:end] for sig in signals)\n</code></pre>"},{"location":"reference/fptools/preprocess/lib/#fptools.preprocess.lib.zscore_signals","title":"<code>zscore_signals(*signals)</code>","text":"<p>Z-score one or more signals.</p> <p>see: scipy.stats.zscore()</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>ndarray</code>, default:                   <code>()</code> )           \u2013            <p>one or more signals to be z-scores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ...]</code>           \u2013            <p>tuple of z-scored signals</p> </li> </ul> Source code in <code>fptools/preprocess/lib.py</code> <pre><code>def zscore_signals(*signals: np.ndarray) -&gt; tuple[np.ndarray, ...]:\n    \"\"\"Z-score one or more signals.\n\n    see: scipy.stats.zscore()\n\n    Args:\n        signals: one or more signals to be z-scores\n\n    Returns:\n        tuple of z-scored signals\n    \"\"\"\n    return tuple(scipy.stats.zscore(sig) for sig in signals)\n</code></pre>"},{"location":"reference/fptools/preprocess/pipelines/","title":"pipelines","text":""},{"location":"reference/fptools/preprocess/pipelines/#fptools.preprocess.pipelines","title":"<code>fptools.preprocess.pipelines</code>","text":"<p>Modules:</p> <ul> <li> <code>dxp_motion_dff</code>           \u2013            </li> <li> <code>lowpass_dff</code>           \u2013            </li> <li> <code>tdt_default</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/preprocess/pipelines/dxp_motion_dff/","title":"dxp_motion_dff","text":""},{"location":"reference/fptools/preprocess/pipelines/dxp_motion_dff/#fptools.preprocess.pipelines.dxp_motion_dff","title":"<code>fptools.preprocess.pipelines.dxp_motion_dff</code>","text":"<p>Classes:</p> <ul> <li> <code>DxpMotionDffPipeline</code>           \u2013            <p>Preprocess using a double exponential fit for detrending, producing df/f values.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>dxp_motion_dff</code>             \u2013              <p>Preprocess using a double exponential fit for detrending, producing df/f values.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/pipelines/dxp_motion_dff/#fptools.preprocess.pipelines.dxp_motion_dff.DxpMotionDffPipeline","title":"<code>DxpMotionDffPipeline</code>","text":"<p>               Bases: <code>Pipeline</code></p> <p>Preprocess using a double exponential fit for detrending, producing df/f values.</p> <p>Implemented as described in: TODO: add citation</p> <p>Pipeline steps: 1) Signals are optionally trimmed to the optical system start. 2) Signals are fit with a double exponential to detrend 3) Signals are motion corrected using a control signal 4) Signals are converted to dF/F 5) Signals are optionally downsampled factor <code>downsample</code></p> <p>Methods:</p> <ul> <li> <code>__init__</code>             \u2013              <p>Initialize this pipeline.</p> </li> </ul> Source code in <code>fptools/preprocess/pipelines/dxp_motion_dff.py</code> <pre><code>class DxpMotionDffPipeline(Pipeline):\n    \"\"\"Preprocess using a double exponential fit for detrending, producing df/f values.\n\n    Implemented as described in:\n    TODO: add citation\n\n    Pipeline steps:\n    1) Signals are optionally trimmed to the optical system start.\n    2) Signals are fit with a double exponential to detrend\n    3) Signals are motion corrected using a control signal\n    4) Signals are converted to dF/F\n    5) Signals are optionally downsampled factor `downsample`\n    \"\"\"\n\n    def __init__(\n        self,\n        signals: PairedSignalList,\n        trim_extent: Union[None, Literal[\"auto\"], float, tuple[float, float]] = \"auto\",\n        downsample: Optional[int] = 10,\n        plot: bool = True,\n        plot_dir: Optional[str] = None,\n    ):\n        \"\"\"Initialize this pipeline.\n\n        Args:\n            signals: list of signal names to be processed\n            trim_extent: specification for trimming. None disables trimming, auto uses the offset stored in `block.scalars.Fi1i.ts`, a single float trims that amount of time (in seconds) from the beginning, a tuple of two floats specifies the amount of time (in seconds) from the beginning and end to trim, respectively.\n            downsample: if not `None`, downsample signal by `downsample` factor.\n            plot: whether to plot the results of each step\n            plot_dir: directory to save plots to\n        \"\"\"\n        steps: list[Preprocessor] = [\n            TrimSignals(_flatten_paired_signals(signals), extent=trim_extent),\n            DblExpFit(_flatten_paired_signals(signals)),\n            MotionCorrect(signals),\n            Dff([(s, f\"{s}_dxp\") for s in signals]),\n        ]\n        if downsample is not None:\n            steps.append(Downsample(_flatten_paired_signals(signals), window=downsample, factor=downsample))\n\n        super().__init__(steps=steps, plot=plot, plot_dir=plot_dir)\n</code></pre>"},{"location":"reference/fptools/preprocess/pipelines/dxp_motion_dff/#fptools.preprocess.pipelines.dxp_motion_dff.DxpMotionDffPipeline.__init__","title":"<code>__init__(signals, trim_extent='auto', downsample=10, plot=True, plot_dir=None)</code>","text":"<p>Initialize this pipeline.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>PairedSignalList</code>)           \u2013            <p>list of signal names to be processed</p> </li> <li> <code>trim_extent</code>               (<code>Union[None, Literal['auto'], float, tuple[float, float]]</code>, default:                   <code>'auto'</code> )           \u2013            <p>specification for trimming. None disables trimming, auto uses the offset stored in <code>block.scalars.Fi1i.ts</code>, a single float trims that amount of time (in seconds) from the beginning, a tuple of two floats specifies the amount of time (in seconds) from the beginning and end to trim, respectively.</p> </li> <li> <code>downsample</code>               (<code>Optional[int]</code>, default:                   <code>10</code> )           \u2013            <p>if not <code>None</code>, downsample signal by <code>downsample</code> factor.</p> </li> <li> <code>plot</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to plot the results of each step</p> </li> <li> <code>plot_dir</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>directory to save plots to</p> </li> </ul> Source code in <code>fptools/preprocess/pipelines/dxp_motion_dff.py</code> <pre><code>def __init__(\n    self,\n    signals: PairedSignalList,\n    trim_extent: Union[None, Literal[\"auto\"], float, tuple[float, float]] = \"auto\",\n    downsample: Optional[int] = 10,\n    plot: bool = True,\n    plot_dir: Optional[str] = None,\n):\n    \"\"\"Initialize this pipeline.\n\n    Args:\n        signals: list of signal names to be processed\n        trim_extent: specification for trimming. None disables trimming, auto uses the offset stored in `block.scalars.Fi1i.ts`, a single float trims that amount of time (in seconds) from the beginning, a tuple of two floats specifies the amount of time (in seconds) from the beginning and end to trim, respectively.\n        downsample: if not `None`, downsample signal by `downsample` factor.\n        plot: whether to plot the results of each step\n        plot_dir: directory to save plots to\n    \"\"\"\n    steps: list[Preprocessor] = [\n        TrimSignals(_flatten_paired_signals(signals), extent=trim_extent),\n        DblExpFit(_flatten_paired_signals(signals)),\n        MotionCorrect(signals),\n        Dff([(s, f\"{s}_dxp\") for s in signals]),\n    ]\n    if downsample is not None:\n        steps.append(Downsample(_flatten_paired_signals(signals), window=downsample, factor=downsample))\n\n    super().__init__(steps=steps, plot=plot, plot_dir=plot_dir)\n</code></pre>"},{"location":"reference/fptools/preprocess/pipelines/dxp_motion_dff/#fptools.preprocess.pipelines.dxp_motion_dff.dxp_motion_dff","title":"<code>dxp_motion_dff(session, signal_map, show_steps=True, plot_dir='', trim_extent='auto')</code>","text":"<p>Preprocess using a double exponential fit for detrending, producing df/f values.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to populate.</p> </li> <li> <code>signal_map</code>               (<code>list[SignalMapping]</code>)           \u2013            <p>mapping of signals to perform</p> </li> <li> <code>show_steps</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if <code>True</code>, produce diagnostic plots of the preprocessing steps.</p> </li> <li> <code>plot_dir</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>path where diagnostic plots of the preprocessing steps should be saved.</p> </li> <li> <code>trim_extent</code>               (<code>Union[None, Literal['auto'], float, tuple[float, float]]</code>, default:                   <code>'auto'</code> )           \u2013            <p>specification for trimming. None disables trimming, auto uses the offset stored in <code>block.scalars.Fi1i.ts</code>, a single float trims that amount of time (in seconds) from the beginning, a tuple of two floats specifies the amount of time (in seconds) from the beginning and end to trim, respectively.</p> </li> </ul> Source code in <code>fptools/preprocess/pipelines/dxp_motion_dff.py</code> <pre><code>def dxp_motion_dff(\n    session: Session,\n    signal_map: list[SignalMapping],\n    show_steps: bool = True,\n    plot_dir: str = \"\",\n    trim_extent: Union[None, Literal[\"auto\"], float, tuple[float, float]] = \"auto\",\n):\n    \"\"\"Preprocess using a double exponential fit for detrending, producing df/f values.\n\n    Args:\n        session: the session to populate.\n        signal_map: mapping of signals to perform\n        show_steps: if `True`, produce diagnostic plots of the preprocessing steps.\n        plot_dir: path where diagnostic plots of the preprocessing steps should be saved.\n        trim_extent: specification for trimming. None disables trimming, auto uses the offset stored in `block.scalars.Fi1i.ts`, a single float trims that amount of time (in seconds) from the beginning, a tuple of two floats specifies the amount of time (in seconds) from the beginning and end to trim, respectively.\n    \"\"\"\n    try:\n        if show_steps:\n            fig, axs = plt.subplots(6, 1, figsize=(24, 6 * 6))\n            palette = sns.color_palette(\"colorblind\", n_colors=len(signal_map))\n\n        signals: list[Signal] = []\n        for sm in signal_map:\n            signals.append(session.signals[sm[\"tdt_name\"]].copy(new_name=sm[\"dest_name\"]))\n\n        if show_steps:\n            for i, sig in enumerate(signals):\n                axs[0].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n            axs[0].set_title(\"Raw signal\")\n            axs[0].legend()\n\n        # trim raw signal start to when the optical system came online\n        if trim_extent is not None:\n            for sig in signals:\n                if trim_extent == \"auto\":\n                    trim_args = {\"begin\": int(session.scalars[\"Fi1i\"][0] * sig.fs)}\n                elif isinstance(trim_extent, float):\n                    trim_args = {\"begin\": int(trim_extent * sig.fs)}\n                elif len(trim_extent) == 2:\n                    trim_args = {\"begin\": int(trim_extent[0] * sig.fs), \"end\": int(trim_extent[1] * sig.fs)}\n\n                sig.signal, sig.time = trim(sig.signal, sig.time, **trim_args)\n\n            if show_steps:\n                for i, sig in enumerate(signals):\n                    axs[1].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n                axs[1].set_title(\"Trimmed Raw signal\")\n                axs[1].legend()\n        else:\n            axs[1].set_title(\"Trimming Disabled\")\n\n        # detrend using a double exponential fit\n        fits: list[np.ndarray] = []\n        for sig in signals:\n            detrended_sig, fit = detrend_double_exponential(sig.time, sig.signal)\n            fits.append(fit)\n\n            if show_steps:\n                axs[2].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n                axs[2].plot(sig.time, fit, label=f\"{sig.name} dExp Fit\", c=sns.desaturate(palette[i], 0.3))\n\n            sig.signal = detrended_sig\n\n        if show_steps:\n            axs[2].set_title(\"Double Exponential Fit\")\n            axs[2].legend()\n\n            for i, sig in enumerate(signals):\n                axs[3].plot(sig.time, detrended_sig, label=sig.name, c=palette[i])\n            axs[3].set_title(\"De-trended signals\")\n            axs[3].legend()\n\n        # correct for motion artifacts\n        ctrl_idx = next((i for i, sm in enumerate(signal_map) if sm[\"role\"] == \"control\"), None)\n        if ctrl_idx is None:\n            raise ValueError('To use motion correction, at least one signal must be marked with `role`=\"Control\" in the `signal_map`!')\n        for sm, sig in zip(signal_map, signals):\n            if sm[\"role\"] == \"experimental\":\n                motion_corrected, est_motion = estimate_motion(sig.signal, signals[ctrl_idx].signal)\n\n                if show_steps:\n                    axs[4].plot(sig.time, motion_corrected, label=sig.name, c=palette[i])\n                    axs[4].plot(sig.time, est_motion, label=f\"{sig.name} Est. Motion\", c=sns.desaturate(palette[i], 0.3))\n\n        if show_steps:\n            axs[4].set_title(\"Motion Correction\")\n            axs[4].legend()\n\n        # calculate dF/F\n        for sig, fit in zip(signals, fits):\n            sig.signal = (sig.signal / fit) * 100\n            sig.units = \"\u0394F/F\"\n\n        if show_steps:\n            for i, sig in enumerate(signals):\n                axs[5].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n            axs[5].set_title(\"Normalized\")\n            axs[5].legend()\n\n        # construct Signals and add to the Session\n        for sig in signals:\n            session.add_signal(sig)\n\n        return session\n    except:\n        raise\n    finally:\n        if show_steps:\n            fig.savefig(os.path.join(plot_dir, f\"{session.name}.png\"), dpi=600)\n            fig.savefig(os.path.join(plot_dir, f\"{session.name}.pdf\"))\n            plt.close(fig)\n</code></pre>"},{"location":"reference/fptools/preprocess/pipelines/lowpass_dff/","title":"lowpass_dff","text":""},{"location":"reference/fptools/preprocess/pipelines/lowpass_dff/#fptools.preprocess.pipelines.lowpass_dff","title":"<code>fptools.preprocess.pipelines.lowpass_dff</code>","text":"<p>Classes:</p> <ul> <li> <code>LowpassDFFPipeline</code>           \u2013            <p>A \"simple\" preprocess pipeline based on ultra-lowpass filtering.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/pipelines/lowpass_dff/#fptools.preprocess.pipelines.lowpass_dff.LowpassDFFPipeline","title":"<code>LowpassDFFPipeline</code>","text":"<p>               Bases: <code>Pipeline</code></p> <p>A \"simple\" preprocess pipeline based on ultra-lowpass filtering.</p> <p>Implemented as described in: Cai, Kaeser, et al. Dopamine dynamics are dispensable for movement but promote reward responses. Nature, 2024. https://doi.org/10.1038/s41586-024-08038-z</p> <p>Pipeline steps: 1) Signals are optionally trimmed to the optical system start. 2) Signals are lowpass filtered at 0.01 Hz (~100 second timescale) 3) Signals are converted to dF/F using lowpass filtered signals as F0 4) Signals are optionally downsampled factor <code>downsample</code></p> <p>Methods:</p> <ul> <li> <code>__init__</code>             \u2013              <p>Initialize this pipeline.</p> </li> </ul> Source code in <code>fptools/preprocess/pipelines/lowpass_dff.py</code> <pre><code>class LowpassDFFPipeline(Pipeline):\n    \"\"\"A \"simple\" preprocess pipeline based on ultra-lowpass filtering.\n\n    Implemented as described in:\n    Cai, Kaeser, et al. Dopamine dynamics are dispensable for movement but promote reward responses.\n    Nature, 2024. https://doi.org/10.1038/s41586-024-08038-z\n\n    Pipeline steps:\n    1) Signals are optionally trimmed to the optical system start.\n    2) Signals are lowpass filtered at 0.01 Hz (~100 second timescale)\n    3) Signals are converted to dF/F using lowpass filtered signals as F0\n    4) Signals are optionally downsampled factor `downsample`\n    \"\"\"\n\n    def __init__(\n        self,\n        signals: SignalList,\n        rename_map: Optional[dict[Literal[\"signals\", \"epocs\", \"scalars\"], dict[str, str]]] = None,\n        trim_extent: Union[None, Literal[\"auto\"], float, tuple[float, float]] = \"auto\",\n        downsample: Optional[int] = 10,\n        plot: bool = True,\n        plot_dir: Optional[str] = None,\n    ):\n        \"\"\"Initialize this pipeline.\n\n        Args:\n            signals: list of signal names to be processed\n            rename_map: dictionary of signal, epoc, and scalar names to be renamed\n            trim_extent: specification for trimming. None disables trimming, auto uses the offset stored in `block.scalars.Fi1i.ts`, a single float trims that amount of time (in seconds) from the beginning, a tuple of two floats specifies the amount of time (in seconds) from the beginning and end to trim, respectively.\n            downsample: if not `None`, downsample signal by `downsample` factor.\n            plot: whether to plot the results of each step\n            plot_dir: directory to save plots to\n        \"\"\"\n        steps: list[Preprocessor] = []\n        if rename_map is not None:\n            steps.append(\n                Rename(\n                    signals=rename_map.get(\"signals\", None), epocs=rename_map.get(\"epocs\", None), scalars=rename_map.get(\"scalars\", None)\n                )\n            )\n            signals = [rename_map[\"signals\"].get(s, s) for s in signals]  # remap the signals to the new names for the remaining steps\n\n        if trim_extent is not None:\n            steps.append(TrimSignals(signals, extent=trim_extent))\n\n        steps.append(Lowpass(signals))\n        steps.append(Dff([(s, f\"{s}_lowpass\") for s in signals]))\n\n        if downsample is not None:\n            steps.append(Downsample(signals, window=downsample, factor=downsample))\n\n        super().__init__(steps=steps, plot=plot, plot_dir=plot_dir)\n</code></pre>"},{"location":"reference/fptools/preprocess/pipelines/lowpass_dff/#fptools.preprocess.pipelines.lowpass_dff.LowpassDFFPipeline.__init__","title":"<code>__init__(signals, rename_map=None, trim_extent='auto', downsample=10, plot=True, plot_dir=None)</code>","text":"<p>Initialize this pipeline.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>SignalList</code>)           \u2013            <p>list of signal names to be processed</p> </li> <li> <code>rename_map</code>               (<code>Optional[dict[Literal['signals', 'epocs', 'scalars'], dict[str, str]]]</code>, default:                   <code>None</code> )           \u2013            <p>dictionary of signal, epoc, and scalar names to be renamed</p> </li> <li> <code>trim_extent</code>               (<code>Union[None, Literal['auto'], float, tuple[float, float]]</code>, default:                   <code>'auto'</code> )           \u2013            <p>specification for trimming. None disables trimming, auto uses the offset stored in <code>block.scalars.Fi1i.ts</code>, a single float trims that amount of time (in seconds) from the beginning, a tuple of two floats specifies the amount of time (in seconds) from the beginning and end to trim, respectively.</p> </li> <li> <code>downsample</code>               (<code>Optional[int]</code>, default:                   <code>10</code> )           \u2013            <p>if not <code>None</code>, downsample signal by <code>downsample</code> factor.</p> </li> <li> <code>plot</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to plot the results of each step</p> </li> <li> <code>plot_dir</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>directory to save plots to</p> </li> </ul> Source code in <code>fptools/preprocess/pipelines/lowpass_dff.py</code> <pre><code>def __init__(\n    self,\n    signals: SignalList,\n    rename_map: Optional[dict[Literal[\"signals\", \"epocs\", \"scalars\"], dict[str, str]]] = None,\n    trim_extent: Union[None, Literal[\"auto\"], float, tuple[float, float]] = \"auto\",\n    downsample: Optional[int] = 10,\n    plot: bool = True,\n    plot_dir: Optional[str] = None,\n):\n    \"\"\"Initialize this pipeline.\n\n    Args:\n        signals: list of signal names to be processed\n        rename_map: dictionary of signal, epoc, and scalar names to be renamed\n        trim_extent: specification for trimming. None disables trimming, auto uses the offset stored in `block.scalars.Fi1i.ts`, a single float trims that amount of time (in seconds) from the beginning, a tuple of two floats specifies the amount of time (in seconds) from the beginning and end to trim, respectively.\n        downsample: if not `None`, downsample signal by `downsample` factor.\n        plot: whether to plot the results of each step\n        plot_dir: directory to save plots to\n    \"\"\"\n    steps: list[Preprocessor] = []\n    if rename_map is not None:\n        steps.append(\n            Rename(\n                signals=rename_map.get(\"signals\", None), epocs=rename_map.get(\"epocs\", None), scalars=rename_map.get(\"scalars\", None)\n            )\n        )\n        signals = [rename_map[\"signals\"].get(s, s) for s in signals]  # remap the signals to the new names for the remaining steps\n\n    if trim_extent is not None:\n        steps.append(TrimSignals(signals, extent=trim_extent))\n\n    steps.append(Lowpass(signals))\n    steps.append(Dff([(s, f\"{s}_lowpass\") for s in signals]))\n\n    if downsample is not None:\n        steps.append(Downsample(signals, window=downsample, factor=downsample))\n\n    super().__init__(steps=steps, plot=plot, plot_dir=plot_dir)\n</code></pre>"},{"location":"reference/fptools/preprocess/pipelines/tdt_default/","title":"tdt_default","text":""},{"location":"reference/fptools/preprocess/pipelines/tdt_default/#fptools.preprocess.pipelines.tdt_default","title":"<code>fptools.preprocess.pipelines.tdt_default</code>","text":"<p>Functions:</p> <ul> <li> <code>tdt_default</code>             \u2013              <p>A preprocess pipeline based on TDT tutorials.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/pipelines/tdt_default/#fptools.preprocess.pipelines.tdt_default.tdt_default","title":"<code>tdt_default(session, signal_map, show_steps=True, plot_dir='', trim_extent='auto', downsample=None)</code>","text":"<p>A preprocess pipeline based on TDT tutorials.</p> <p>Pipeline steps: 1) Signals are trimmed to the optical system start. 2) Perform linear regression between sensor and isosbestic 3) calculate dFF</p>"},{"location":"reference/fptools/preprocess/pipelines/tdt_default/#fptools.preprocess.pipelines.tdt_default.tdt_default--4-signals-are-optionally-downsampled-factor-downsample","title":"4) Signals are optionally downsampled factor <code>downsample</code>","text":"<p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to populate.</p> </li> <li> <code>signal_map</code>               (<code>list[SignalMapping]</code>)           \u2013            <p>mapping of signals to perform</p> </li> <li> <code>show_steps</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if <code>True</code>, produce diagnostic plots of the preprocessing steps.</p> </li> <li> <code>plot_dir</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>path where diagnostic plots of the preprocessing steps should be saved.</p> </li> <li> <code>trim_extent</code>               (<code>Union[None, Literal['auto'], float, tuple[float, float]]</code>, default:                   <code>'auto'</code> )           \u2013            <p>specification for trimming. None disables trimming, auto uses the offset stored in <code>block.scalars.Fi1i.ts</code>, a single float trims that amount of time (in seconds) from the beginning, a tuple of two floats specifies the amount of time (in seconds) from the beginning and end to trim, respectively.</p> </li> <li> <code>downsample</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>if not <code>None</code>, downsample signal by <code>downsample</code> factor.</p> </li> </ul> Source code in <code>fptools/preprocess/pipelines/tdt_default.py</code> <pre><code>def tdt_default(\n    session: Session,\n    signal_map: list[SignalMapping],\n    show_steps: bool = True,\n    plot_dir: str = \"\",\n    trim_extent: Union[None, Literal[\"auto\"], float, tuple[float, float]] = \"auto\",\n    downsample: Optional[int] = None,\n) -&gt; Session:\n    \"\"\"A preprocess pipeline based on TDT tutorials.\n\n    Pipeline steps:\n    1) Signals are trimmed to the optical system start.\n    2) Perform linear regression between sensor and isosbestic\n    3) calculate dFF\n    #4) Signals are optionally downsampled factor `downsample`\n\n    Args:\n        session: the session to populate.\n        signal_map: mapping of signals to perform\n        show_steps: if `True`, produce diagnostic plots of the preprocessing steps.\n        plot_dir: path where diagnostic plots of the preprocessing steps should be saved.\n        trim_extent: specification for trimming. None disables trimming, auto uses the offset stored in `block.scalars.Fi1i.ts`, a single float trims that amount of time (in seconds) from the beginning, a tuple of two floats specifies the amount of time (in seconds) from the beginning and end to trim, respectively.\n        downsample: if not `None`, downsample signal by `downsample` factor.\n    \"\"\"\n    try:\n        if show_steps:\n            fig, axs = plt.subplots(4, 1, figsize=(24, 6 * 4))\n            palette = sns.color_palette(\"colorblind\", n_colors=len(signal_map))\n\n        signals: list[Signal] = []\n        for sm in signal_map:\n            signals.append(session.signals[sm[\"tdt_name\"]].copy(new_name=sm[\"dest_name\"]))\n\n        if show_steps:\n            for i, sig in enumerate(signals):\n                axs[0].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n            axs[0].set_title(\"Raw signal\")\n            axs[0].legend()\n\n        # trim raw signal start to when the optical system came online\n        if trim_extent is not None:\n            for sig in signals:\n                if trim_extent == \"auto\":\n                    trim_args = {\"begin\": int(session.scalars[\"Fi1i\"][0] * sig.fs)}\n                elif isinstance(trim_extent, float):\n                    trim_args = {\"begin\": int(trim_extent * sig.fs)}\n                elif len(trim_extent) == 2:\n                    trim_args = {\"begin\": int(trim_extent[0] * sig.fs), \"end\": int(trim_extent[1] * sig.fs)}\n\n                sig.signal, sig.time = trim(sig.signal, sig.time, **trim_args)\n\n            if show_steps:\n                for i, sig in enumerate(signals):\n                    axs[1].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n                axs[1].set_title(\"Trimmed Raw signal\")\n                axs[1].legend()\n        else:\n            axs[1].set_title(\"Trimming Disabled\")\n\n        ctrl_idx = next((i for i, sm in enumerate(signal_map) if sm[\"role\"] == \"control\"), None)\n        if ctrl_idx is None:\n            raise ValueError('at least one signal must be marked with `role`=\"Control\" in the `signal_map`!')\n        for sm, sig in zip(signal_map, signals):\n            if sm[\"role\"] == \"experimental\":\n\n                x = np.array(signals[ctrl_idx].signal)\n                y = np.array(sig.signal)\n                bls = np.polyfit(x, y, 1)\n                Y_fit_all = np.multiply(bls[0], x) + bls[1]\n                Y_dF_all = y - Y_fit_all\n                dFF = np.multiply(100, np.divide(Y_dF_all, Y_fit_all))\n                sig.signal = dFF\n                sig.units = \"\u0394F/F\"\n\n        if show_steps:\n            for i, sig in enumerate(signals):\n                axs[2].plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n            axs[2].set_title(\"Normalized (dff)\")\n            axs[2].legend()\n\n        # construct Signals and add to the Session\n        for sig in signals:\n            session.add_signal(sig)\n\n        return session\n    except:\n        raise\n    finally:\n        if show_steps:\n            fig.savefig(os.path.join(plot_dir, f\"{session.name}.png\"), dpi=600)\n            fig.savefig(os.path.join(plot_dir, f\"{session.name}.pdf\"))\n            plt.close(fig)\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/","title":"steps","text":""},{"location":"reference/fptools/preprocess/steps/#fptools.preprocess.steps","title":"<code>fptools.preprocess.steps</code>","text":"<p>Modules:</p> <ul> <li> <code>dbl_exp_fit</code>           \u2013            </li> <li> <code>dff</code>           \u2013            </li> <li> <code>downsample</code>           \u2013            </li> <li> <code>lowpass</code>           \u2013            </li> <li> <code>motion_correct</code>           \u2013            </li> <li> <code>rename</code>           \u2013            </li> <li> <code>trim_signals</code>           \u2013            </li> <li> <code>zscore</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/preprocess/steps/dbl_exp_fit/","title":"dbl_exp_fit","text":""},{"location":"reference/fptools/preprocess/steps/dbl_exp_fit/#fptools.preprocess.steps.dbl_exp_fit","title":"<code>fptools.preprocess.steps.dbl_exp_fit</code>","text":"<p>Classes:</p> <ul> <li> <code>DblExpFit</code>           \u2013            <p>A <code>Preprocessor</code> that fits a double exponential function.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/steps/dbl_exp_fit/#fptools.preprocess.steps.dbl_exp_fit.DblExpFit","title":"<code>DblExpFit</code>","text":"<p>               Bases: <code>PreprocessorStep</code></p> <p>A <code>Preprocessor</code> that fits a double exponential function.</p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Effect this preprocessing step.</p> </li> <li> <code>__init__</code>             \u2013              <p>Initialize this preprocessor.</p> </li> <li> <code>plot</code>             \u2013              <p>Plot the effects of this preprocessing step. Will show the double exponential fit.</p> </li> </ul> Source code in <code>fptools/preprocess/steps/dbl_exp_fit.py</code> <pre><code>class DblExpFit(PreprocessorStep):\n    \"\"\"A `Preprocessor` that fits a double exponential function.\"\"\"\n\n    def __init__(self, signals: SignalList):\n        \"\"\"Initialize this preprocessor.\n\n        Args:\n            signals: list of signal names to be fitted\n        \"\"\"\n        self.signals = signals\n\n    def __call__(self, session: Session) -&gt; Session:\n        \"\"\"Effect this preprocessing step.\n\n        Args:\n            session: the session to operate upon\n\n        Returns:\n            Session with the preprocessing step applied\n        \"\"\"\n        for signame in self._resolve_signal_names(session, self.signals):\n            sig = session.signals[signame]\n\n            dxp_sig = sig.copy(f\"{signame}_dxp\")\n            _, dxp_sig.signal = detrend_double_exponential(sig.time, sig.signal)\n            session.add_signal(dxp_sig)\n\n        return session\n\n    def plot(self, session: Session, ax: Axes):\n        \"\"\"Plot the effects of this preprocessing step. Will show the double exponential fit.\n\n        Args:\n            session: the session being operated upon\n            ax: matplotlib Axes for plotting onto\n        \"\"\"\n        signals = self._resolve_signal_names(session, self.signals)\n        palette = sns.color_palette(\"colorblind\", n_colors=len(signals))\n        for i, signame in enumerate(signals):\n            sig = session.signals[f\"{signame}_dxp\"]\n            ax.plot(sig.time, sig.signal, label=sig.name, c=palette[i], linestyle=\"--\")\n        ax.set_title(\"Double Exponential Fit\")\n        ax.legend()\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/dbl_exp_fit/#fptools.preprocess.steps.dbl_exp_fit.DblExpFit.__call__","title":"<code>__call__(session)</code>","text":"<p>Effect this preprocessing step.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to operate upon</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session with the preprocessing step applied</p> </li> </ul> Source code in <code>fptools/preprocess/steps/dbl_exp_fit.py</code> <pre><code>def __call__(self, session: Session) -&gt; Session:\n    \"\"\"Effect this preprocessing step.\n\n    Args:\n        session: the session to operate upon\n\n    Returns:\n        Session with the preprocessing step applied\n    \"\"\"\n    for signame in self._resolve_signal_names(session, self.signals):\n        sig = session.signals[signame]\n\n        dxp_sig = sig.copy(f\"{signame}_dxp\")\n        _, dxp_sig.signal = detrend_double_exponential(sig.time, sig.signal)\n        session.add_signal(dxp_sig)\n\n    return session\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/dbl_exp_fit/#fptools.preprocess.steps.dbl_exp_fit.DblExpFit.__init__","title":"<code>__init__(signals)</code>","text":"<p>Initialize this preprocessor.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>SignalList</code>)           \u2013            <p>list of signal names to be fitted</p> </li> </ul> Source code in <code>fptools/preprocess/steps/dbl_exp_fit.py</code> <pre><code>def __init__(self, signals: SignalList):\n    \"\"\"Initialize this preprocessor.\n\n    Args:\n        signals: list of signal names to be fitted\n    \"\"\"\n    self.signals = signals\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/dbl_exp_fit/#fptools.preprocess.steps.dbl_exp_fit.DblExpFit.plot","title":"<code>plot(session, ax)</code>","text":"<p>Plot the effects of this preprocessing step. Will show the double exponential fit.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session being operated upon</p> </li> <li> <code>ax</code>               (<code>Axes</code>)           \u2013            <p>matplotlib Axes for plotting onto</p> </li> </ul> Source code in <code>fptools/preprocess/steps/dbl_exp_fit.py</code> <pre><code>def plot(self, session: Session, ax: Axes):\n    \"\"\"Plot the effects of this preprocessing step. Will show the double exponential fit.\n\n    Args:\n        session: the session being operated upon\n        ax: matplotlib Axes for plotting onto\n    \"\"\"\n    signals = self._resolve_signal_names(session, self.signals)\n    palette = sns.color_palette(\"colorblind\", n_colors=len(signals))\n    for i, signame in enumerate(signals):\n        sig = session.signals[f\"{signame}_dxp\"]\n        ax.plot(sig.time, sig.signal, label=sig.name, c=palette[i], linestyle=\"--\")\n    ax.set_title(\"Double Exponential Fit\")\n    ax.legend()\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/dff/","title":"dff","text":""},{"location":"reference/fptools/preprocess/steps/dff/#fptools.preprocess.steps.dff","title":"<code>fptools.preprocess.steps.dff</code>","text":"<p>Classes:</p> <ul> <li> <code>Dff</code>           \u2013            <p>A <code>Preprocessor</code> that calculates signal dF/F.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/steps/dff/#fptools.preprocess.steps.dff.Dff","title":"<code>Dff</code>","text":"<p>               Bases: <code>PreprocessorStep</code></p> <p>A <code>Preprocessor</code> that calculates signal dF/F.</p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Effect this preprocessing step.</p> </li> <li> <code>__init__</code>             \u2013              <p>Initialize this preprocessor.</p> </li> <li> <code>plot</code>             \u2013              <p>Plot the effects of this preprocessing step. Will show the computed dF/F signal.</p> </li> </ul> Source code in <code>fptools/preprocess/steps/dff.py</code> <pre><code>class Dff(PreprocessorStep):\n    \"\"\"A `Preprocessor` that calculates signal dF/F.\"\"\"\n\n    def __init__(self, signals: PairedSignalList):\n        \"\"\"Initialize this preprocessor.\n\n        Args:\n            signals: list of signal names to be downsampled\n            frequency: critical frequency used for lowpass filter\n        \"\"\"\n        self.signals = signals\n\n    def __call__(self, session: Session) -&gt; Session:\n        \"\"\"Effect this preprocessing step.\n\n        Args:\n            session: the session to operate upon\n\n        Returns:\n            Session with the preprocessing step applied\n        \"\"\"\n        for sig1, sig2 in self.signals:\n            exp = session.signals[sig1]\n            ctr = session.signals[sig2]\n\n            exp.signal = (((exp - ctr) / ctr) * 100).signal\n            exp.units = \"\u0394F/F\"\n\n        return session\n\n    def plot(self, session: Session, ax: Axes):\n        \"\"\"Plot the effects of this preprocessing step. Will show the computed dF/F signal.\n\n        Args:\n            session: the session being operated upon\n            ax: matplotlib Axes for plotting onto\n        \"\"\"\n        palette = sns.color_palette(\"colorblind\", n_colors=len(self.signals))\n        for i, (signame, _) in enumerate(self.signals):\n            sig = session.signals[signame]\n            ax.plot(sig.time, sig.signal, label=sig.name, c=palette[i], linestyle=\"-\")\n        ax.set_title(\"Calculated dF/F Signal\")\n        ax.legend()\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/dff/#fptools.preprocess.steps.dff.Dff.__call__","title":"<code>__call__(session)</code>","text":"<p>Effect this preprocessing step.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to operate upon</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session with the preprocessing step applied</p> </li> </ul> Source code in <code>fptools/preprocess/steps/dff.py</code> <pre><code>def __call__(self, session: Session) -&gt; Session:\n    \"\"\"Effect this preprocessing step.\n\n    Args:\n        session: the session to operate upon\n\n    Returns:\n        Session with the preprocessing step applied\n    \"\"\"\n    for sig1, sig2 in self.signals:\n        exp = session.signals[sig1]\n        ctr = session.signals[sig2]\n\n        exp.signal = (((exp - ctr) / ctr) * 100).signal\n        exp.units = \"\u0394F/F\"\n\n    return session\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/dff/#fptools.preprocess.steps.dff.Dff.__init__","title":"<code>__init__(signals)</code>","text":"<p>Initialize this preprocessor.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>PairedSignalList</code>)           \u2013            <p>list of signal names to be downsampled</p> </li> <li> <code>frequency</code>           \u2013            <p>critical frequency used for lowpass filter</p> </li> </ul> Source code in <code>fptools/preprocess/steps/dff.py</code> <pre><code>def __init__(self, signals: PairedSignalList):\n    \"\"\"Initialize this preprocessor.\n\n    Args:\n        signals: list of signal names to be downsampled\n        frequency: critical frequency used for lowpass filter\n    \"\"\"\n    self.signals = signals\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/dff/#fptools.preprocess.steps.dff.Dff.plot","title":"<code>plot(session, ax)</code>","text":"<p>Plot the effects of this preprocessing step. Will show the computed dF/F signal.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session being operated upon</p> </li> <li> <code>ax</code>               (<code>Axes</code>)           \u2013            <p>matplotlib Axes for plotting onto</p> </li> </ul> Source code in <code>fptools/preprocess/steps/dff.py</code> <pre><code>def plot(self, session: Session, ax: Axes):\n    \"\"\"Plot the effects of this preprocessing step. Will show the computed dF/F signal.\n\n    Args:\n        session: the session being operated upon\n        ax: matplotlib Axes for plotting onto\n    \"\"\"\n    palette = sns.color_palette(\"colorblind\", n_colors=len(self.signals))\n    for i, (signame, _) in enumerate(self.signals):\n        sig = session.signals[signame]\n        ax.plot(sig.time, sig.signal, label=sig.name, c=palette[i], linestyle=\"-\")\n    ax.set_title(\"Calculated dF/F Signal\")\n    ax.legend()\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/downsample/","title":"downsample","text":""},{"location":"reference/fptools/preprocess/steps/downsample/#fptools.preprocess.steps.downsample","title":"<code>fptools.preprocess.steps.downsample</code>","text":"<p>Classes:</p> <ul> <li> <code>Downsample</code>           \u2013            <p>A <code>Preprocessor</code> that downsamples signals.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/steps/downsample/#fptools.preprocess.steps.downsample.Downsample","title":"<code>Downsample</code>","text":"<p>               Bases: <code>PreprocessorStep</code></p> <p>A <code>Preprocessor</code> that downsamples signals.</p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Effect this preprocessing step.</p> </li> <li> <code>__init__</code>             \u2013              <p>Initialize this preprocessor.</p> </li> <li> <code>plot</code>             \u2013              <p>Plot the effects of the preprocessing step. Will show the downsampled signals.</p> </li> </ul> Source code in <code>fptools/preprocess/steps/downsample.py</code> <pre><code>class Downsample(PreprocessorStep):\n    \"\"\"A `Preprocessor` that downsamples signals.\"\"\"\n\n    def __init__(self, signals: SignalList, window: int = 10, factor: int = 10):\n        \"\"\"Initialize this preprocessor.\n\n        Downsampling performs a moving average convolution, with window size `window`, and then samples every `factor` steps.\n\n        Args:\n            signals: list of signal names to be downsampled\n            window: size of the window for the moving average calculation\n            factor: step size for taking samples\n        \"\"\"\n        self.signals = signals\n        self.window = window\n        self.factor = factor\n\n    def __call__(self, session: Session) -&gt; Session:\n        \"\"\"Effect this preprocessing step.\n\n        Args:\n            session: the session to operate upon\n\n        Returns:\n            Session with the preprocessing step applied\n        \"\"\"\n        for signame in self._resolve_signal_names(session, self.signals):\n            sig = session.signals[signame]\n\n            sig.signal, sig.time = downsample(sig.signal, sig.time, window=self.window, factor=self.factor)\n            sig.fs = t2fs(sig.time)\n        return session\n\n    def plot(self, session: Session, ax: Axes):\n        \"\"\"Plot the effects of the preprocessing step. Will show the downsampled signals.\n\n        Args:\n            session: the session being operated upon\n            ax: matplotlib Axes for plotting onto\n        \"\"\"\n        signals = self._resolve_signal_names(session, self.signals)\n        palette = sns.color_palette(\"colorblind\", n_colors=len(signals))\n        for i, signame in enumerate(signals):\n            sig = session.signals[signame]\n            ax.plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n        ax.set_title(\"Downsampled Signal\")\n        ax.legend()\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/downsample/#fptools.preprocess.steps.downsample.Downsample.__call__","title":"<code>__call__(session)</code>","text":"<p>Effect this preprocessing step.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to operate upon</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session with the preprocessing step applied</p> </li> </ul> Source code in <code>fptools/preprocess/steps/downsample.py</code> <pre><code>def __call__(self, session: Session) -&gt; Session:\n    \"\"\"Effect this preprocessing step.\n\n    Args:\n        session: the session to operate upon\n\n    Returns:\n        Session with the preprocessing step applied\n    \"\"\"\n    for signame in self._resolve_signal_names(session, self.signals):\n        sig = session.signals[signame]\n\n        sig.signal, sig.time = downsample(sig.signal, sig.time, window=self.window, factor=self.factor)\n        sig.fs = t2fs(sig.time)\n    return session\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/downsample/#fptools.preprocess.steps.downsample.Downsample.__init__","title":"<code>__init__(signals, window=10, factor=10)</code>","text":"<p>Initialize this preprocessor.</p> <p>Downsampling performs a moving average convolution, with window size <code>window</code>, and then samples every <code>factor</code> steps.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>SignalList</code>)           \u2013            <p>list of signal names to be downsampled</p> </li> <li> <code>window</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>size of the window for the moving average calculation</p> </li> <li> <code>factor</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>step size for taking samples</p> </li> </ul> Source code in <code>fptools/preprocess/steps/downsample.py</code> <pre><code>def __init__(self, signals: SignalList, window: int = 10, factor: int = 10):\n    \"\"\"Initialize this preprocessor.\n\n    Downsampling performs a moving average convolution, with window size `window`, and then samples every `factor` steps.\n\n    Args:\n        signals: list of signal names to be downsampled\n        window: size of the window for the moving average calculation\n        factor: step size for taking samples\n    \"\"\"\n    self.signals = signals\n    self.window = window\n    self.factor = factor\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/downsample/#fptools.preprocess.steps.downsample.Downsample.plot","title":"<code>plot(session, ax)</code>","text":"<p>Plot the effects of the preprocessing step. Will show the downsampled signals.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session being operated upon</p> </li> <li> <code>ax</code>               (<code>Axes</code>)           \u2013            <p>matplotlib Axes for plotting onto</p> </li> </ul> Source code in <code>fptools/preprocess/steps/downsample.py</code> <pre><code>def plot(self, session: Session, ax: Axes):\n    \"\"\"Plot the effects of the preprocessing step. Will show the downsampled signals.\n\n    Args:\n        session: the session being operated upon\n        ax: matplotlib Axes for plotting onto\n    \"\"\"\n    signals = self._resolve_signal_names(session, self.signals)\n    palette = sns.color_palette(\"colorblind\", n_colors=len(signals))\n    for i, signame in enumerate(signals):\n        sig = session.signals[signame]\n        ax.plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n    ax.set_title(\"Downsampled Signal\")\n    ax.legend()\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/lowpass/","title":"lowpass","text":""},{"location":"reference/fptools/preprocess/steps/lowpass/#fptools.preprocess.steps.lowpass","title":"<code>fptools.preprocess.steps.lowpass</code>","text":"<p>Classes:</p> <ul> <li> <code>Lowpass</code>           \u2013            <p>A <code>Preprocessor</code> that generates a lowpass filtered signal.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/steps/lowpass/#fptools.preprocess.steps.lowpass.Lowpass","title":"<code>Lowpass</code>","text":"<p>               Bases: <code>PreprocessorStep</code></p> <p>A <code>Preprocessor</code> that generates a lowpass filtered signal.</p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Effect this preprocessing step.</p> </li> <li> <code>__init__</code>             \u2013              <p>Initialize this preprocessor.</p> </li> <li> <code>plot</code>             \u2013              <p>Plot the effects of this preprocessing step. Will show the lowpass filtered signal.</p> </li> </ul> Source code in <code>fptools/preprocess/steps/lowpass.py</code> <pre><code>class Lowpass(PreprocessorStep):\n    \"\"\"A `Preprocessor` that generates a lowpass filtered signal.\"\"\"\n\n    def __init__(self, signals: SignalList, frequency: float = 0.01):\n        \"\"\"Initialize this preprocessor.\n\n        Args:\n            signals: list of signal names to be downsampled\n            frequency: critical frequency used for lowpass filter\n        \"\"\"\n        self.signals = signals\n        self.frequency = frequency\n\n    def __call__(self, session: Session) -&gt; Session:\n        \"\"\"Effect this preprocessing step.\n\n        Args:\n            session: the session to operate upon\n\n        Returns:\n            Session with the preprocessing step applied\n        \"\"\"\n        for signame in self._resolve_signal_names(session, self.signals):\n            sig = session.signals[signame]\n\n            lowpass_sig = sig.copy(f\"{signame}_lowpass\")\n            lowpass_sig.signal = lowpass_filter(sig.signal, sig.fs, self.frequency)\n            session.add_signal(lowpass_sig)\n\n        return session\n\n    def plot(self, session: Session, ax: Axes):\n        \"\"\"Plot the effects of this preprocessing step. Will show the lowpass filtered signal.\n\n        Args:\n            session: the session being operated upon\n            ax: matplotlib Axes for plotting onto\n        \"\"\"\n        signals = self._resolve_signal_names(session, self.signals)\n        palette = sns.color_palette(\"colorblind\", n_colors=len(signals))\n        for i, signame in enumerate(signals):\n            sig = session.signals[f\"{signame}_lowpass\"]\n            ax.plot(sig.time, sig.signal, label=sig.name, c=palette[i], linestyle=\"--\")\n        ax.set_title(\"Lowpass Filtered Signal\")\n        ax.legend()\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/lowpass/#fptools.preprocess.steps.lowpass.Lowpass.__call__","title":"<code>__call__(session)</code>","text":"<p>Effect this preprocessing step.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to operate upon</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session with the preprocessing step applied</p> </li> </ul> Source code in <code>fptools/preprocess/steps/lowpass.py</code> <pre><code>def __call__(self, session: Session) -&gt; Session:\n    \"\"\"Effect this preprocessing step.\n\n    Args:\n        session: the session to operate upon\n\n    Returns:\n        Session with the preprocessing step applied\n    \"\"\"\n    for signame in self._resolve_signal_names(session, self.signals):\n        sig = session.signals[signame]\n\n        lowpass_sig = sig.copy(f\"{signame}_lowpass\")\n        lowpass_sig.signal = lowpass_filter(sig.signal, sig.fs, self.frequency)\n        session.add_signal(lowpass_sig)\n\n    return session\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/lowpass/#fptools.preprocess.steps.lowpass.Lowpass.__init__","title":"<code>__init__(signals, frequency=0.01)</code>","text":"<p>Initialize this preprocessor.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>SignalList</code>)           \u2013            <p>list of signal names to be downsampled</p> </li> <li> <code>frequency</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>critical frequency used for lowpass filter</p> </li> </ul> Source code in <code>fptools/preprocess/steps/lowpass.py</code> <pre><code>def __init__(self, signals: SignalList, frequency: float = 0.01):\n    \"\"\"Initialize this preprocessor.\n\n    Args:\n        signals: list of signal names to be downsampled\n        frequency: critical frequency used for lowpass filter\n    \"\"\"\n    self.signals = signals\n    self.frequency = frequency\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/lowpass/#fptools.preprocess.steps.lowpass.Lowpass.plot","title":"<code>plot(session, ax)</code>","text":"<p>Plot the effects of this preprocessing step. Will show the lowpass filtered signal.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session being operated upon</p> </li> <li> <code>ax</code>               (<code>Axes</code>)           \u2013            <p>matplotlib Axes for plotting onto</p> </li> </ul> Source code in <code>fptools/preprocess/steps/lowpass.py</code> <pre><code>def plot(self, session: Session, ax: Axes):\n    \"\"\"Plot the effects of this preprocessing step. Will show the lowpass filtered signal.\n\n    Args:\n        session: the session being operated upon\n        ax: matplotlib Axes for plotting onto\n    \"\"\"\n    signals = self._resolve_signal_names(session, self.signals)\n    palette = sns.color_palette(\"colorblind\", n_colors=len(signals))\n    for i, signame in enumerate(signals):\n        sig = session.signals[f\"{signame}_lowpass\"]\n        ax.plot(sig.time, sig.signal, label=sig.name, c=palette[i], linestyle=\"--\")\n    ax.set_title(\"Lowpass Filtered Signal\")\n    ax.legend()\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/motion_correct/","title":"motion_correct","text":""},{"location":"reference/fptools/preprocess/steps/motion_correct/#fptools.preprocess.steps.motion_correct","title":"<code>fptools.preprocess.steps.motion_correct</code>","text":"<p>Classes:</p> <ul> <li> <code>MotionCorrect</code>           \u2013            <p>A <code>Preprocessor</code> that estimates and corrects for motion artifacts.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/steps/motion_correct/#fptools.preprocess.steps.motion_correct.MotionCorrect","title":"<code>MotionCorrect</code>","text":"<p>               Bases: <code>PreprocessorStep</code></p> <p>A <code>Preprocessor</code> that estimates and corrects for motion artifacts.</p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Effect this preprocessing step.</p> </li> <li> <code>__init__</code>             \u2013              <p>Initialize this preprocessor.</p> </li> <li> <code>plot</code>             \u2013              <p>Plot the effects of this preprocessing step. Will show the motion estimate and the corrected signal.</p> </li> </ul> Source code in <code>fptools/preprocess/steps/motion_correct.py</code> <pre><code>class MotionCorrect(PreprocessorStep):\n    \"\"\"A `Preprocessor` that estimates and corrects for motion artifacts.\"\"\"\n\n    def __init__(self, signals: PairedSignalList):\n        \"\"\"Initialize this preprocessor.\n\n        Args:\n            signals: list of signal names to be downsampled\n        \"\"\"\n        self.signals = signals\n\n    def __call__(self, session: Session) -&gt; Session:\n        \"\"\"Effect this preprocessing step.\n\n        Args:\n            session: the session to operate upon\n\n        Returns:\n            Session with the preprocessing step applied\n        \"\"\"\n        for sig1, sig2 in self.signals:\n            exp = session.signals[sig1]\n            ctr = session.signals[sig2]\n            est_motion = exp.copy(f\"{exp.name}_motion_est\")\n            est_motion.units = \"AU\"\n\n            exp.signal, est_motion.signal = estimate_motion(exp.signal, ctr.signal)\n            session.add_signal(est_motion)\n\n        return session\n\n    def plot(self, session: Session, ax: Axes):\n        \"\"\"Plot the effects of this preprocessing step. Will show the motion estimate and the corrected signal.\n\n        Args:\n            session: the session being operated upon\n            ax: matplotlib Axes for plotting onto\n        \"\"\"\n        palette = sns.color_palette(\"colorblind\", n_colors=len(self.signals))\n        for i, (signame, _) in enumerate(self.signals):\n            sig = session.signals[signame]\n            mot = session.signals[f\"{signame}_motion_est\"]\n            ax.plot(sig.time, sig.signal, label=sig.name, c=palette[i], linestyle=\"-\")\n            ax.plot(mot.time, mot.signal, label=mot.name, c=sns.desaturate(palette[i], 0.3), linestyle=\"-\")\n        ax.set_title(\"Motion Estimation and Correction\")\n        ax.legend()\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/motion_correct/#fptools.preprocess.steps.motion_correct.MotionCorrect.__call__","title":"<code>__call__(session)</code>","text":"<p>Effect this preprocessing step.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to operate upon</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session with the preprocessing step applied</p> </li> </ul> Source code in <code>fptools/preprocess/steps/motion_correct.py</code> <pre><code>def __call__(self, session: Session) -&gt; Session:\n    \"\"\"Effect this preprocessing step.\n\n    Args:\n        session: the session to operate upon\n\n    Returns:\n        Session with the preprocessing step applied\n    \"\"\"\n    for sig1, sig2 in self.signals:\n        exp = session.signals[sig1]\n        ctr = session.signals[sig2]\n        est_motion = exp.copy(f\"{exp.name}_motion_est\")\n        est_motion.units = \"AU\"\n\n        exp.signal, est_motion.signal = estimate_motion(exp.signal, ctr.signal)\n        session.add_signal(est_motion)\n\n    return session\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/motion_correct/#fptools.preprocess.steps.motion_correct.MotionCorrect.__init__","title":"<code>__init__(signals)</code>","text":"<p>Initialize this preprocessor.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>PairedSignalList</code>)           \u2013            <p>list of signal names to be downsampled</p> </li> </ul> Source code in <code>fptools/preprocess/steps/motion_correct.py</code> <pre><code>def __init__(self, signals: PairedSignalList):\n    \"\"\"Initialize this preprocessor.\n\n    Args:\n        signals: list of signal names to be downsampled\n    \"\"\"\n    self.signals = signals\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/motion_correct/#fptools.preprocess.steps.motion_correct.MotionCorrect.plot","title":"<code>plot(session, ax)</code>","text":"<p>Plot the effects of this preprocessing step. Will show the motion estimate and the corrected signal.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session being operated upon</p> </li> <li> <code>ax</code>               (<code>Axes</code>)           \u2013            <p>matplotlib Axes for plotting onto</p> </li> </ul> Source code in <code>fptools/preprocess/steps/motion_correct.py</code> <pre><code>def plot(self, session: Session, ax: Axes):\n    \"\"\"Plot the effects of this preprocessing step. Will show the motion estimate and the corrected signal.\n\n    Args:\n        session: the session being operated upon\n        ax: matplotlib Axes for plotting onto\n    \"\"\"\n    palette = sns.color_palette(\"colorblind\", n_colors=len(self.signals))\n    for i, (signame, _) in enumerate(self.signals):\n        sig = session.signals[signame]\n        mot = session.signals[f\"{signame}_motion_est\"]\n        ax.plot(sig.time, sig.signal, label=sig.name, c=palette[i], linestyle=\"-\")\n        ax.plot(mot.time, mot.signal, label=mot.name, c=sns.desaturate(palette[i], 0.3), linestyle=\"-\")\n    ax.set_title(\"Motion Estimation and Correction\")\n    ax.legend()\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/rename/","title":"rename","text":""},{"location":"reference/fptools/preprocess/steps/rename/#fptools.preprocess.steps.rename","title":"<code>fptools.preprocess.steps.rename</code>","text":"<p>Classes:</p> <ul> <li> <code>Rename</code>           \u2013            <p>A <code>Preprocessor</code> that allows you to rename things.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/steps/rename/#fptools.preprocess.steps.rename.Rename","title":"<code>Rename</code>","text":"<p>               Bases: <code>PreprocessorStep</code></p> <p>A <code>Preprocessor</code> that allows you to rename things.</p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Effect this preprocessing step.</p> </li> <li> <code>__init__</code>             \u2013              <p>Initialize this preprocessor.</p> </li> <li> <code>plot</code>             \u2013              <p>Plot the effects of this preprocessing step. Will show the computed dF/F signal.</p> </li> </ul> Source code in <code>fptools/preprocess/steps/rename.py</code> <pre><code>class Rename(PreprocessorStep):\n    \"\"\"A `Preprocessor` that allows you to rename things.\"\"\"\n\n    def __init__(\n        self, signals: Optional[dict[str, str]] = None, epocs: Optional[dict[str, str]] = None, scalars: Optional[dict[str, str]] = None\n    ):\n        \"\"\"Initialize this preprocessor.\n\n        Args:\n            signals: dictionary of signal names to be renamed\n            epocs: dictionary of epoc names to be renamed\n            scalars: dictionary of scalar names to be renamed\n        \"\"\"\n        self.signals = signals\n        self.epocs = epocs\n        self.scalars = scalars\n\n    def __call__(self, session: Session) -&gt; Session:\n        \"\"\"Effect this preprocessing step.\n\n        Args:\n            session: the session to operate upon\n\n        Returns:\n            Session with the preprocessing step applied\n        \"\"\"\n        if self.signals is not None:\n            for k, v in self.signals.items():\n                session.rename_signal(k, v)\n\n        if self.epocs is not None:\n            for k, v in self.epocs.items():\n                session.rename_epoc(k, v)\n\n        if self.scalars is not None:\n            for k, v in self.scalars.items():\n                session.rename_scalar(k, v)\n\n        return session\n\n    def plot(self, session: Session, ax: Axes):\n        \"\"\"Plot the effects of this preprocessing step. Will show the computed dF/F signal.\n\n        Args:\n            session: the session being operated upon\n            ax: matplotlib Axes for plotting onto\n        \"\"\"\n        headers = [\"Type\", \"Original\", \"New\"]\n        rows = []\n        if self.signals is not None:\n            for k, v in self.signals.items():\n                rows.append([\"signal\", k, v])\n\n        if self.epocs is not None:\n            for k, v in self.epocs.items():\n                rows.append([\"epoc\", k, v])\n\n        if self.scalars is not None:\n            for k, v in self.scalars.items():\n                rows.append([\"scalar\", k, v])\n\n        table = ax.table(cellText=rows, colLabels=headers, cellLoc=\"center\", loc=\"upper center\")\n        # set header cells to have bold text\n        for row, col in [(0, i) for i in range(len(headers))]:\n            table_cell = table[row, col]\n            table_cell.set_text_props(fontweight=\"bold\")\n\n        ax.axis(\"off\")  # turn off axis, not needed since this is a table\n        ax.set_title(\"Renamed Items\")\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/rename/#fptools.preprocess.steps.rename.Rename.__call__","title":"<code>__call__(session)</code>","text":"<p>Effect this preprocessing step.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to operate upon</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session with the preprocessing step applied</p> </li> </ul> Source code in <code>fptools/preprocess/steps/rename.py</code> <pre><code>def __call__(self, session: Session) -&gt; Session:\n    \"\"\"Effect this preprocessing step.\n\n    Args:\n        session: the session to operate upon\n\n    Returns:\n        Session with the preprocessing step applied\n    \"\"\"\n    if self.signals is not None:\n        for k, v in self.signals.items():\n            session.rename_signal(k, v)\n\n    if self.epocs is not None:\n        for k, v in self.epocs.items():\n            session.rename_epoc(k, v)\n\n    if self.scalars is not None:\n        for k, v in self.scalars.items():\n            session.rename_scalar(k, v)\n\n    return session\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/rename/#fptools.preprocess.steps.rename.Rename.__init__","title":"<code>__init__(signals=None, epocs=None, scalars=None)</code>","text":"<p>Initialize this preprocessor.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>Optional[dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>dictionary of signal names to be renamed</p> </li> <li> <code>epocs</code>               (<code>Optional[dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>dictionary of epoc names to be renamed</p> </li> <li> <code>scalars</code>               (<code>Optional[dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>dictionary of scalar names to be renamed</p> </li> </ul> Source code in <code>fptools/preprocess/steps/rename.py</code> <pre><code>def __init__(\n    self, signals: Optional[dict[str, str]] = None, epocs: Optional[dict[str, str]] = None, scalars: Optional[dict[str, str]] = None\n):\n    \"\"\"Initialize this preprocessor.\n\n    Args:\n        signals: dictionary of signal names to be renamed\n        epocs: dictionary of epoc names to be renamed\n        scalars: dictionary of scalar names to be renamed\n    \"\"\"\n    self.signals = signals\n    self.epocs = epocs\n    self.scalars = scalars\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/rename/#fptools.preprocess.steps.rename.Rename.plot","title":"<code>plot(session, ax)</code>","text":"<p>Plot the effects of this preprocessing step. Will show the computed dF/F signal.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session being operated upon</p> </li> <li> <code>ax</code>               (<code>Axes</code>)           \u2013            <p>matplotlib Axes for plotting onto</p> </li> </ul> Source code in <code>fptools/preprocess/steps/rename.py</code> <pre><code>def plot(self, session: Session, ax: Axes):\n    \"\"\"Plot the effects of this preprocessing step. Will show the computed dF/F signal.\n\n    Args:\n        session: the session being operated upon\n        ax: matplotlib Axes for plotting onto\n    \"\"\"\n    headers = [\"Type\", \"Original\", \"New\"]\n    rows = []\n    if self.signals is not None:\n        for k, v in self.signals.items():\n            rows.append([\"signal\", k, v])\n\n    if self.epocs is not None:\n        for k, v in self.epocs.items():\n            rows.append([\"epoc\", k, v])\n\n    if self.scalars is not None:\n        for k, v in self.scalars.items():\n            rows.append([\"scalar\", k, v])\n\n    table = ax.table(cellText=rows, colLabels=headers, cellLoc=\"center\", loc=\"upper center\")\n    # set header cells to have bold text\n    for row, col in [(0, i) for i in range(len(headers))]:\n        table_cell = table[row, col]\n        table_cell.set_text_props(fontweight=\"bold\")\n\n    ax.axis(\"off\")  # turn off axis, not needed since this is a table\n    ax.set_title(\"Renamed Items\")\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/trim_signals/","title":"trim_signals","text":""},{"location":"reference/fptools/preprocess/steps/trim_signals/#fptools.preprocess.steps.trim_signals","title":"<code>fptools.preprocess.steps.trim_signals</code>","text":"<p>Classes:</p> <ul> <li> <code>TrimSignals</code>           \u2013            <p>A <code>Preprocessor</code> that trims signals.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/steps/trim_signals/#fptools.preprocess.steps.trim_signals.TrimSignals","title":"<code>TrimSignals</code>","text":"<p>               Bases: <code>PreprocessorStep</code></p> <p>A <code>Preprocessor</code> that trims signals.</p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Effect this preprocessing step.</p> </li> <li> <code>__init__</code>             \u2013              <p>Initialize this preprocessor.</p> </li> <li> <code>plot</code>             \u2013              <p>Plot the effects of this preprocessing step. Will show trimmed signals.</p> </li> </ul> Source code in <code>fptools/preprocess/steps/trim_signals.py</code> <pre><code>class TrimSignals(PreprocessorStep):\n    \"\"\"A `Preprocessor` that trims signals.\"\"\"\n\n    def __init__(self, signals: SignalList, extent: Union[None, Literal[\"auto\"], float, tuple[float, float]] = \"auto\"):\n        \"\"\"Initialize this preprocessor.\n\n        Args:\n            signals: list of signal names to be trimmed\n            extent: specification for trimming. \"auto\" uses the offset stored in `scalars['Fi1i']`, a single float trims that amount of time (in seconds) from the beginning, a tuple of two floats specifies the amount of time (in seconds) from the beginning and end to trim, respectively. If None, no trimming will be performed.\n        \"\"\"\n        self.signals = signals\n        self.extent = extent\n\n    def __call__(self, session: Session) -&gt; Session:\n        \"\"\"Effect this preprocessing step.\n\n        Args:\n            session: the session to operate upon\n\n        Returns:\n            Session with the preprocessing step applied\n        \"\"\"\n        for signame in self._resolve_signal_names(session, self.signals):\n            sig = session.signals[signame]\n            if self.extent is None:\n                continue\n            elif self.extent == \"auto\":\n                trim_args = {\"begin\": int(session.scalars[\"Fi1i\"][0] * sig.fs)}\n            elif isinstance(self.extent, float):\n                trim_args = {\"begin\": int(self.extent * sig.fs)}\n            elif len(self.extent) == 2:\n                trim_args = {\"begin\": int(self.extent[0] * sig.fs), \"end\": int(self.extent[1] * sig.fs)}\n\n            sig.signal, sig.time = trim(sig.signal, sig.time, **trim_args)\n        return session\n\n    def plot(self, session: Session, ax: Axes):\n        \"\"\"Plot the effects of this preprocessing step. Will show trimmed signals.\n\n        Args:\n            session: the session being operated upon\n            ax: matplotlib Axes for plotting onto\n        \"\"\"\n        signals = self._resolve_signal_names(session, self.signals)\n        palette = sns.color_palette(\"colorblind\", n_colors=len(signals))\n        for i, signame in enumerate(signals):\n            sig = session.signals[signame]\n            ax.plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n        ax.set_title(\"Trimmed Signal\")\n        ax.legend()\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/trim_signals/#fptools.preprocess.steps.trim_signals.TrimSignals.__call__","title":"<code>__call__(session)</code>","text":"<p>Effect this preprocessing step.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to operate upon</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session with the preprocessing step applied</p> </li> </ul> Source code in <code>fptools/preprocess/steps/trim_signals.py</code> <pre><code>def __call__(self, session: Session) -&gt; Session:\n    \"\"\"Effect this preprocessing step.\n\n    Args:\n        session: the session to operate upon\n\n    Returns:\n        Session with the preprocessing step applied\n    \"\"\"\n    for signame in self._resolve_signal_names(session, self.signals):\n        sig = session.signals[signame]\n        if self.extent is None:\n            continue\n        elif self.extent == \"auto\":\n            trim_args = {\"begin\": int(session.scalars[\"Fi1i\"][0] * sig.fs)}\n        elif isinstance(self.extent, float):\n            trim_args = {\"begin\": int(self.extent * sig.fs)}\n        elif len(self.extent) == 2:\n            trim_args = {\"begin\": int(self.extent[0] * sig.fs), \"end\": int(self.extent[1] * sig.fs)}\n\n        sig.signal, sig.time = trim(sig.signal, sig.time, **trim_args)\n    return session\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/trim_signals/#fptools.preprocess.steps.trim_signals.TrimSignals.__init__","title":"<code>__init__(signals, extent='auto')</code>","text":"<p>Initialize this preprocessor.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>SignalList</code>)           \u2013            <p>list of signal names to be trimmed</p> </li> <li> <code>extent</code>               (<code>Union[None, Literal['auto'], float, tuple[float, float]]</code>, default:                   <code>'auto'</code> )           \u2013            <p>specification for trimming. \"auto\" uses the offset stored in <code>scalars['Fi1i']</code>, a single float trims that amount of time (in seconds) from the beginning, a tuple of two floats specifies the amount of time (in seconds) from the beginning and end to trim, respectively. If None, no trimming will be performed.</p> </li> </ul> Source code in <code>fptools/preprocess/steps/trim_signals.py</code> <pre><code>def __init__(self, signals: SignalList, extent: Union[None, Literal[\"auto\"], float, tuple[float, float]] = \"auto\"):\n    \"\"\"Initialize this preprocessor.\n\n    Args:\n        signals: list of signal names to be trimmed\n        extent: specification for trimming. \"auto\" uses the offset stored in `scalars['Fi1i']`, a single float trims that amount of time (in seconds) from the beginning, a tuple of two floats specifies the amount of time (in seconds) from the beginning and end to trim, respectively. If None, no trimming will be performed.\n    \"\"\"\n    self.signals = signals\n    self.extent = extent\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/trim_signals/#fptools.preprocess.steps.trim_signals.TrimSignals.plot","title":"<code>plot(session, ax)</code>","text":"<p>Plot the effects of this preprocessing step. Will show trimmed signals.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session being operated upon</p> </li> <li> <code>ax</code>               (<code>Axes</code>)           \u2013            <p>matplotlib Axes for plotting onto</p> </li> </ul> Source code in <code>fptools/preprocess/steps/trim_signals.py</code> <pre><code>def plot(self, session: Session, ax: Axes):\n    \"\"\"Plot the effects of this preprocessing step. Will show trimmed signals.\n\n    Args:\n        session: the session being operated upon\n        ax: matplotlib Axes for plotting onto\n    \"\"\"\n    signals = self._resolve_signal_names(session, self.signals)\n    palette = sns.color_palette(\"colorblind\", n_colors=len(signals))\n    for i, signame in enumerate(signals):\n        sig = session.signals[signame]\n        ax.plot(sig.time, sig.signal, label=sig.name, c=palette[i])\n    ax.set_title(\"Trimmed Signal\")\n    ax.legend()\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/zscore/","title":"zscore","text":""},{"location":"reference/fptools/preprocess/steps/zscore/#fptools.preprocess.steps.zscore","title":"<code>fptools.preprocess.steps.zscore</code>","text":"<p>Classes:</p> <ul> <li> <code>Zscore</code>           \u2013            <p>A <code>Preprocessor</code> that calculates signal z-scores.</p> </li> </ul>"},{"location":"reference/fptools/preprocess/steps/zscore/#fptools.preprocess.steps.zscore.Zscore","title":"<code>Zscore</code>","text":"<p>               Bases: <code>PreprocessorStep</code></p> <p>A <code>Preprocessor</code> that calculates signal z-scores.</p> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Effect this preprocessing step.</p> </li> <li> <code>__init__</code>             \u2013              <p>Initialize this preprocessor.</p> </li> <li> <code>plot</code>             \u2013              <p>Plot the effects of this preprocessing step. Will show the computed zscore signal.</p> </li> </ul> Source code in <code>fptools/preprocess/steps/zscore.py</code> <pre><code>class Zscore(PreprocessorStep):\n    \"\"\"A `Preprocessor` that calculates signal z-scores.\"\"\"\n\n    def __init__(self, signals: SignalList):\n        \"\"\"Initialize this preprocessor.\n\n        Args:\n            signals: list of signal names to be downsampled\n            frequency: critical frequency used for lowpass filter\n        \"\"\"\n        self.signals = signals\n\n    def __call__(self, session: Session) -&gt; Session:\n        \"\"\"Effect this preprocessing step.\n\n        Args:\n            session: the session to operate upon\n\n        Returns:\n            Session with the preprocessing step applied\n        \"\"\"\n        for signame in self._resolve_signal_names(session, self.signals):\n            sig = session.signals[signame]\n\n            sig.signal = stats.zscore(sig.signal)\n            sig.units = \"AU\"\n\n        return session\n\n    def plot(self, session: Session, ax: Axes):\n        \"\"\"Plot the effects of this preprocessing step. Will show the computed zscore signal.\n\n        Args:\n            session: the session being operated upon\n            ax: matplotlib Axes for plotting onto\n        \"\"\"\n        signals = self._resolve_signal_names(session, self.signals)\n        palette = sns.color_palette(\"colorblind\", n_colors=len(signals))\n        for i, signame in enumerate(signals):\n            sig = session.signals[signame]\n            ax.plot(sig.time, sig.signal, label=f\"corrected {sig.name}\", c=palette[i], linestyle=\"-\")\n        ax.set_title(\"Calculated zscore Signal\")\n        ax.legend()\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/zscore/#fptools.preprocess.steps.zscore.Zscore.__call__","title":"<code>__call__(session)</code>","text":"<p>Effect this preprocessing step.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session to operate upon</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Session</code>           \u2013            <p>Session with the preprocessing step applied</p> </li> </ul> Source code in <code>fptools/preprocess/steps/zscore.py</code> <pre><code>def __call__(self, session: Session) -&gt; Session:\n    \"\"\"Effect this preprocessing step.\n\n    Args:\n        session: the session to operate upon\n\n    Returns:\n        Session with the preprocessing step applied\n    \"\"\"\n    for signame in self._resolve_signal_names(session, self.signals):\n        sig = session.signals[signame]\n\n        sig.signal = stats.zscore(sig.signal)\n        sig.units = \"AU\"\n\n    return session\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/zscore/#fptools.preprocess.steps.zscore.Zscore.__init__","title":"<code>__init__(signals)</code>","text":"<p>Initialize this preprocessor.</p> <p>Parameters:</p> <ul> <li> <code>signals</code>               (<code>SignalList</code>)           \u2013            <p>list of signal names to be downsampled</p> </li> <li> <code>frequency</code>           \u2013            <p>critical frequency used for lowpass filter</p> </li> </ul> Source code in <code>fptools/preprocess/steps/zscore.py</code> <pre><code>def __init__(self, signals: SignalList):\n    \"\"\"Initialize this preprocessor.\n\n    Args:\n        signals: list of signal names to be downsampled\n        frequency: critical frequency used for lowpass filter\n    \"\"\"\n    self.signals = signals\n</code></pre>"},{"location":"reference/fptools/preprocess/steps/zscore/#fptools.preprocess.steps.zscore.Zscore.plot","title":"<code>plot(session, ax)</code>","text":"<p>Plot the effects of this preprocessing step. Will show the computed zscore signal.</p> <p>Parameters:</p> <ul> <li> <code>session</code>               (<code>Session</code>)           \u2013            <p>the session being operated upon</p> </li> <li> <code>ax</code>               (<code>Axes</code>)           \u2013            <p>matplotlib Axes for plotting onto</p> </li> </ul> Source code in <code>fptools/preprocess/steps/zscore.py</code> <pre><code>def plot(self, session: Session, ax: Axes):\n    \"\"\"Plot the effects of this preprocessing step. Will show the computed zscore signal.\n\n    Args:\n        session: the session being operated upon\n        ax: matplotlib Axes for plotting onto\n    \"\"\"\n    signals = self._resolve_signal_names(session, self.signals)\n    palette = sns.color_palette(\"colorblind\", n_colors=len(signals))\n    for i, signame in enumerate(signals):\n        sig = session.signals[signame]\n        ax.plot(sig.time, sig.signal, label=f\"corrected {sig.name}\", c=palette[i], linestyle=\"-\")\n    ax.set_title(\"Calculated zscore Signal\")\n    ax.legend()\n</code></pre>"},{"location":"reference/fptools/viz/","title":"viz","text":""},{"location":"reference/fptools/viz/#fptools.viz","title":"<code>fptools.viz</code>","text":"<p>Modules:</p> <ul> <li> <code>behavior</code>           \u2013            </li> <li> <code>common</code>           \u2013            </li> <li> <code>signal</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/viz/common/","title":"common","text":""},{"location":"reference/fptools/viz/common/#fptools.viz.common","title":"<code>fptools.viz.common</code>","text":"<p>Functions:</p> <ul> <li> <code>get_colormap</code>             \u2013              <p>Generate a matplotlib Colormap from a <code>Palette</code>.</p> </li> </ul>"},{"location":"reference/fptools/viz/common/#fptools.viz.common.get_colormap","title":"<code>get_colormap(palette)</code>","text":"<p>Generate a matplotlib Colormap from a <code>Palette</code>.</p> <p>Parameters:</p> <ul> <li> <code>palette</code>               (<code>Palette</code>)           \u2013            <p>if None, a diverging palette will be created. If a str, will fetch the corresponding matplotlib colormap. If a list, will generate a matplotlib listed colormap.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Colormap</code>           \u2013            <p>a matplotlib colormap instance.</p> </li> </ul> Source code in <code>fptools/viz/common.py</code> <pre><code>def get_colormap(palette: Palette) -&gt; mpl.colors.Colormap:\n    \"\"\"Generate a matplotlib Colormap from a `Palette`.\n\n    Args:\n        palette: if None, a diverging palette will be created. If a str, will fetch the corresponding matplotlib colormap. If a list, will generate a matplotlib listed colormap.\n\n    Returns:\n        a matplotlib colormap instance.\n    \"\"\"\n    if palette is None:\n        return sns.diverging_palette(250, 30, l=65, center=\"dark\", as_cmap=True)\n    elif isinstance(palette, str):\n        return mpl.colormaps[palette]\n    elif isinstance(palette, list):\n        return mpl.colors.ListedColormap(palette)\n</code></pre>"},{"location":"reference/fptools/viz/signal/","title":"signal","text":""},{"location":"reference/fptools/viz/signal/#fptools.viz.signal","title":"<code>fptools.viz.signal</code>","text":"<p>Functions:</p> <ul> <li> <code>plot_heatmap</code>             \u2013              <p>Plot a signal as a heatmap.</p> </li> <li> <code>plot_signal</code>             \u2013              <p>Plot a signal as a lineplot.</p> </li> <li> <code>sig_catplot</code>             \u2013              <p>Plot signals, similar to <code>seaborn.catplot()</code>.</p> </li> </ul>"},{"location":"reference/fptools/viz/signal/#fptools.viz.signal.plot_heatmap","title":"<code>plot_heatmap(signal, ax=None, cmap='viridis', vmin=None, vmax=None)</code>","text":"<p>Plot a signal as a heatmap.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>Signal</code>)           \u2013            <p>the signal to be plotted</p> </li> <li> <code>ax</code>               (<code>Optional[Axes]</code>, default:                   <code>None</code> )           \u2013            <p>optional axes to plot on. If not provided, a new figure with a single axes will be created</p> </li> <li> <code>cmap</code>               (<code>Palette</code>, default:                   <code>'viridis'</code> )           \u2013            <p>colormap to use for plotting</p> </li> <li> <code>vmin</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>minimum value mapping to colormap start. If None, will use the data minimum value</p> </li> <li> <code>vmax</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>maximum value mapping to colormap end. If None, will use the data maximum value</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Axes</code>           \u2013            <p>Axes</p> </li> </ul> Source code in <code>fptools/viz/signal.py</code> <pre><code>def plot_heatmap(\n    signal: Signal, ax: Optional[Axes] = None, cmap: Palette = \"viridis\", vmin: Optional[float] = None, vmax: Optional[float] = None\n) -&gt; Axes:\n    \"\"\"Plot a signal as a heatmap.\n\n    Args:\n        signal: the signal to be plotted\n        ax: optional axes to plot on. If not provided, a new figure with a single axes will be created\n        cmap: colormap to use for plotting\n        vmin: minimum value mapping to colormap start. If None, will use the data minimum value\n        vmax: maximum value mapping to colormap end. If None, will use the data maximum value\n\n    Returns:\n        Axes\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    cbar_kwargs = {\"label\": f\"{signal.name} ({signal.units})\"}\n\n    sns.heatmap(data=np.atleast_2d(signal.signal), ax=ax, cmap=get_colormap(cmap), vmin=vmin, vmax=vmax, cbar_kws=cbar_kwargs)\n\n    xticks = [0, signal.nsamples]\n    xticklabels = [f\"{signal.time[0]:0.0f}\", f\"{signal.time[-1]:0.0f}\"]\n    for m, t in signal.marks.items():\n        i = signal.tindex(t)\n        ax.axvline(i, c=\"w\", ls=\"--\")\n        xticks.append(i)\n        xticklabels.append(f\"{m}\")\n    order = np.argsort(xticks)\n    xticks = [xticks[i] for i in order]\n    xticklabels = [xticklabels[i] for i in order]\n    ax.set_xticks(xticks, labels=xticklabels, rotation=0)\n\n    ax.set_xlabel(\"Time, Relative to Event (sec)\")\n\n    return ax\n</code></pre>"},{"location":"reference/fptools/viz/signal/#fptools.viz.signal.plot_signal","title":"<code>plot_signal(signal, ax=None, show_indv=True, color='k', indv_c='b', indv_alpha=0.1, indv_kwargs=None, agg_kwargs=None)</code>","text":"<p>Plot a signal as a lineplot.</p> <p>Parameters:</p> <ul> <li> <code>signal</code>               (<code>Signal</code>)           \u2013            <p>the signal to be plotted</p> </li> <li> <code>ax</code>               (<code>Optional[Axes]</code>, default:                   <code>None</code> )           \u2013            <p>optional axes to plot on. If not provided, a new figure with a single axes will be created</p> </li> <li> <code>show_indv</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>if True, plot individual traces, otherwise only plot aggregate traces</p> </li> <li> <code>color</code>               (<code>ColorType</code>, default:                   <code>'k'</code> )           \u2013            <p>color of the aggregated trace</p> </li> <li> <code>indv_c</code>               (<code>ColorType</code>, default:                   <code>'b'</code> )           \u2013            <p>color of individual traces</p> </li> <li> <code>indv_alpha</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>alpha transparency for individual traces</p> </li> <li> <code>indv_kwargs</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>kwargs to pass to <code>seaborn.lineplot()</code> for individual traces</p> </li> <li> <code>agg_kwargs</code>               (<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>kwargs to pass to <code>seaborn.lineplot()</code> for aggregate traces</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Axes</code>           \u2013            <p>Axes</p> </li> </ul> Source code in <code>fptools/viz/signal.py</code> <pre><code>def plot_signal(\n    signal: Signal,\n    ax: Optional[Axes] = None,\n    show_indv: bool = True,\n    color: ColorType = \"k\",\n    indv_c: ColorType = \"b\",\n    indv_alpha: float = 0.1,\n    indv_kwargs: Optional[dict] = None,\n    agg_kwargs: Optional[dict] = None,\n) -&gt; Axes:\n    \"\"\"Plot a signal as a lineplot.\n\n    Args:\n        signal: the signal to be plotted\n        ax: optional axes to plot on. If not provided, a new figure with a single axes will be created\n        show_indv: if True, plot individual traces, otherwise only plot aggregate traces\n        color: color of the aggregated trace\n        indv_c: color of individual traces\n        indv_alpha: alpha transparency for individual traces\n        indv_kwargs: kwargs to pass to `seaborn.lineplot()` for individual traces\n        agg_kwargs: kwargs to pass to `seaborn.lineplot()` for aggregate traces\n\n    Returns:\n        Axes\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    df = pd.DataFrame(signal.signal.T)\n    df.index = signal.time\n    df = df.melt(ignore_index=False)\n\n    _indv_kwargs = {\n        \"alpha\": indv_alpha,\n        \"color\": indv_c,\n    }\n    if indv_kwargs is not None:\n        _indv_kwargs.update(indv_kwargs)\n\n    if show_indv and signal.nobs &gt; 1:\n        for i in range(signal.signal.shape[0]):\n            sns.lineplot(data=None, x=signal.time, y=signal.signal[i, :], ax=ax, **_indv_kwargs)\n\n    _agg_kwargs = {\"color\": color}\n    if agg_kwargs is not None:\n        _agg_kwargs.update(agg_kwargs)\n\n    sns.lineplot(data=df, x=df.index, y=\"value\", ax=ax, **_agg_kwargs)\n\n    ax.set_xlabel(\"Time, Relative to Event (s)\")\n    ax.set_ylabel(f\"{signal.name} ({signal.units})\")\n\n    xticks = ax.get_xticks()\n    xticklabels = ax.get_xticklabels()\n    if len(xticklabels) == 0:\n        xticklabels = [Text(text=f\"{xt}\") for xt in xticks]\n    for k, v in signal.marks.items():\n        # only annotate marks if they are within the time domain\n        if v &gt;= signal.time.min() and v &lt;= signal.time.max():\n            ax.axvline(v, c=\"gray\", ls=\"--\")\n            try:\n                xt = np.where(xticks == float(v))[0][0]\n                xticklabels[xt] = Text(text=k)\n            except:\n                xticks = np.append(xticks, float(v))\n                xticklabels.append(Text(text=k))\n                order = np.argsort(xticks)\n                xticks = xticks[order]\n                xticklabels = [xticklabels[i] for i in order]\n                pass\n    ax.set_xticks(xticks, [t.get_text() for t in xticklabels])\n\n    return ax\n</code></pre>"},{"location":"reference/fptools/viz/signal/#fptools.viz.signal.sig_catplot","title":"<code>sig_catplot(sessions, signal, col=None, col_order=None, row=None, row_order=None, palette=None, hue=None, hue_order=None, show_indv=False, indv_alpha=0.1, height=6, aspect=1.5, sharex=True, sharey=True, agg_method='mean', indv_kwargs=None, agg_kwargs=None)</code>","text":"<p>Plot signals, similar to <code>seaborn.catplot()</code>.</p> <p>You may provide more than one signal name to the <code>signal</code> parameter. In this case, you must also specify one facet (e.x. <code>col</code>, <code>row</code>, <code>hue</code>) as the string \"signal\".</p> <p>Parameters:</p> <ul> <li> <code>sessions</code>               (<code>SessionCollection</code>)           \u2013            <p>sessions to plot data from</p> </li> <li> <code>signal</code>               (<code>Union[str, list[str]]</code>)           \u2013            <p>name of the signal(s) to be plotted.</p> </li> <li> <code>col</code>               (<code>Union[Literal['signal'], str, None]</code>, default:                   <code>None</code> )           \u2013            <p>metadata column on which to form plot columns, or if multiple signal names are given to <code>signal</code>, you may specify \"signal\" here</p> </li> <li> <code>col_order</code>               (<code>Union[list[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>explicit ordering for columns</p> </li> <li> <code>row</code>               (<code>Union[Literal['signal'], str, None]</code>, default:                   <code>None</code> )           \u2013            <p>metadata column on which to form plot rows, or if multiple signal names are given to <code>signal</code>, you may specify \"signal\" here</p> </li> <li> <code>row_order</code>               (<code>Union[list[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>explicit ordering for rows</p> </li> <li> <code>palette</code>               (<code>Optional[Union[str, list, dict[Any, str]]]</code>, default:                   <code>None</code> )           \u2013            <p>palette to use for hue mapping. A dict[value, color], or something that sns.color_palette() understands</p> </li> <li> <code>hue</code>               (<code>Union[str, None]</code>, default:                   <code>None</code> )           \u2013            <p>metadata column on which to group and color, or if multiple signal names are given to <code>signal</code>, you may specify \"signal\" here</p> </li> <li> <code>hue_order</code>               (<code>Union[list[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>explicit ordering for hues</p> </li> <li> <code>show_indv</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if True, show individual session traces</p> </li> <li> <code>indv_alpha</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>alpha transparency level for individual session traces, in range (0, 1)</p> </li> <li> <code>height</code>               (<code>float</code>, default:                   <code>6</code> )           \u2013            <p>height of each facet</p> </li> <li> <code>aspect</code>               (<code>float</code>, default:                   <code>1.5</code> )           \u2013            <p>Aspect ratio of each facet, so that aspect * height gives the width of each facet</p> </li> <li> <code>sharex</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true, the facets will share x axes.</p> </li> <li> <code>sharey</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If true, the facets will share y axes.</p> </li> <li> <code>agg_method</code>               (<code>str</code>, default:                   <code>'mean'</code> )           \u2013            <p>method to use for aggregation (see <code>SessionCollection.aggregate_signals()</code> for more details)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Figure, ndarray]</code>           \u2013            <p>Figure and array of axes</p> </li> </ul> Source code in <code>fptools/viz/signal.py</code> <pre><code>def sig_catplot(\n    sessions: SessionCollection,\n    signal: Union[str, list[str]],\n    col: Union[Literal[\"signal\"], str, None] = None,\n    col_order: Union[list[str], None] = None,\n    row: Union[Literal[\"signal\"], str, None] = None,\n    row_order: Union[list[str], None] = None,\n    palette: Optional[Union[str, list, dict[Any, str]]] = None,\n    hue: Union[str, None] = None,\n    hue_order: Union[list[str], None] = None,\n    show_indv: bool = False,\n    indv_alpha: float = 0.1,\n    height: float = 6,\n    aspect: float = 1.5,\n    sharex: bool = True,\n    sharey: bool = True,\n    agg_method: str = \"mean\",\n    indv_kwargs: Optional[dict] = None,\n    agg_kwargs: Optional[dict] = None,\n) -&gt; tuple[Figure, np.ndarray]:\n    \"\"\"Plot signals, similar to `seaborn.catplot()`.\n\n    You may provide more than one signal name to the `signal` parameter. In this case, you must also specify one\n    facet (e.x. `col`, `row`, `hue`) as the string \"signal\".\n\n    Args:\n        sessions: sessions to plot data from\n        signal: name of the signal(s) to be plotted.\n        col: metadata column on which to form plot columns, or if multiple signal names are given to `signal`, you may specify \"signal\" here\n        col_order: explicit ordering for columns\n        row: metadata column on which to form plot rows, or if multiple signal names are given to `signal`, you may specify \"signal\" here\n        row_order: explicit ordering for rows\n        palette: palette to use for hue mapping. A dict[value, color], or something that sns.color_palette() understands\n        hue: metadata column on which to group and color, or if multiple signal names are given to `signal`, you may specify \"signal\" here\n        hue_order: explicit ordering for hues\n        show_indv: if True, show individual session traces\n        indv_alpha: alpha transparency level for individual session traces, in range (0, 1)\n        height: height of each facet\n        aspect: Aspect ratio of each facet, so that aspect * height gives the width of each facet\n        sharex: If true, the facets will share x axes.\n        sharey: If true, the facets will share y axes.\n        agg_method: method to use for aggregation (see `SessionCollection.aggregate_signals()` for more details)\n\n    Returns:\n        Figure and array of axes\n    \"\"\"\n    metadata = sessions.metadata\n\n    _signals: list[str] = []\n    if isinstance(signal, str):\n        _signals = [signal]\n    else:\n        _signals = list(signal)\n\n    if len(_signals) &gt; 1:\n        # multiple signals provided, need to check a few things...\n        # 1) at least one facet should be \"signal\"\n        # 2) at most one facet should be \"signal\"\n        num_sig_facets = [(fname, fval) for fname, fval in [(\"col\", col), (\"row\", row), (\"hue\", hue)] if fval == \"signal\"]\n        if len(num_sig_facets) &lt;= 0:\n            raise ValueError('When providing multiple values to parameter `signal`, at least one facet must be set to \"signal\"!')\n        if len(num_sig_facets) &gt; 1:\n            raise ValueError('When providing multiple values to parameter `signal`, at most one facet may be set to \"signal\"!')\n\n    plot_cols: list[Any]\n    if col is not None:\n        if col == \"signal\":\n            plot_cols = [s for s in _signals]\n        elif col_order is None:\n            if isinstance(metadata[col].dtype, pd.CategoricalDtype):\n                plot_cols = list(metadata[col].cat.categories.values)\n            else:\n                plot_cols = sorted(metadata[col].unique())\n        else:\n            avail_cols = list(metadata[col].unique())\n            plot_cols = [c for c in col_order if c in avail_cols]\n    else:\n        plot_cols = [None]\n\n    plot_rows: list[Any]\n    if row is not None:\n        if row == \"signal\":\n            plot_rows = [s for s in _signals]\n        elif row_order is None:\n            if isinstance(metadata[row].dtype, pd.CategoricalDtype):\n                plot_rows = list(metadata[row].cat.categories.values)\n            else:\n                plot_rows = sorted(metadata[row].unique())\n        else:\n            avail_rows = list(metadata[row].unique())\n            plot_rows = [r for r in row_order if r in avail_rows]\n    else:\n        plot_rows = [None]\n\n    if hue is not None and hue_order is None:\n        if hue == \"signal\":\n            hue_order = [s for s in _signals]\n        elif isinstance(metadata[hue].dtype, pd.CategoricalDtype):\n            hue_order = metadata[hue].cat.categories.values\n        else:\n            hue_order = sorted(metadata[hue].unique())\n\n    use_palette: list[str] = []\n    if hue_order is not None:\n        if palette is None:\n            # default palette\n            _palette = sns.color_palette(\"colorblind\", n_colors=len(hue_order))\n            use_palette = [_palette[i] for i in range(len(hue_order))]\n        elif isinstance(palette, Mapping):\n            # we got a dict-like of categories -&gt; colors\n            use_palette = [palette[item] for item in hue_order]\n        else:\n            # list or string, it's seaborn's problem now\n            _palette = sns.color_palette(palette, n_colors=len(hue_order))\n            use_palette = [_palette[i] for i in range(len(hue_order))]\n\n    fig, axs = plt.subplots(\n        len(plot_rows),\n        len(plot_cols),\n        figsize=(len(plot_cols) * (height * aspect), len(plot_rows) * height),\n        sharey=sharey,\n        sharex=sharex,\n        squeeze=False,\n    )\n\n    sig_to_plot = _signals[0]\n    for row_i, cur_row in enumerate(plot_rows):\n        if cur_row is not None:\n            if row == \"signal\":\n                row_criteria = np.ones(len(metadata.index), dtype=bool)\n                row_title = None\n                sig_to_plot = cur_row\n            else:\n                row_criteria = metadata[row] == cur_row\n                row_title = f\"{row} = {cur_row}\"\n        else:\n            row_criteria = np.ones(len(metadata.index), dtype=bool)\n            row_title = None\n\n        for col_i, cur_col in enumerate(plot_cols):\n            if cur_col is not None:\n                if col == \"signal\":\n                    col_criteria = np.ones(len(metadata.index), dtype=bool)\n                    col_title = None\n                    sig_to_plot = cur_col\n                else:\n                    col_criteria = metadata[col] == cur_col\n                    col_title = f\"{col} = {cur_col}\"\n            else:\n                col_criteria = np.ones(len(metadata.index), dtype=bool)\n                col_title = None\n\n            ax = axs[row_i, col_i]\n            ax.xaxis.set_tick_params(labelbottom=True)\n            ax.yaxis.set_tick_params(labelbottom=True)\n\n            if hue == \"signal\":\n                title = \"\"\n                if col_title is not None or row_title is not None:\n                    title += \" &amp; \".join([t for t in [col_title, row_title] if t is not None])\n                ax.set_title(title)\n            else:\n                title = f\"{sig_to_plot}\"\n                if col_title is not None or row_title is not None:\n                    title += \" at \" + \" &amp; \".join([t for t in [col_title, row_title] if t is not None])\n                ax.set_title(title)\n\n            if hue is None:\n                try:\n                    sig = sessions.select(row_criteria, col_criteria).aggregate_signals(sig_to_plot, method=agg_method)\n                    plot_signal(\n                        sig,\n                        ax=ax,\n                        show_indv=show_indv,\n                        color=use_palette[0],\n                        indv_c=use_palette[0],\n                        indv_alpha=indv_alpha,\n                        indv_kwargs=indv_kwargs,\n                        agg_kwargs=agg_kwargs,\n                    )\n                except:\n                    pass\n\n            elif hue_order is not None:\n                legend_items = []\n                legend_labels = []\n                for hi, curr_hue in enumerate(hue_order):\n                    try:\n                        if hue == \"signal\":\n                            sess_subset = sessions.select(row_criteria, col_criteria)\n                            sig_to_plot = curr_hue\n                        else:\n                            sess_subset = sessions.select(row_criteria, col_criteria, metadata[hue] == curr_hue)\n\n                        if len(sess_subset) &gt; 0:\n                            sig = sess_subset.aggregate_signals(sig_to_plot, method=agg_method)\n                            plot_signal(\n                                sig,\n                                ax=ax,\n                                show_indv=show_indv,\n                                color=use_palette[hi],\n                                indv_c=use_palette[hi],\n                                indv_kwargs=indv_kwargs,\n                                agg_kwargs=agg_kwargs,\n                            )\n\n                        legend_items.append(Line2D([0], [0], color=use_palette[hi]))\n                        legend_labels.append(f\"{curr_hue}, n={len(sess_subset)}\")\n\n                    except:\n                        raise\n\n                ax.legend(legend_items, legend_labels, loc=\"upper right\")\n                # sns.move_legend(ax, loc=\"upper left\", bbox_to_anchor=(1, 1))\n    return fig, axs\n</code></pre>"},{"location":"reference/fptools/viz/behavior/","title":"behavior","text":""},{"location":"reference/fptools/viz/behavior/#fptools.viz.behavior","title":"<code>fptools.viz.behavior</code>","text":"<p>Modules:</p> <ul> <li> <code>cumulative</code>           \u2013            </li> <li> <code>raster</code>           \u2013            </li> </ul>"},{"location":"reference/fptools/viz/behavior/cumulative/","title":"cumulative","text":""},{"location":"reference/fptools/viz/behavior/cumulative/#fptools.viz.behavior.cumulative","title":"<code>fptools.viz.behavior.cumulative</code>","text":"<p>Classes:</p> <ul> <li> <code>CumulativeEventsResult</code>           \u2013            <p>Object to store results from <code>plot_cumulative_events()</code>.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>plot_cumulative_events</code>             \u2013              <p>Plot cumulative number of events over time.</p> </li> </ul>"},{"location":"reference/fptools/viz/behavior/cumulative/#fptools.viz.behavior.cumulative.CumulativeEventsResult","title":"<code>CumulativeEventsResult</code>  <code>dataclass</code>","text":"<p>Object to store results from <code>plot_cumulative_events()</code>.</p> Source code in <code>fptools/viz/behavior/cumulative.py</code> <pre><code>@dataclasses.dataclass(init=False)\nclass CumulativeEventsResult:\n    \"\"\"Object to store results from `plot_cumulative_events()`.\"\"\"\n\n    fig: Figure\n    means: pd.DataFrame\n</code></pre>"},{"location":"reference/fptools/viz/behavior/cumulative/#fptools.viz.behavior.cumulative.plot_cumulative_events","title":"<code>plot_cumulative_events(event_df, col=None, col_order=None, row=None, row_order=None, event='rewarded_nosepoke', individual='Subject', palette=None, hue=None, hue_order=None, indv_alpha=0.1)</code>","text":"<p>Plot cumulative number of events over time.</p> <p>Parameters:</p> <ul> <li> <code>event_df</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame of events</p> </li> <li> <code>col</code>               (<code>Union[str, None]</code>, default:                   <code>None</code> )           \u2013            <p>column in the dataframe to form plot columns on</p> </li> <li> <code>col_order</code>               (<code>Union[List[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>order for the columns in the plot. If None, use the natural sorted order of unique items</p> </li> <li> <code>row</code>               (<code>Union[str, None]</code>, default:                   <code>None</code> )           \u2013            <p>column in the dataframe to form plot rows on</p> </li> <li> <code>row_order</code>               (<code>Union[List[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>order for the rows in the plot. If None, use the natural sorted order of unique items</p> </li> <li> <code>event</code>               (<code>str</code>, default:                   <code>'rewarded_nosepoke'</code> )           \u2013            <p>the event type to be plotted</p> </li> <li> <code>individual</code>               (<code>Union[str, List[str]]</code>, default:                   <code>'Subject'</code> )           \u2013            <p>key in the dataframe for identifying individual subjects</p> </li> <li> <code>palette</code>               (<code>Palette</code>, default:                   <code>None</code> )           \u2013            <p>palette of colors to be used.</p> </li> <li> <code>hue</code>               (<code>Union[str, None]</code>, default:                   <code>None</code> )           \u2013            <p>key in the dataframe that will produce different colors</p> </li> <li> <code>hue_order</code>               (<code>Union[List[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>order for processing the hue semantic</p> </li> <li> <code>indv_alpha</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>alpha transparency for individual lines</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>CumulativeEventsResult</code>           \u2013            <p>CumulativeEventsResult containing figure, summary dataframe</p> </li> </ul> Source code in <code>fptools/viz/behavior/cumulative.py</code> <pre><code>def plot_cumulative_events(\n    event_df: pd.DataFrame,\n    col: Union[str, None] = None,\n    col_order: Union[List[str], None] = None,\n    row: Union[str, None] = None,\n    row_order: Union[List[str], None] = None,\n    event: str = \"rewarded_nosepoke\",\n    individual: Union[str, List[str]] = \"Subject\",\n    palette: Palette = None,\n    hue: Union[str, None] = None,\n    hue_order: Union[List[str], None] = None,\n    indv_alpha: float = 0.1,\n) -&gt; CumulativeEventsResult:\n    \"\"\"Plot cumulative number of events over time.\n\n    Args:\n        event_df: DataFrame of events\n        col: column in the dataframe to form plot columns on\n        col_order: order for the columns in the plot. If None, use the natural sorted order of unique items\n        row: column in the dataframe to form plot rows on\n        row_order: order for the rows in the plot. If None, use the natural sorted order of unique items\n        event: the event type to be plotted\n        individual: key in the dataframe for identifying individual subjects\n        palette: palette of colors to be used.\n        hue: key in the dataframe that will produce different colors\n        hue_order: order for processing the hue semantic\n        indv_alpha: alpha transparency for individual lines\n\n    Returns:\n        CumulativeEventsResult containing figure, summary dataframe\n    \"\"\"\n    result = CumulativeEventsResult()\n\n    if col is not None:\n        if col_order is None:\n            plot_cols = sorted(event_df[col].unique())\n        else:\n            avail_cols = list(event_df[col].unique())\n            plot_cols = [c for c in col_order if c in avail_cols]\n    else:\n        plot_cols = [None]\n\n    if row is not None:\n        if row_order is None:\n            plot_rows = sorted(event_df[row].unique())\n        else:\n            avail_rows = list(event_df[row].unique())\n            plot_rows = [r for r in row_order if r in avail_rows]\n    else:\n        plot_rows = [None]\n\n    fig, axs = plt.subplots(\n        len(plot_rows), len(plot_cols), figsize=(len(plot_cols) * 5, len(plot_rows) * 5), sharey=True, sharex=True, squeeze=False\n    )\n    result.fig = fig\n\n    y_label = \"Cumulative \" + \" \".join([part.capitalize() for part in event.split(\"_\")])\n\n    if len(plot_rows) &gt; 1:\n        fig.text(0, 0.5, y_label, rotation=\"vertical\", ha=\"center\", va=\"top\", rotation_mode=\"anchor\")\n\n    if hue is not None and hue_order is None:\n        hue_order = sorted(event_df[hue].unique())\n\n    if hue_order is not None and palette is None:\n        palette = sns.color_palette(\"colorblind\", n_colors=len(hue_order))\n\n    bins = np.arange(np.ceil(event_df[\"time\"].max()))\n    event_condition = event_df[\"event\"] == event\n    all_mean_df_items = []\n\n    for row_i, cur_row in enumerate(plot_rows):\n        if cur_row is not None:\n            row_condition = event_condition &amp; (event_df[row] == cur_row)\n        else:\n            row_condition = event_condition\n\n        for col_i, cur_col in enumerate(plot_cols):\n            if cur_col is not None:\n                col_condition = row_condition &amp; (event_df[col] == cur_col)\n            else:\n                col_condition = row_condition\n\n            ax = axs[row_i, col_i]\n\n            for indv in event_df[individual].drop_duplicates().values:\n                if isinstance(individual, str):\n                    sub_condition = col_condition &amp; (event_df[individual] == indv)\n                else:\n                    sub_condition = col_condition &amp; (event_df[individual] == indv).all(axis=1)\n                sub_data = event_df[sub_condition]\n\n                # check if any events, if none, don't plot -- otherwise we generate a warning!\n                if len(sub_data.index) &gt; 0:\n                    sns.ecdfplot(\n                        data=event_df[sub_condition],\n                        x=\"time\",\n                        hue=hue,\n                        stat=\"count\",\n                        hue_order=hue_order,\n                        palette=palette,\n                        alpha=indv_alpha,\n                        ax=ax,\n                        legend=False,\n                    )\n            if row_i == 0:\n                ax.set_title(cur_col)\n            ax.set_xlabel(\"Time (minutes)\")\n            if cur_row is None:\n                ax.set_ylabel(y_label)\n            else:\n                ax.set_ylabel(cur_row)\n            ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(base=600))\n            formatter = mpl.ticker.FuncFormatter(lambda sec, pos: f\"{sec / 60:0.0f}\")\n            ax.xaxis.set_major_formatter(formatter)\n            sns.despine(ax=ax)\n\n            # compute averaged cumulative events\n            id_vars = {}\n            if len(plot_cols) &gt; 1:\n                id_vars[col] = cur_col\n            if len(plot_rows) &gt; 1:\n                id_vars[row] = cur_row\n\n            mean_df_items: List[dict] = []\n            if hue is None:\n                rows = event_df[col_condition]\n                num_animals = len(rows[individual].drop_duplicates().index)\n                for bin in bins:\n                    counts = rows[rows[\"time\"] &lt; bin].value_counts(individual)\n                    mean_df_items.append({**id_vars, \"time\": bin, f\"mean_{event}\": counts.sum() / num_animals})\n                mean_palette = None\n                color = \"#333333\"\n\n            else:\n                for g, rows in event_df[col_condition].groupby(hue):\n                    num_animals = len(rows[individual].drop_duplicates().index)\n                    for bin in bins:\n                        counts = rows[rows[\"time\"] &lt; bin].value_counts(individual)\n                        mean_df_items.append({**id_vars, hue: g, \"time\": bin, f\"mean_{event}\": counts.sum() / num_animals})\n                mean_palette = palette\n                color = None\n\n            all_mean_df_items.extend(mean_df_items)\n\n            sns.lineplot(\n                data=pd.DataFrame(mean_df_items),\n                x=\"time\",\n                y=f\"mean_{event}\",\n                palette=mean_palette,\n                color=color,\n                hue=hue,\n                hue_order=hue_order,\n                ax=ax,\n                legend=\"full\" if cur_col == plot_cols[-1] else False,\n            )\n            # print(True if cur_col == plot_cols[-1] else False)\n\n        if hue is not None:\n            sns.move_legend(ax, loc=\"upper left\", bbox_to_anchor=(1, 1))\n\n    result.means = pd.DataFrame(all_mean_df_items)\n\n    fig.tight_layout()\n    return result\n</code></pre>"},{"location":"reference/fptools/viz/behavior/raster/","title":"raster","text":""},{"location":"reference/fptools/viz/behavior/raster/#fptools.viz.behavior.raster","title":"<code>fptools.viz.behavior.raster</code>","text":"<p>Classes:</p> <ul> <li> <code>RasterPlotResult</code>           \u2013            <p>Object to store results from <code>plot_event_raster()</code>.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>plot_event_raster</code>             \u2013              <p>Generate a raster plot of events over time.</p> </li> </ul>"},{"location":"reference/fptools/viz/behavior/raster/#fptools.viz.behavior.raster.RasterPlotResult","title":"<code>RasterPlotResult</code>  <code>dataclass</code>","text":"<p>Object to store results from <code>plot_event_raster()</code>.</p> Source code in <code>fptools/viz/behavior/raster.py</code> <pre><code>@dataclasses.dataclass(init=False)\nclass RasterPlotResult:\n    \"\"\"Object to store results from `plot_event_raster()`.\"\"\"\n\n    fig: Figure\n    sort_order: Union[dict[str, np.ndarray], None]\n    max_rate: float\n    events: dict[Union[str, None], dict[Union[str, None], List[np.ndarray]]]  # indexed as `rates[column][row][animal][event]`\n    rates: dict[Union[str, None], dict[Union[str, None], List[np.ndarray]]]  # indexed as `rates[column][row][animal][event]`\n    all_rates: list[float]  # flattened list of all rates\n</code></pre>"},{"location":"reference/fptools/viz/behavior/raster/#fptools.viz.behavior.raster.plot_event_raster","title":"<code>plot_event_raster(event_df, col=None, col_order=None, row=None, row_order=None, event='rewarded_nosepoke', individual='Subject', palette=None, sort_col=None, sort_metric='max_rate', sort_dir='asc', rate_max='auto')</code>","text":"<p>Generate a raster plot of events over time.</p> <p>Parameters:</p> <ul> <li> <code>event_df</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame of events</p> </li> <li> <code>col</code>               (<code>Union[str, None]</code>, default:                   <code>None</code> )           \u2013            <p>column in the dataframe to form plot columns on</p> </li> <li> <code>col_order</code>               (<code>Union[list[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>order for the columns in the plot. If None, use the natural sorted order of unique items</p> </li> <li> <code>row</code>               (<code>Union[str, None]</code>, default:                   <code>None</code> )           \u2013            <p>column in the dataframe to form plot rows on</p> </li> <li> <code>row_order</code>               (<code>Union[list[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>order for the rows in the plot. If None, use the natural sorted order of unique items</p> </li> <li> <code>event</code>               (<code>str</code>, default:                   <code>'rewarded_nosepoke'</code> )           \u2013            <p>the event type to be plotted</p> </li> <li> <code>individual</code>               (<code>Union[str, List[str]]</code>, default:                   <code>'Subject'</code> )           \u2013            <p>key in the dataframe for identifying individual subjects</p> </li> <li> <code>palette</code>               (<code>Palette</code>, default:                   <code>None</code> )           \u2013            <p>palette of colors to be used.</p> </li> <li> <code>sort_col</code>               (<code>Union[str, None]</code>, default:                   <code>None</code> )           \u2013            <p>col value to sort individuals</p> </li> <li> <code>rate_max</code>               (<code>Union[float, Literal['auto'], str]</code>, default:                   <code>'auto'</code> )           \u2013            <p>max rate for the ceiling of the colormap. If \"auto\" calculate max from the data; if float the value is taken literally, if str and ends with \"%\" the value is interpreted as a percentage.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RasterPlotResult</code>           \u2013            <p>RasterPlotResult containing figure, summary dataframe</p> </li> </ul> Source code in <code>fptools/viz/behavior/raster.py</code> <pre><code>def plot_event_raster(\n    event_df: pd.DataFrame,\n    col: Union[str, None] = None,\n    col_order: Union[list[str], None] = None,\n    row: Union[str, None] = None,\n    row_order: Union[list[str], None] = None,\n    event: str = \"rewarded_nosepoke\",\n    individual: Union[str, List[str]] = \"Subject\",\n    palette: Palette = None,\n    sort_col: Union[str, None] = None,\n    sort_metric: SORT_METRICS = \"max_rate\",\n    sort_dir: Literal[\"asc\", \"dsc\"] = \"asc\",\n    rate_max: Union[float, Literal[\"auto\"], str] = \"auto\",\n) -&gt; RasterPlotResult:\n    \"\"\"Generate a raster plot of events over time.\n\n    Args:\n        event_df: DataFrame of events\n        col: column in the dataframe to form plot columns on\n        col_order: order for the columns in the plot. If None, use the natural sorted order of unique items\n        row: column in the dataframe to form plot rows on\n        row_order: order for the rows in the plot. If None, use the natural sorted order of unique items\n        event: the event type to be plotted\n        individual: key in the dataframe for identifying individual subjects\n        palette: palette of colors to be used.\n        sort_col: col value to sort individuals\n        rate_max: max rate for the ceiling of the colormap. If \"auto\" calculate max from the data; if float the value is taken literally, if str and ends with \"%\" the value is interpreted as a percentage.\n\n    Returns:\n        RasterPlotResult containing figure, summary dataframe\n    \"\"\"\n    # create a result object to stash results as we go\n    result = RasterPlotResult()\n\n    # determine some parameters of the plot layout\n    plot_cols: list[Any]\n    if col is None:\n        plot_cols = [None]\n    else:\n        if col_order is None:\n            plot_cols = sorted(event_df[col].unique())\n        else:\n            avail_cols = list(event_df[col].unique())\n            plot_cols = [c for c in col_order if c in avail_cols]\n\n    plot_rows: list[Any]\n    if row is None:\n        plot_rows = [None]\n    else:\n        if row_order is None:\n            plot_rows = sorted(event_df[row].unique())\n        else:\n            avail_rows = list(event_df[row].unique())\n            plot_rows = [r for r in row_order if r in avail_rows]\n\n    # construct a figure and axes, and store it in the result object\n    fig, axs = plt.subplots(\n        len(plot_rows), len(plot_cols), figsize=(len(plot_cols) * 5, len(plot_rows) * 2.5), sharey=False, sharex=True, squeeze=False\n    )\n    result.fig = fig\n\n    max_time = event_df[\"time\"].max()\n    raster_events: dict[Union[str, None], dict[Union[str, None], List[np.ndarray]]] = {c: {r: [] for r in plot_rows} for c in plot_cols}\n    raster_event_rates: dict[Union[str, None], dict[Union[str, None], List[np.ndarray]]] = {\n        c: {r: [] for r in plot_rows} for c in plot_cols\n    }\n    for ci, c in enumerate(plot_cols):\n        if c is None:\n            condition = event_df[\"event\"] == event\n        else:\n            condition = (event_df[\"event\"] == event) &amp; (event_df[col] == c)\n\n        for ri, r in enumerate(plot_rows):\n            if r is None:\n                sub_condition = condition\n            else:\n                sub_condition = condition &amp; (event_df[row] == r)\n\n            for indv in event_df[sub_condition][individual].drop_duplicates().values:\n                if isinstance(individual, str):\n                    sub_sub_condition = sub_condition &amp; (event_df[individual] == indv)\n                else:\n                    sub_sub_condition = sub_condition &amp; (event_df[individual] == indv).all(axis=1)\n                events = event_df[sub_sub_condition][\"time\"].values\n                rate = np.array([0] + list(1 / (np.diff(events) / 60)))\n\n                raster_events[c][r].append(events)\n                raster_event_rates[c][r].append(rate)\n    result.events = raster_events\n    result.rates = raster_event_rates\n\n    # Compute sorting orders\n    if sort_col is None:\n        sort_orders = None\n    else:\n        sort_orders = _compute_sort_order(raster_events, raster_event_rates, sort_col=sort_col, sort_metric=sort_metric, sort_dir=sort_dir)\n    result.sort_order = sort_orders\n\n    all_rates: List[float] = []\n    for c, c_rates in raster_event_rates.items():\n        for r, r_rates in c_rates.items():\n            for animal_rates in r_rates:\n                all_rates.extend(animal_rates)\n    result.all_rates = all_rates\n\n    max_rate: float\n    if rate_max == \"auto\":\n        max_rate = np.max(all_rates)\n\n    elif isinstance(rate_max, str) and rate_max.endswith(\"%\"):\n        percent = float(rate_max.replace(\"%\", \"\"))\n        max_rate = float(np.percentile(all_rates, percent))\n\n    elif isinstance(rate_max, (float, int)):\n        max_rate = float(rate_max)\n\n    else:\n        raise ValueError(f'Did not understand argument for `rate_max` = \"{rate_max}\"')\n\n    result.max_rate = max_rate\n\n    # Define a diverging color palette using seaborn\n    cmap = get_colormap(palette)\n    norm = mpl.colors.Normalize(0, max_rate)  # Update normalization range\n\n    # time to build the plot!\n    for ci, c in enumerate(plot_cols):\n        for ri, r in enumerate(plot_rows):\n            ax = axs[ri, ci]\n\n            events = raster_events[c][r]\n            colors = [[cmap(norm(r)) for r in animal] for animal in raster_event_rates[c][r]]\n\n            if sort_orders is not None:\n                events = [events[s] for s in sort_orders[r] if s &lt; len(events)]\n                colors = [colors[s] for s in sort_orders[r] if s &lt; len(colors)]\n\n            ax.eventplot(events, colors=colors, orientation=\"horizontal\", zorder=0.5)\n\n            # first row\n            if ri == 0:\n                ax.set_title(c)\n\n            # last row\n            if ri == len(plot_rows) - 1:\n                ax.set_xlabel(\"Time (minutes)\")\n\n            # first column\n            if ci == 0:\n                ax.set_ylabel(r)\n\n            # set x-tick marks to be every 10 minutes, and format them into minutes\n            ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(base=600))\n            ax.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda sec, pos: f\"{sec / 60:0.0f}\"))\n\n            # set y-tick marks to be integers only\n            ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=\"auto\", integer=True))\n            ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(lambda i, pos: f\"{int(i + 1)}\"))\n\n            # remove spines\n            sns.despine(ax=ax, left=True)\n\n    # finally, add a single colorbar to the plot\n    cbar_label = \" \".join([part.capitalize() for part in event.split(\"_\")]) + \" Rate\"\n    fig.colorbar(ScalarMappable(norm=norm, cmap=cmap), ax=axs, location=\"right\", label=cbar_label)\n\n    return result\n</code></pre>"}]}